{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15195d7",
   "metadata": {},
   "source": [
    "# 虚拟井生成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090bca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保src目录在Python路径中\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LassoCV, LinearRegression, RidgeCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "# 导入模块\n",
    "from src.data_utils import (\n",
    "    extract_uniform_seismic_samples,\n",
    "    filter_anomalous_attributes,\n",
    "    filter_seismic_by_wells,\n",
    "    identify_attributes,\n",
    "    parse_petrel_file,\n",
    ")\n",
    "from src.feature_selection import select_best_features\n",
    "from src.visualization import visualize_attribute_map\n",
    "\n",
    "output_dir = \"output\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = \"SimHei\"  # 黑体 SimHei 支持中文\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False  # 正常显示负号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db20df",
   "metadata": {},
   "source": [
    "## 导入地震数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb9fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_H6_2_attr = parse_petrel_file(\"../data/H6-2_attr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4dc3e2",
   "metadata": {},
   "source": [
    "## 导入井震数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_H6_2_well = \"../data/well_processed.xlsx\"\n",
    "data_H6_2_well = pd.read_excel(file_H6_2_well, sheet_name=\"Sheet1\")\n",
    "\n",
    "# 只选择层位（Surface）为 H6-2 的行，并丢弃砂厚为 NaN 的行\n",
    "data_H6_2_well_selected = (\n",
    "    data_H6_2_well[data_H6_2_well[\"Surface\"] == \"H6-2\"]\n",
    "    .query(\"Well != 'PH6' and Well != 'PH8' and Well != 'PH3' and Well != 'PH2'\")\n",
    "    .replace(-999, np.nan)  # 将-999替换为NaN（通常-999是缺失值的代码）\n",
    "    .dropna(subset=[\"Thickness of facies(1: Fine sand)\"])  # 丢弃砂厚为NaN的行\n",
    "    .reset_index(drop=True)  # 重置索引\n",
    ")\n",
    "\n",
    "# 显示筛选后的前几行数据\n",
    "data_H6_2_well_selected.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5756f454",
   "metadata": {},
   "source": [
    "## 提取共同属性\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cf0842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取地震属性列表\n",
    "seismic_attr, _ = identify_attributes(\"../data/H6-2_attr\")\n",
    "\n",
    "# 提取Excel的属性列表（从第8列开始的所有列）\n",
    "well_seismic_attr = data_H6_2_well.columns[7:].tolist()\n",
    "\n",
    "# 计算两个列表的交集\n",
    "common_attributes = list(set(seismic_attr) & set(well_seismic_attr))\n",
    "\n",
    "# 打印结果\n",
    "print(f\"地震属性数量: {len(seismic_attr)}\")\n",
    "print(f\"Excel属性数量: {len(well_seismic_attr)}\")\n",
    "print(f\"共同属性数量: {len(common_attributes)}\")\n",
    "print(\"\\n共同属性列表:\")\n",
    "for attr in common_attributes:\n",
    "    print(f\"- {attr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80684df4",
   "metadata": {},
   "source": [
    "## 根据井点分布，缩小工区范围\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77821d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 限制工区范围\n",
    "data_H6_2_attr_filtered, area_bounds = filter_seismic_by_wells(\n",
    "    seismic_data=data_H6_2_attr,\n",
    "    well_data=data_H6_2_well_selected,\n",
    "    expansion_factor=1.5,  # 扩展50%\n",
    "    plot=True,\n",
    "    output_dir=output_dir,\n",
    ")\n",
    "\n",
    "# 后续可以直接使用area_bounds中的边界信息\n",
    "print(\"区域边界信息:\")\n",
    "for key, value in area_bounds.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bff370",
   "metadata": {},
   "source": [
    "## 生成统计摘要\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e80ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选出质量良好的属性\n",
    "good_attributes, anomalous_attributes, attribute_stats = filter_anomalous_attributes(\n",
    "    seismic_data=data_H6_2_attr_filtered,\n",
    "    well_data=data_H6_2_well_selected,\n",
    "    common_attributes=common_attributes,\n",
    "    ratio_threshold=5.0,  # 均值比值阈值\n",
    "    range_ratio_threshold=10.0,  # 数值范围比值阈值\n",
    "    std_ratio_threshold=10.0,  # 标准差比值阈值\n",
    "    output_dir=None,  # 输出图表目录\n",
    "    verbose=True,  # 打印详细信息\n",
    ")\n",
    "\n",
    "print(\"\\n筛选后保留的质量良好属性:\")\n",
    "for attr in good_attributes:\n",
    "    print(f\"- {attr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4bdff3",
   "metadata": {},
   "source": [
    "## 随机森林重要性和相关性分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd0117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用随机森林评估特征重要性并移除冗余特征\n",
    "selected_features = select_best_features(\n",
    "    well_data=data_H6_2_well_selected,\n",
    "    attribute_columns=good_attributes,\n",
    "    target_column=\"Thickness of facies(1: Fine sand)\",\n",
    "    n_features=3,\n",
    "    corr_threshold=0.85,\n",
    "    output_dir=output_dir,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 输出特征选择结果\n",
    "print(\"\\n基于随机森林重要性和相关性分析的最佳特征:\")\n",
    "for i, feature in enumerate(selected_features):\n",
    "    print(f\"{i + 1}. {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6418d68",
   "metadata": {},
   "source": [
    "## 提取样本，准备设置虚拟井\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd54b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用筛选后的地震数据区域提取等间距样本\n",
    "seismic_samples = extract_uniform_seismic_samples(\n",
    "    seismic_data=data_H6_2_attr_filtered,\n",
    "    n_rows=40,\n",
    "    n_cols=40,\n",
    "    area_bounds=area_bounds,  # 直接传入边界字典\n",
    ")\n",
    "\n",
    "# 可视化真实井点和采样点\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 绘制地震数据点（使用抽样）\n",
    "sample_ratio = min(1.0, 5000 / len(data_H6_2_attr_filtered))\n",
    "seismic_sample = data_H6_2_attr_filtered.sample(frac=sample_ratio)\n",
    "plt.scatter(seismic_sample[\"X\"], seismic_sample[\"Y\"], color=\"lightgray\", alpha=0.3, s=10, label=\"地震数据(抽样)\")\n",
    "\n",
    "# 绘制真实井点位置\n",
    "plt.scatter(\n",
    "    data_H6_2_well_selected[\"X\"], data_H6_2_well_selected[\"Y\"], color=\"red\", s=100, marker=\"^\", label=\"真实井点\"\n",
    ")\n",
    "\n",
    "# 绘制等间距采样点位置\n",
    "plt.scatter(seismic_samples[\"X\"], seismic_samples[\"Y\"], color=\"blue\", s=50, marker=\"o\", label=\"等间距采样点\")\n",
    "\n",
    "# 添加标题和图例\n",
    "plt.title(\"真实井点与等间距采样点分布\", fontsize=16)\n",
    "plt.xlabel(\"X坐标\", fontsize=14)\n",
    "plt.ylabel(\"Y坐标\", fontsize=14)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# 保存图片\n",
    "plt.savefig(os.path.join(output_dir, \"real_wells_and_seismic_samples.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 保存提取的样本数据\n",
    "seismic_samples.to_csv(os.path.join(output_dir, \"seismic_samples.csv\"), index=False)\n",
    "print(f\"等间距地震样本数据已保存至 {os.path.join(output_dir, 'seismic_samples.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f8fc8",
   "metadata": {},
   "source": [
    "## 多线性模型一致性预测设置虚拟井\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efd7ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建融合属性和多模型预测\n",
    "print(\"======== 创建融合属性和多模型预测 ========\")\n",
    "target_column = \"Thickness of facies(1: Fine sand)\"\n",
    "min_corr_threshold = 0.2  # 最小相关性阈值，低于此值的属性将被排除\n",
    "\n",
    "# 检查每个选定属性在井点数据中的有效性\n",
    "print(\"检查属性在井点数据中的有效性:\")\n",
    "for feature in selected_features:\n",
    "    nan_count = data_H6_2_well_selected[feature].isna().sum()\n",
    "    print(\n",
    "        f\"属性 '{feature}' 在井点数据中的NaN值数量: {nan_count}/{len(data_H6_2_well_selected)} ({nan_count / len(data_H6_2_well_selected) * 100:.1f}%)\"\n",
    "    )\n",
    "\n",
    "# 筛选出所有选定属性都有有效值的井点\n",
    "valid_wells = data_H6_2_well_selected.dropna(subset=selected_features + [target_column])\n",
    "print(f\"\\n所有属性都有有效值的井点数量: {len(valid_wells)} / {len(data_H6_2_well_selected)}\")\n",
    "\n",
    "# 准备训练数据\n",
    "X_labeled = valid_wells[selected_features].values\n",
    "y_labeled = valid_wells[target_column].values\n",
    "\n",
    "# 准备未标记数据（地震样本点）\n",
    "X_unlabeled = seismic_samples[selected_features].dropna().values\n",
    "unlabeled_indices = seismic_samples[selected_features].dropna().index\n",
    "\n",
    "# 1. 融合属性线性加权预测\n",
    "print(\"\\n=== 模型1: 融合属性线性加权 ===\")\n",
    "\n",
    "# 计算相关性权重\n",
    "correlation_weights = {}\n",
    "for i, feature in enumerate(selected_features):\n",
    "    corr, _ = spearmanr(valid_wells[feature], valid_wells[target_column])\n",
    "    if abs(corr) >= min_corr_threshold:\n",
    "        correlation_weights[feature] = corr\n",
    "        print(f\"属性 '{feature}' 与砂厚的Spearman相关性: {corr:.4f}\")\n",
    "    else:\n",
    "        print(f\"属性 '{feature}' 与砂厚的相关性过低 ({corr:.4f})，不纳入融合\")\n",
    "\n",
    "# 如果没有有效属性，使用所有属性且权重相等\n",
    "if len(correlation_weights) == 0:\n",
    "    print(\"警告: 没有属性满足相关性阈值，将使用所有属性且权重相等\")\n",
    "    for feature in selected_features:\n",
    "        correlation_weights[feature] = 1.0\n",
    "        print(f\"属性 '{feature}' 使用默认权重: 1.0\")\n",
    "\n",
    "# 标准化数据\n",
    "scaler = StandardScaler()\n",
    "X_labeled_scaled = scaler.fit_transform(X_labeled)\n",
    "X_unlabeled_scaled = scaler.transform(X_unlabeled)\n",
    "\n",
    "\n",
    "# 创建融合属性函数\n",
    "def create_fused_attribute(X_scaled, features, weights):\n",
    "    \"\"\"\n",
    "    基于选定特征和权重创建融合属性\n",
    "\n",
    "    参数:\n",
    "        X_scaled (ndarray): 标准化后的特征矩阵\n",
    "        features (list): 特征列表\n",
    "        weights (dict): 每个特征的权重\n",
    "\n",
    "    返回:\n",
    "        ndarray: 融合属性\n",
    "    \"\"\"\n",
    "    # 初始化融合属性\n",
    "    fused_attr = np.zeros(X_scaled.shape[0])\n",
    "    weight_sum = 0\n",
    "\n",
    "    # 对每个特征进行加权融合\n",
    "    for i, feature in enumerate(features):\n",
    "        if feature in weights:\n",
    "            weight = weights[feature]\n",
    "            fused_attr += X_scaled[:, i] * weight\n",
    "            weight_sum += abs(weight)\n",
    "\n",
    "    # 归一化融合结果\n",
    "    if weight_sum > 0:\n",
    "        fused_attr /= weight_sum\n",
    "\n",
    "    return fused_attr\n",
    "\n",
    "\n",
    "# 在训练数据上创建融合属性\n",
    "fused_attr_labeled = create_fused_attribute(X_labeled_scaled, selected_features, correlation_weights)\n",
    "\n",
    "# 在未标记数据上创建融合属性\n",
    "fused_attr_unlabeled = create_fused_attribute(X_unlabeled_scaled, selected_features, correlation_weights)\n",
    "\n",
    "# 使用融合属性拟合线性回归\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(fused_attr_labeled.reshape(-1, 1), y_labeled)\n",
    "\n",
    "# 预测\n",
    "fused_pred_labeled = linear_model.predict(fused_attr_labeled.reshape(-1, 1))\n",
    "fused_pred_unlabeled = linear_model.predict(fused_attr_unlabeled.reshape(-1, 1))\n",
    "\n",
    "# 评估融合属性预测效果\n",
    "fused_corr = np.corrcoef(fused_pred_labeled, y_labeled)[0, 1]\n",
    "print(f\"融合属性预测结果与真实砂厚的相关性: {fused_corr:.4f}\")\n",
    "\n",
    "# 2. Lasso回归 + Bootstrap\n",
    "print(\"\\n=== 模型2: LassoCV + Bootstrap ===\")\n",
    "\n",
    "n_bootstrap = 100  # Bootstrap重采样次数\n",
    "alpha_values = np.logspace(-4, 1, 30)  # alpha候选值\n",
    "\n",
    "# 主Lasso模型\n",
    "lasso_model = LassoCV(alphas=alpha_values, cv=5, max_iter=10000, tol=1e-3)\n",
    "lasso_model.fit(X_labeled_scaled, y_labeled)\n",
    "print(f\"Lasso最优alpha值: {lasso_model.alpha_:.6f}\")\n",
    "\n",
    "# 在训练数据上的预测\n",
    "lasso_pred_labeled = lasso_model.predict(X_labeled_scaled)\n",
    "lasso_corr = np.corrcoef(lasso_pred_labeled, y_labeled)[0, 1]\n",
    "print(f\"Lasso预测结果与真实砂厚的相关性: {lasso_corr:.4f}\")\n",
    "\n",
    "# Bootstrap重采样预测\n",
    "lasso_bootstrap_preds = np.zeros((n_bootstrap, X_unlabeled_scaled.shape[0]))\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    # Bootstrap重采样\n",
    "    indices = np.random.choice(len(X_labeled_scaled), len(X_labeled_scaled), replace=True)\n",
    "    X_boot, y_boot = X_labeled_scaled[indices], y_labeled[indices]\n",
    "\n",
    "    # 拟合模型\n",
    "    lasso_boot = LassoCV(alphas=alpha_values, cv=5, max_iter=10000, tol=1e-3)\n",
    "    lasso_boot.fit(X_boot, y_boot)\n",
    "\n",
    "    # 预测\n",
    "    lasso_bootstrap_preds[i, :] = lasso_boot.predict(X_unlabeled_scaled)\n",
    "\n",
    "# 计算预测均值和置信区间\n",
    "lasso_pred_unlabeled = np.mean(lasso_bootstrap_preds, axis=0)\n",
    "lasso_lower_ci = np.percentile(lasso_bootstrap_preds, 2.5, axis=0)\n",
    "lasso_upper_ci = np.percentile(lasso_bootstrap_preds, 97.5, axis=0)\n",
    "\n",
    "# 3. Ridge回归 + Bootstrap\n",
    "print(\"\\n=== 模型3: RidgeCV + Bootstrap ===\")\n",
    "\n",
    "alpha_values = np.logspace(-3, 3, 30)  # alpha候选值\n",
    "\n",
    "# 主Ridge模型\n",
    "ridge_model = RidgeCV(alphas=alpha_values, cv=5)\n",
    "ridge_model.fit(X_labeled_scaled, y_labeled)\n",
    "print(f\"Ridge最优alpha值: {ridge_model.alpha_:.6f}\")\n",
    "\n",
    "# 在训练数据上的预测\n",
    "ridge_pred_labeled = ridge_model.predict(X_labeled_scaled)\n",
    "ridge_corr = np.corrcoef(ridge_pred_labeled, y_labeled)[0, 1]\n",
    "print(f\"Ridge预测结果与真实砂厚的相关性: {ridge_corr:.4f}\")\n",
    "\n",
    "# Bootstrap重采样预测\n",
    "ridge_bootstrap_preds = np.zeros((n_bootstrap, X_unlabeled_scaled.shape[0]))\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    # Bootstrap重采样\n",
    "    indices = np.random.choice(len(X_labeled_scaled), len(X_labeled_scaled), replace=True)\n",
    "    X_boot, y_boot = X_labeled_scaled[indices], y_labeled[indices]\n",
    "\n",
    "    # 拟合模型\n",
    "    ridge_boot = RidgeCV(alphas=alpha_values, cv=5)\n",
    "    ridge_boot.fit(X_boot, y_boot)\n",
    "\n",
    "    # 预测\n",
    "    ridge_bootstrap_preds[i, :] = ridge_boot.predict(X_unlabeled_scaled)\n",
    "\n",
    "# 计算预测均值和置信区间\n",
    "ridge_pred_unlabeled = np.mean(ridge_bootstrap_preds, axis=0)\n",
    "ridge_lower_ci = np.percentile(ridge_bootstrap_preds, 2.5, axis=0)\n",
    "ridge_upper_ci = np.percentile(ridge_bootstrap_preds, 97.5, axis=0)\n",
    "\n",
    "# 4. 一致性筛选（带详细日志）\n",
    "print(\"\\n=== 一致性筛选（详细分析）===\")\n",
    "\n",
    "# 将三个模型的预测结果整合\n",
    "predictions = np.column_stack([fused_pred_unlabeled, lasso_pred_unlabeled, ridge_pred_unlabeled])\n",
    "\n",
    "# 计算每个点三个预测值的最大差异\n",
    "max_diffs = np.max(predictions, axis=1) - np.min(predictions, axis=1)\n",
    "\n",
    "# 计算平均预测值用于相对差异计算\n",
    "mean_preds = np.mean(predictions, axis=1)\n",
    "\n",
    "# 定义分段式一致性阈值 - 只有两个区间\n",
    "value_threshold = 10.0  # 区分小值和大值的边界\n",
    "\n",
    "# 小值区域（< 10米）和大值区域（>= 10米）的掩码\n",
    "small_value_mask = mean_preds < value_threshold\n",
    "large_value_mask = mean_preds >= value_threshold\n",
    "\n",
    "# 小值和大值区域的阈值设置\n",
    "abs_threshold_small = 3.0  # 小值区域绝对差异阈值\n",
    "abs_threshold_large = 5.0  # 大值区域绝对差异阈值\n",
    "rel_threshold_small = 0.15  # 小值区域相对差异阈值\n",
    "rel_threshold_large = 0.25  # 大值区域相对差异阈值\n",
    "\n",
    "# 计算相对差异\n",
    "rel_diffs = max_diffs / (mean_preds + 1e-10)  # 避免除零\n",
    "\n",
    "# 统计各区域原始点数\n",
    "total_small = np.sum(small_value_mask)\n",
    "total_large = np.sum(large_value_mask)\n",
    "\n",
    "print(\"\\n=== 筛选前统计 ===\")\n",
    "print(f\"总点数: {len(mean_preds)}\")\n",
    "print(f\"小值区域 (<{value_threshold}米) 点数: {total_small} ({total_small / len(mean_preds) * 100:.1f}%)\")\n",
    "print(f\"大值区域 (>={value_threshold}米) 点数: {total_large} ({total_large / len(mean_preds) * 100:.1f}%)\")\n",
    "\n",
    "# 对各区域应用不同的一致性标准，保持原有的\"与\"逻辑\n",
    "consistent_small = (max_diffs <= abs_threshold_small) & (rel_diffs <= rel_threshold_small)\n",
    "consistent_large = (max_diffs <= abs_threshold_large) & (rel_diffs <= rel_threshold_large)\n",
    "\n",
    "# 统计各区域通过一致性筛选的点数\n",
    "pass_abs_small = np.sum(small_value_mask & (max_diffs <= abs_threshold_small))\n",
    "pass_rel_small = np.sum(small_value_mask & (rel_diffs <= rel_threshold_small))\n",
    "pass_both_small = np.sum(small_value_mask & consistent_small)\n",
    "\n",
    "pass_abs_large = np.sum(large_value_mask & (max_diffs <= abs_threshold_large))\n",
    "pass_rel_large = np.sum(large_value_mask & (rel_diffs <= rel_threshold_large))\n",
    "pass_both_large = np.sum(large_value_mask & consistent_large)\n",
    "\n",
    "print(\"\\n=== 一致性筛选统计 ===\")\n",
    "print(\n",
    "    f\"小值区域通过绝对差异 (<={abs_threshold_small}): {pass_abs_small}/{total_small} ({pass_abs_small / total_small * 100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"小值区域通过相对差异 (<={rel_threshold_small * 100:.1f}%): {pass_rel_small}/{total_small} ({pass_rel_small / total_small * 100:.1f}%)\"\n",
    ")\n",
    "print(f\"小值区域同时满足两条件: {pass_both_small}/{total_small} ({pass_both_small / total_small * 100:.1f}%)\")\n",
    "\n",
    "print(\n",
    "    f\"大值区域通过绝对差异 (<={abs_threshold_large}): {pass_abs_large}/{total_large} ({pass_abs_large / total_large * 100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"大值区域通过相对差异 (<={rel_threshold_large * 100:.1f}%): {pass_rel_large}/{total_large} ({pass_rel_large / total_large * 100:.1f}%)\"\n",
    ")\n",
    "print(f\"大值区域同时满足两条件: {pass_both_large}/{total_large} ({pass_both_large / total_large * 100:.1f}%)\")\n",
    "\n",
    "# 组合各区域的一致性掩码\n",
    "consistent_mask = (small_value_mask & consistent_small) | (large_value_mask & consistent_large)\n",
    "consistent_count = np.sum(consistent_mask)\n",
    "print(\n",
    "    f\"\\n通过一致性筛选的总点数: {consistent_count}/{len(mean_preds)} ({consistent_count / len(mean_preds) * 100:.1f}%)\"\n",
    ")\n",
    "\n",
    "# Bootstrap预测标准差（作为不确定性）\n",
    "lasso_std = np.std(lasso_bootstrap_preds, axis=0)\n",
    "ridge_std = np.std(ridge_bootstrap_preds, axis=0)\n",
    "\n",
    "# 设置分段式不确定性阈值\n",
    "uncertainty_threshold_small = 2.5  # 小值区域标准差阈值\n",
    "uncertainty_threshold_large = 5.0  # 大值区域标准差阈值\n",
    "\n",
    "# 对各区域应用不同的不确定性标准\n",
    "uncertainty_small = (lasso_std <= uncertainty_threshold_small) & (ridge_std <= uncertainty_threshold_small)\n",
    "uncertainty_large = (lasso_std <= uncertainty_threshold_large) & (ridge_std <= uncertainty_threshold_large)\n",
    "\n",
    "# 统计通过不确定性筛选的点数\n",
    "pass_lasso_small = np.sum(small_value_mask & (lasso_std <= uncertainty_threshold_small))\n",
    "pass_ridge_small = np.sum(small_value_mask & (ridge_std <= uncertainty_threshold_small))\n",
    "pass_both_uncert_small = np.sum(small_value_mask & uncertainty_small)\n",
    "\n",
    "pass_lasso_large = np.sum(large_value_mask & (lasso_std <= uncertainty_threshold_large))\n",
    "pass_ridge_large = np.sum(large_value_mask & (ridge_std <= uncertainty_threshold_large))\n",
    "pass_both_uncert_large = np.sum(large_value_mask & uncertainty_large)\n",
    "\n",
    "print(\"\\n=== 不确定性筛选统计 ===\")\n",
    "print(\n",
    "    f\"小值区域通过Lasso标准差 (<={uncertainty_threshold_small}): {pass_lasso_small}/{total_small} ({pass_lasso_small / total_small * 100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"小值区域通过Ridge标准差 (<={uncertainty_threshold_small}): {pass_ridge_small}/{total_small} ({pass_ridge_small / total_small * 100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"小值区域同时满足两条件: {pass_both_uncert_small}/{total_small} ({pass_both_uncert_small / total_small * 100:.1f}%)\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"大值区域通过Lasso标准差 (<={uncertainty_threshold_large}): {pass_lasso_large}/{total_large} ({pass_lasso_large / total_large * 100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"大值区域通过Ridge标准差 (<={uncertainty_threshold_large}): {pass_ridge_large}/{total_large} ({pass_ridge_large / total_large * 100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"大值区域同时满足两条件: {pass_both_uncert_large}/{total_large} ({pass_both_uncert_large / total_large * 100:.1f}%)\"\n",
    ")\n",
    "\n",
    "# 组合各区域的不确定性掩码\n",
    "uncertainty_mask = (small_value_mask & uncertainty_small) | (large_value_mask & uncertainty_large)\n",
    "uncertainty_count = np.sum(uncertainty_mask)\n",
    "print(\n",
    "    f\"\\n通过不确定性筛选的总点数: {uncertainty_count}/{len(mean_preds)} ({uncertainty_count / len(mean_preds) * 100:.1f}%)\"\n",
    ")\n",
    "\n",
    "# ✅ 最终一致性掩码：预测一致 + 不确定性低\n",
    "final_consistent_mask = consistent_mask & uncertainty_mask\n",
    "final_consistent_indices = np.where(final_consistent_mask)[0]\n",
    "\n",
    "# 统计最终筛选结果\n",
    "final_small_count = np.sum(small_value_mask & final_consistent_mask)\n",
    "final_large_count = np.sum(large_value_mask & final_consistent_mask)\n",
    "\n",
    "print(\"\\n=== 最终筛选结果 ===\")\n",
    "print(f\"小值区域通过最终筛选: {final_small_count}/{total_small} ({final_small_count / total_small * 100:.1f}%)\")\n",
    "print(f\"大值区域通过最终筛选: {final_large_count}/{total_large} ({final_large_count / total_large * 100:.1f}%)\")\n",
    "print(\n",
    "    f\"总计通过筛选点数: {len(final_consistent_indices)}/{len(mean_preds)} ({len(final_consistent_indices) / len(mean_preds) * 100:.1f}%)\"\n",
    ")\n",
    "\n",
    "# 可选：分析大值区域中被筛除的原因\n",
    "if total_large > 0 and final_large_count < total_large:\n",
    "    # 分析大值区域的筛选失败原因\n",
    "    large_indices = np.where(large_value_mask)[0]\n",
    "    fail_abs_count = np.sum(large_value_mask & (max_diffs > abs_threshold_large))\n",
    "    fail_rel_count = np.sum(large_value_mask & (rel_diffs > rel_threshold_large))\n",
    "    fail_both_consist = np.sum(large_value_mask & ~consistent_large)\n",
    "\n",
    "    fail_lasso_count = np.sum(large_value_mask & (lasso_std > uncertainty_threshold_large))\n",
    "    fail_ridge_count = np.sum(large_value_mask & (ridge_std > uncertainty_threshold_large))\n",
    "    fail_both_uncert = np.sum(large_value_mask & ~uncertainty_large)\n",
    "\n",
    "    print(\"\\n=== 大值区域筛选失败原因分析 ===\")\n",
    "    print(\n",
    "        f\"失败原因 - 绝对差异过大 (>{abs_threshold_large}): {fail_abs_count}/{total_large} ({fail_abs_count / total_large * 100:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"失败原因 - 相对差异过大 (>{rel_threshold_large * 100:.1f}%): {fail_rel_count}/{total_large} ({fail_rel_count / total_large * 100:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"失败原因 - 一致性检验不通过: {fail_both_consist}/{total_large} ({fail_both_consist / total_large * 100:.1f}%)\"\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"失败原因 - Lasso标准差过大 (>{uncertainty_threshold_large}): {fail_lasso_count}/{total_large} ({fail_lasso_count / total_large * 100:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"失败原因 - Ridge标准差过大 (>{uncertainty_threshold_large}): {fail_ridge_count}/{total_large} ({fail_ridge_count / total_large * 100:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"失败原因 - 不确定性检验不通过: {fail_both_uncert}/{total_large} ({fail_both_uncert / total_large * 100:.1f}%)\"\n",
    "    )\n",
    "\n",
    "    # 输出一些大值区域样本的详细信息\n",
    "    large_sample_indices = np.random.choice(large_indices, min(5, len(large_indices)), replace=False)\n",
    "    print(\"\\n大值区域样本详情 (随机5个):\")\n",
    "    for idx in large_sample_indices:\n",
    "        print(\n",
    "            f\"  样本 #{idx}: 平均值={mean_preds[idx]:.2f}, 最大差异={max_diffs[idx]:.2f}, 相对差异={rel_diffs[idx] * 100:.1f}%, \"\n",
    "            f\"Lasso标准差={lasso_std[idx]:.2f}, Ridge标准差={ridge_std[idx]:.2f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"    模型预测值: Fused={fused_pred_unlabeled[idx]:.2f}, Lasso={lasso_pred_unlabeled[idx]:.2f}, Ridge={ridge_pred_unlabeled[idx]:.2f}\"\n",
    "        )\n",
    "        print(f\"    通过一致性检验: {consistent_large[idx]}, 通过不确定性检验: {uncertainty_large[idx]}\")\n",
    "\n",
    "# 创建伪标记数据\n",
    "X_pseudo = X_unlabeled[final_consistent_indices]\n",
    "y_pseudo = mean_preds[final_consistent_indices]\n",
    "\n",
    "# 获取一致性样本在原始seismic_samples中的索引\n",
    "consistent_orig_indices = unlabeled_indices[final_consistent_indices]\n",
    "\n",
    "# 添加预测结果到seismic_samples\n",
    "seismic_samples[\"Fused_Pred\"] = np.nan\n",
    "seismic_samples[\"Lasso_Pred\"] = np.nan\n",
    "seismic_samples[\"Ridge_Pred\"] = np.nan\n",
    "seismic_samples[\"Mean_Pred\"] = np.nan\n",
    "seismic_samples[\"Max_Diff\"] = np.nan\n",
    "seismic_samples[\"Is_Consistent\"] = False\n",
    "\n",
    "# 填充预测结果\n",
    "seismic_samples.loc[unlabeled_indices, \"Fused_Pred\"] = fused_pred_unlabeled\n",
    "seismic_samples.loc[unlabeled_indices, \"Lasso_Pred\"] = lasso_pred_unlabeled\n",
    "seismic_samples.loc[unlabeled_indices, \"Ridge_Pred\"] = ridge_pred_unlabeled\n",
    "seismic_samples.loc[unlabeled_indices, \"Mean_Pred\"] = mean_preds\n",
    "seismic_samples.loc[unlabeled_indices, \"Max_Diff\"] = max_diffs\n",
    "seismic_samples.loc[consistent_orig_indices, \"Is_Consistent\"] = True\n",
    "\n",
    "# 统计并报告负值数量\n",
    "neg_count = (seismic_samples[\"Mean_Pred\"] < 0).sum()\n",
    "if neg_count > 0:\n",
    "    print(f\"\\n注意: 有 {neg_count} 个负的砂厚预测值已被替换为0\")\n",
    "\n",
    "# 将负数的Mean_Pred值置为0\n",
    "seismic_samples[\"Mean_Pred\"] = seismic_samples[\"Mean_Pred\"].clip(lower=0)\n",
    "\n",
    "# 输出结果分布统计\n",
    "if len(y_pseudo) > 0:\n",
    "    bins = [0, 5, 10, 15, 20, np.inf]\n",
    "    labels = [\"0-5\", \"5-10\", \"10-15\", \"15-20\", \">20\"]\n",
    "    y_binned = pd.cut(y_pseudo, bins=bins, labels=labels)\n",
    "    bin_counts = y_binned.value_counts().sort_index()\n",
    "\n",
    "    print(\"\\n=== 虚拟井砂厚分布统计 ===\")\n",
    "    for i, count in enumerate(bin_counts):\n",
    "        print(f\"砂厚范围 {labels[i]}米: {count}个 ({count / len(y_pseudo) * 100:.1f}%)\")\n",
    "\n",
    "# 保存预测结果\n",
    "seismic_samples.to_csv(os.path.join(output_dir, \"seismic_samples_with_predictions.csv\"), index=False)\n",
    "print(f\"\\n预测结果已保存至 {os.path.join(output_dir, 'seismic_samples_with_predictions.csv')}\")\n",
    "\n",
    "# 保存一致性虚拟井数据\n",
    "consistent_samples = seismic_samples[seismic_samples[\"Is_Consistent\"] == True].copy()\n",
    "consistent_samples.to_csv(os.path.join(output_dir, \"consistent_pseudo_wells.csv\"), index=False)\n",
    "print(f\"一致性虚拟井数据已保存至 {os.path.join(output_dir, 'consistent_pseudo_wells.csv')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d87c3",
   "metadata": {},
   "source": [
    "## 展示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855d32a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备小工区地震数据（使用已筛选的data_H6_2_attr_filtered）\n",
    "# 1. 首先创建融合属性\n",
    "print(\"为整个小工区地震数据创建融合属性...\")\n",
    "\n",
    "# 标准化数据\n",
    "scaler = StandardScaler()\n",
    "attr_data = data_H6_2_attr_filtered[selected_features].copy()\n",
    "attr_data_scaled = scaler.fit_transform(attr_data)\n",
    "\n",
    "# 使用之前定义的create_fused_attribute函数创建融合属性\n",
    "fused_attr = create_fused_attribute(attr_data_scaled, selected_features, correlation_weights)\n",
    "\n",
    "# 将融合属性添加到地震数据中\n",
    "data_H6_2_attr_filtered[\"Fused_Attribute\"] = fused_attr\n",
    "\n",
    "# 2. 准备真实井点数据\n",
    "real_wells = data_H6_2_well_selected\n",
    "\n",
    "# 3. 准备虚拟井点数据（使用之前保存的一致性虚拟井）\n",
    "pseudo_wells = seismic_samples[seismic_samples[\"Is_Consistent\"] == True].copy()\n",
    "\n",
    "# 可视化融合属性分布与井点位置\n",
    "visualize_attribute_map(\n",
    "    data_points=data_H6_2_attr_filtered,\n",
    "    attribute_name=\"Fused_Attribute\",\n",
    "    attribute_label=\"地震融合属性值\",\n",
    "    real_wells=real_wells,\n",
    "    pseudo_wells=pseudo_wells,\n",
    "    target_column=\"Thickness of facies(1: Fine sand)\",\n",
    "    output_dir=output_dir,\n",
    "    filename_prefix=\"fused_attribute\",\n",
    "    class_thresholds=[0.1, 10],\n",
    "    figsize=(16, 14),\n",
    "    dpi=300,\n",
    "    cmap=\"viridis\",\n",
    "    point_size=140,\n",
    "    well_size=200,\n",
    ")\n",
    "\n",
    "print(\"融合属性分布与井点位置可视化完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e3208c",
   "metadata": {},
   "source": [
    "## SVR、随机森林（限制树深）、XGBoost（控制学习率和复杂度）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c370044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 组合真实样本和伪样本进行建模\n",
    "\n",
    "# 设置随机种子以确保结果可复现\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"======== 真实井点与虚拟井点结合建模 ========\")\n",
    "\n",
    "# 步骤1: 准备数据 - 合并真实样本和伪样本\n",
    "# 读取虚拟井数据\n",
    "pseudo_wells = pd.read_csv(os.path.join(output_dir, \"consistent_pseudo_wells.csv\"))\n",
    "print(f\"加载了 {len(pseudo_wells)} 个虚拟井点数据\")\n",
    "\n",
    "# 创建用于建模的真实井点数据\n",
    "real_wells = data_H6_2_well_selected.copy()\n",
    "print(f\"真实井点数据数量: {len(real_wells)}\")\n",
    "\n",
    "# 确定共同的特征列（选用之前筛选出的最佳特征）\n",
    "common_features = selected_features.copy()\n",
    "print(f\"使用的特征: {common_features}\")\n",
    "\n",
    "# 确保真实井点和虚拟井点都有这些特征列\n",
    "real_wells_valid = real_wells.dropna(subset=common_features + [\"Thickness of facies(1: Fine sand)\"])\n",
    "print(f\"有效真实井点数据数量: {len(real_wells_valid)} (丢弃了缺失值)\")\n",
    "\n",
    "# 创建合并数据集\n",
    "# 从真实井点中提取特征和目标\n",
    "X_real = real_wells_valid[common_features].values\n",
    "y_real = real_wells_valid[\"Thickness of facies(1: Fine sand)\"].values\n",
    "\n",
    "# 从虚拟井点中提取特征和目标(使用平均预测作为虚拟井的砂厚)\n",
    "X_pseudo = pseudo_wells[common_features].values\n",
    "y_pseudo = pseudo_wells[\"Mean_Pred\"].values  # 使用平均预测作为目标值\n",
    "\n",
    "# 合并数据\n",
    "X_combined = np.vstack((X_real, X_pseudo))\n",
    "y_combined = np.concatenate((y_real, y_pseudo))\n",
    "print(f\"合并后的样本数量: {len(X_combined)}\")\n",
    "\n",
    "# 创建样本权重 - 给真实样本更高的权重\n",
    "sample_weights = np.ones(len(X_combined))\n",
    "# 设置真实样本权重为虚拟样本的2倍\n",
    "sample_weights[: len(X_real)] = 2.0\n",
    "\n",
    "# 步骤2: 划分训练集和测试集(仅使用真实样本做测试)\n",
    "# 首先划分真实样本\n",
    "X_real_train, X_real_test, y_real_train, y_real_test = train_test_split(X_real, y_real, test_size=0.3, random_state=42)\n",
    "\n",
    "# 合并真实训练样本和所有虚拟样本作为完整训练集\n",
    "X_train = np.vstack((X_real_train, X_pseudo))\n",
    "y_train = np.concatenate((y_real_train, y_pseudo))\n",
    "\n",
    "# 创建训练样本权重\n",
    "train_weights = np.ones(len(X_train))\n",
    "# 设置真实样本权重为虚拟样本的2倍\n",
    "train_weights[: len(X_real_train)] = 2.0\n",
    "\n",
    "print(f\"训练集大小: {len(X_train)}, 测试集大小: {len(X_real_test)}\")\n",
    "\n",
    "# 标准化特征\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_real_test)\n",
    "\n",
    "\n",
    "# 步骤3: 模型训练与评估\n",
    "# 创建评估函数\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name, sample_weights=None):\n",
    "    \"\"\"评估模型性能并返回指标\"\"\"\n",
    "    # 训练模型\n",
    "    if sample_weights is not None:\n",
    "        model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    # 在测试集上预测\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 计算指标\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\n{model_name} 模型评估:\")\n",
    "    print(f\"  - RMSE: {rmse:.4f}\")\n",
    "    print(f\"  - R²: {r2:.4f}\")\n",
    "    print(f\"  - MAE: {mae:.4f}\")\n",
    "\n",
    "    # 绘制真实值vs预测值散点图\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], \"r--\")\n",
    "    plt.xlabel(\"真实值\")\n",
    "    plt.ylabel(\"预测值\")\n",
    "    plt.title(f\"{model_name}: 真实值 vs 预测值\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(\n",
    "        os.path.join(output_dir, f\"{model_name.lower().replace(' ', '_')}_pred_vs_true.png\"),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    return {\"model\": model, \"rmse\": rmse, \"r2\": r2, \"mae\": mae, \"y_pred\": y_pred}\n",
    "\n",
    "\n",
    "# 模型1: 支持向量回归(SVR)\n",
    "print(\"\\n训练SVR模型...\")\n",
    "# SVR对特征数量敏感，使用较少特征以避免维度灾难\n",
    "# 如果特征数量较多，可以考虑使用前2-3个最重要的特征\n",
    "svr_features = min(3, len(common_features))\n",
    "X_train_svr = X_train_scaled[:, :svr_features]\n",
    "X_test_svr = X_test_scaled[:, :svr_features]\n",
    "\n",
    "# 设置SVR参数网格\n",
    "param_grid_svr = {\"C\": [0.1, 1, 10], \"gamma\": [\"scale\", 0.01, 0.1], \"epsilon\": [0.1, 0.2], \"kernel\": [\"rbf\"]}\n",
    "\n",
    "# 创建并训练SVR模型\n",
    "svr = SVR()\n",
    "grid_search_svr = GridSearchCV(svr, param_grid_svr, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "grid_search_svr.fit(X_train_svr, y_train, sample_weight=train_weights)\n",
    "best_svr = grid_search_svr.best_estimator_\n",
    "\n",
    "print(f\"SVR最佳参数: {grid_search_svr.best_params_}\")\n",
    "svr_results = evaluate_model(best_svr, X_train_svr, y_train, X_test_svr, y_real_test, \"SVR\", train_weights)\n",
    "\n",
    "# 模型2: 随机森林(RandomForest) - 限制树深以避免过拟合\n",
    "print(\"\\n训练随机森林模型...\")\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"min_samples_leaf\": [3, 5],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "grid_search_rf.fit(X_train_scaled, y_train, sample_weight=train_weights)\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "print(f\"随机森林最佳参数: {grid_search_rf.best_params_}\")\n",
    "rf_results = evaluate_model(best_rf, X_train_scaled, y_train, X_test_scaled, y_real_test, \"随机森林\", train_weights)\n",
    "\n",
    "# 模型3: XGBoost - 控制学习率和复杂度\n",
    "print(\"\\n训练XGBoost模型...\")\n",
    "param_grid_xgb = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"subsample\": [0.8],\n",
    "    \"colsample_bytree\": [0.8],\n",
    "    \"min_child_weight\": [3, 5],\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "grid_search_xgb = GridSearchCV(xgb, param_grid_xgb, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "grid_search_xgb.fit(X_train_scaled, y_train, sample_weight=train_weights)\n",
    "best_xgb = grid_search_xgb.best_estimator_\n",
    "\n",
    "print(f\"XGBoost最佳参数: {grid_search_xgb.best_params_}\")\n",
    "xgb_results = evaluate_model(best_xgb, X_train_scaled, y_train, X_test_scaled, y_real_test, \"XGBoost\", train_weights)\n",
    "\n",
    "# 步骤4: 比较模型性能\n",
    "models_comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"模型\": [\"SVR\", \"随机森林\", \"XGBoost\"],\n",
    "        \"RMSE\": [svr_results[\"rmse\"], rf_results[\"rmse\"], xgb_results[\"rmse\"]],\n",
    "        \"R²\": [svr_results[\"r2\"], rf_results[\"r2\"], xgb_results[\"r2\"]],\n",
    "        \"MAE\": [svr_results[\"mae\"], rf_results[\"mae\"], xgb_results[\"mae\"]],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n模型性能比较:\")\n",
    "print(models_comparison)\n",
    "\n",
    "# 保存比较结果\n",
    "models_comparison.to_csv(os.path.join(output_dir, \"model_comparison.csv\"), index=False)\n",
    "\n",
    "# 选择最佳模型(基于R²)\n",
    "best_model_idx = models_comparison[\"R²\"].idxmax()\n",
    "best_model_name = models_comparison.loc[best_model_idx, \"模型\"]\n",
    "print(f\"\\n最佳模型: {best_model_name}\")\n",
    "\n",
    "# 为地震数据创建预测\n",
    "print(\"\\n使用最佳模型为整个工区生成预测...\")\n",
    "\n",
    "# 准备地震数据\n",
    "seismic_data = data_H6_2_attr_filtered.copy()\n",
    "X_seismic = seismic_data[common_features].fillna(seismic_data[common_features].mean())\n",
    "\n",
    "# 标准化特征\n",
    "X_seismic_scaled = scaler.transform(X_seismic)\n",
    "\n",
    "# 根据最佳模型选择预测方法\n",
    "if best_model_name == \"SVR\":\n",
    "    X_seismic_model = X_seismic_scaled[:, :svr_features]\n",
    "    predictions = best_svr.predict(X_seismic_model)\n",
    "    best_model = best_svr\n",
    "elif best_model_name == \"随机森林\":\n",
    "    predictions = best_rf.predict(X_seismic_scaled)\n",
    "    best_model = best_rf\n",
    "else:  # XGBoost\n",
    "    predictions = best_xgb.predict(X_seismic_scaled)\n",
    "    best_model = best_xgb\n",
    "\n",
    "# 将预测结果添加到地震数据\n",
    "seismic_data[\"Predicted_Sand_Thickness\"] = predictions\n",
    "\n",
    "# 将负值预测设为0\n",
    "if (predictions < 0).any():\n",
    "    neg_count = (predictions < 0).sum()\n",
    "    print(f\"注意: {neg_count} 个负的砂厚预测值已被替换为0\")\n",
    "    seismic_data[\"Predicted_Sand_Thickness\"] = seismic_data[\"Predicted_Sand_Thickness\"].clip(lower=0)\n",
    "\n",
    "# 保存预测结果\n",
    "seismic_data.to_csv(os.path.join(output_dir, \"seismic_with_predictions.csv\"), index=False)\n",
    "\n",
    "# 步骤5: 可视化预测结果\n",
    "print(\"\\n可视化预测结果...\")\n",
    "\n",
    "# 使用最佳模型进行可视化\n",
    "visualize_attribute_map(\n",
    "    data_points=seismic_data,\n",
    "    attribute_name=\"Predicted_Sand_Thickness\",\n",
    "    attribute_label=f\"砂厚预测值(米) - {best_model_name}模型\",\n",
    "    real_wells=real_wells_valid,\n",
    "    pseudo_wells=None,\n",
    "    target_column=\"Thickness of facies(1: Fine sand)\",\n",
    "    output_dir=output_dir,\n",
    "    filename_prefix=f\"predicted_sand_thickness_{best_model_name.lower()}\",\n",
    "    class_thresholds=[0.1, 10],  # 分类阈值：低值(<0.1)、中值(0.1-10)、高值(>10)\n",
    "    figsize=(16, 14),\n",
    "    dpi=300,\n",
    "    cmap=\"viridis\",\n",
    "    point_size=150,\n",
    "    well_size=200,\n",
    ")\n",
    "\n",
    "# 可选：特征重要性分析（对于树模型）\n",
    "if best_model_name in [\"随机森林\", \"XGBoost\"]:\n",
    "    print(\"\\n分析特征重要性...\")\n",
    "\n",
    "    # 获取特征重要性\n",
    "    if best_model_name == \"随机森林\":\n",
    "        importances = best_rf.feature_importances_\n",
    "    else:\n",
    "        importances = best_xgb.feature_importances_\n",
    "\n",
    "    # 创建特征重要性DataFrame\n",
    "    feature_importance_df = pd.DataFrame({\"特征\": common_features, \"重要性\": importances})\n",
    "    feature_importance_df = feature_importance_df.sort_values(\"重要性\", ascending=False)\n",
    "\n",
    "    # 绘制特征重要性\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance_df[\"特征\"], feature_importance_df[\"重要性\"])\n",
    "    plt.xlabel(\"特征重要性\")\n",
    "    plt.title(f\"{best_model_name}模型特征重要性\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(output_dir, f\"{best_model_name.lower()}_feature_importance.png\"), dpi=300, bbox_inches=\"tight\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    # 保存特征重要性\n",
    "    feature_importance_df.to_csv(\n",
    "        os.path.join(output_dir, f\"{best_model_name.lower()}_feature_importance.csv\"), index=False\n",
    "    )\n",
    "\n",
    "print(\"\\n建模与预测完成。所有结果已保存到输出目录。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attr_fusion_20250428",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
