{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e11838ca",
   "metadata": {},
   "source": [
    "# GMM 测试\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cca1d22",
   "metadata": {},
   "source": [
    "## 导入数据 & 数据分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b9176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold, KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "output_dir = \"output\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = \"SimHei\"  # 黑体 SimHei 支持中文\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False  # 正常显示负号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68378e5",
   "metadata": {},
   "source": [
    "## 函数：解析表头 & 导入数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81729ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_attributes(file_path):\n",
    "    \"\"\"\n",
    "    从Petrel格式的地震属性文件中识别属性\n",
    "\n",
    "    参数:\n",
    "        file_path (str): Petrel文件的路径\n",
    "\n",
    "    返回:\n",
    "        tuple: (属性列表, END ATTRIBUTES所在行号)\n",
    "    \"\"\"\n",
    "    print(f\"正在识别文件属性: {file_path}\")\n",
    "\n",
    "    # 读取文件内容\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    attr_start = -1\n",
    "    attr_end = -1\n",
    "    attributes = []\n",
    "\n",
    "    # 查找属性部分的起始和结束位置\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip() == \"ATTRIBUTES\":\n",
    "            attr_start = i\n",
    "        elif line.strip() == \"END ATTRIBUTES\":\n",
    "            attr_end = i\n",
    "            break\n",
    "\n",
    "    print(f\"识别到 END ATTRIBUTES 位于第 {attr_end} 行\")\n",
    "\n",
    "    # 提取属性名称\n",
    "    if attr_start != -1 and attr_end != -1:\n",
    "        # 跳过ATTRIBUTES行和下一行的1,N\n",
    "        for i in range(attr_start + 2, attr_end):\n",
    "            line = lines[i].strip()\n",
    "            # 直接查找最后一个英文逗号，之后的内容即为属性名\n",
    "            last_comma_index = line.rfind(\",\")\n",
    "            if last_comma_index != -1:\n",
    "                attr_name = line[last_comma_index + 1 :].strip()\n",
    "                attributes.append(attr_name)\n",
    "\n",
    "    print(f\"识别到 {len(attributes)} 个属性:\")\n",
    "    for attr in attributes:\n",
    "        print(f\"  - {attr}\")\n",
    "\n",
    "    return attributes, attr_end\n",
    "\n",
    "\n",
    "def parse_petrel_file(file_path):\n",
    "    \"\"\"\n",
    "    解析Petrel格式的地震属性文件\n",
    "\n",
    "    参数:\n",
    "        file_path (str): Petrel文件的路径\n",
    "\n",
    "    返回:\n",
    "        pandas.DataFrame: 包含解析后数据的DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"正在解析文件: {file_path}\")\n",
    "\n",
    "    # 步骤1: 识别属性和确定跳过的行数\n",
    "    attributes, skip_rows = identify_attributes(file_path)\n",
    "\n",
    "    # 读取文件内容以分析数据结构\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # 步骤2: 解析数据列数\n",
    "    if skip_rows + 1 < len(lines):\n",
    "        first_data_line = lines[skip_rows + 1].strip()\n",
    "        data_columns = first_data_line.split()\n",
    "        num_columns = len(data_columns)\n",
    "        print(f\"解析到数据有 {num_columns} 列\")\n",
    "    else:\n",
    "        print(\"未找到数据行\")\n",
    "        return None\n",
    "\n",
    "    # 步骤3: 创建列名\n",
    "    column_names = [\"X\", \"Y\", \"Z\"]\n",
    "\n",
    "    # 添加中间的占位符列名\n",
    "    placeholder_count = num_columns - 3 - len(attributes)\n",
    "    for i in range(placeholder_count):\n",
    "        column_names.append(f\"placeholder{i + 1}\")\n",
    "\n",
    "    # 添加属性列名\n",
    "    column_names.extend(attributes)\n",
    "\n",
    "    print(f\"总列数: {num_columns}, 其中:\")\n",
    "    print(f\"  - 3 列为坐标 (X, Y, Z)\")\n",
    "    print(f\"  - {placeholder_count} 列为占位符\")\n",
    "    print(f\"  - {len(attributes)} 列为属性\")\n",
    "\n",
    "    # 读取数据\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            file_path,\n",
    "            delim_whitespace=True,\n",
    "            skiprows=skip_rows + 1,  # 跳过END ATTRIBUTES行及之前的所有行\n",
    "            names=column_names,\n",
    "            dtype=float,\n",
    "            engine=\"python\",\n",
    "        )\n",
    "        print(f\"成功读取数据，共 {len(df)} 行\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"读取数据时出错: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e19b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "H6_2_attr, _ = identify_attributes(\"../data/H6-2_attr\")\n",
    "\n",
    "data_H6_2_attr = parse_petrel_file(\"../data/H6-2_attr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc16d87",
   "metadata": {},
   "source": [
    "## 解析井震数据\n",
    "\n",
    "使用 xlsx / csv 数据，注意表名为 Sheet1，注意数据需包含表头\n",
    "\n",
    "请检查表头和下面代码中的 selected_columns 是否一致\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417fb8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_H6_2_well = \"../data/H6-2_well.xlsx\"\n",
    "data_H6_2_well = pd.read_excel(file_H6_2_well, sheet_name=\"Sheet1\")\n",
    "\n",
    "selected_columns = [\n",
    "    \"X\",\n",
    "    \"Y\",\n",
    "    \"Z\",\n",
    "    \"Well\",\n",
    "    \"Thickness of LITHOLOGIES(1: sand)\",\n",
    "    # \"LITHOLOGIES(1: sand)\",\n",
    "    # \"标签\",\n",
    "    *H6_2_attr,\n",
    "]\n",
    "\n",
    "# 只选择 Surface 为 H6-2 的行，并丢弃砂厚为 NaN 的行\n",
    "data_H6_2_well_selected = (\n",
    "    data_H6_2_well.loc[data_H6_2_well[\"Surface\"] == \"H6-2\", selected_columns]\n",
    "    .replace(-999, np.nan)\n",
    "    .dropna(subset=[\"Thickness of LITHOLOGIES(1: sand)\"])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a861cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_H6_2_well_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf543b98",
   "metadata": {},
   "source": [
    "## 函数：缺失值处理、异常值替换\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c771f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(data, attribute_columns, missing_values=[-999], verbose=True):\n",
    "    \"\"\"\n",
    "    预处理特征数据，包括缺失值处理、异常值替换和特征筛选\n",
    "\n",
    "    参数:\n",
    "        data (DataFrame): 包含特征的数据框\n",
    "        attribute_columns (list): 需要处理的特征列名列表\n",
    "        missing_values (list): 要替换为NaN的值列表，默认为[-999]\n",
    "        verbose (bool): 是否打印详细信息，默认为True\n",
    "\n",
    "    返回:\n",
    "        DataFrame: 处理后的特征数据框\n",
    "    \"\"\"\n",
    "    # 提取特征\n",
    "    features = data[attribute_columns].copy()\n",
    "\n",
    "    # 替换缺失值\n",
    "    for val in missing_values:\n",
    "        features = features.replace(val, np.nan)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"处理前特征: {features.shape}\")\n",
    "\n",
    "    # 检查缺失值情况\n",
    "    missing_per_column = features.isna().sum()\n",
    "    if verbose:\n",
    "        print(\"\\n每列缺失值数量:\")\n",
    "    missing_cols = []\n",
    "    for col, missing in missing_per_column.items():\n",
    "        missing_ratio = missing / len(features) * 100\n",
    "        if verbose:\n",
    "            print(f\"  - {col}: {missing} ({missing_ratio:.2f}%)\")\n",
    "\n",
    "        # 标记缺失率较高的列\n",
    "        if missing_ratio >= 89.9:\n",
    "            missing_cols.append(col)\n",
    "\n",
    "    if missing_cols:\n",
    "        if verbose:\n",
    "            print(f\"\\n删除以下全部缺失的列: {missing_cols}\")\n",
    "        features = features.drop(columns=missing_cols)\n",
    "\n",
    "    # 填充剩余列中的NaN值\n",
    "    # 对于每一列，如果均值是NaN，则使用0填充\n",
    "    for col in features.columns:\n",
    "        if pd.isna(features[col].mean()):\n",
    "            if verbose:\n",
    "                print(f\"列 '{col}' 的均值为NaN，使用0填充\")\n",
    "            features[col] = features[col].fillna(0)\n",
    "        else:\n",
    "            features[col] = features[col].fillna(features[col].mean())\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n清理并填充后的特征形状: {features.shape}\")\n",
    "\n",
    "    # 检查是否仍有NaN值\n",
    "    if features.isna().any().any():\n",
    "        if verbose:\n",
    "            print(\"警告：数据中仍然存在NaN值，将它们替换为0\")\n",
    "        features = features.fillna(0)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1360e3",
   "metadata": {},
   "source": [
    "## 函数：PCA 降维\n",
    "\n",
    "会失去可解释性，这是一次尝试。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef9953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pca_analysis(\n",
    "    data,\n",
    "    attribute_columns,\n",
    "    variance_threshold=0.95,\n",
    "    output_dir=\"output\",\n",
    "    missing_values=[-999],\n",
    "):\n",
    "    \"\"\"\n",
    "    对地震属性数据进行PCA降维分析\n",
    "\n",
    "    参数:\n",
    "        data (DataFrame): 包含地震属性的数据框\n",
    "        attribute_columns (list): 需要进行分析的属性列名列表\n",
    "        variance_threshold (float): PCA保留方差的阈值，默认为0.95\n",
    "        output_dir (str): 输出图表的目录，默认为\"output\"\n",
    "        missing_values (list): 要替换为NaN的值列表，默认为[-999]\n",
    "\n",
    "    返回:\n",
    "        dict: 包含PCA分析结果的字典\n",
    "    \"\"\"\n",
    "    print(\"======== PCA降维分析开始 ========\")\n",
    "    print(f\"数据集大小: {data.shape}\")\n",
    "\n",
    "    # 预处理特征\n",
    "    features = preprocess_features(data, attribute_columns, missing_values)\n",
    "\n",
    "    # 提取对应的坐标\n",
    "    coords_clean = data[[\"X\", \"Y\", \"Z\"]]\n",
    "\n",
    "    # 标准化\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    # 应用PCA\n",
    "    pca = PCA()\n",
    "    pca_result = pca.fit_transform(features_scaled)\n",
    "\n",
    "    # 计算累积解释方差比\n",
    "    explained_variance_ratio_cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "    # 确定需要保留的主成分数量\n",
    "    n_components = np.argmax(explained_variance_ratio_cumsum >= variance_threshold) + 1\n",
    "    print(f\"为保留至少{variance_threshold * 100}%的方差，需要保留{n_components}个主成分\")\n",
    "\n",
    "    # 绘制解释方差比\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(\n",
    "        range(1, len(pca.explained_variance_ratio_) + 1),\n",
    "        pca.explained_variance_ratio_,\n",
    "        alpha=0.8,\n",
    "        label=\"单个方差贡献\",\n",
    "    )\n",
    "    plt.step(\n",
    "        range(1, len(pca.explained_variance_ratio_) + 1),\n",
    "        explained_variance_ratio_cumsum,\n",
    "        where=\"mid\",\n",
    "        label=\"累积方差贡献\",\n",
    "    )\n",
    "    plt.axhline(\n",
    "        y=variance_threshold,\n",
    "        linestyle=\"--\",\n",
    "        color=\"r\",\n",
    "        label=f\"{variance_threshold * 100}%方差阈值\",\n",
    "    )\n",
    "    plt.axvline(\n",
    "        x=n_components,\n",
    "        linestyle=\"--\",\n",
    "        color=\"g\",\n",
    "        label=f\"选择的主成分数({n_components})\",\n",
    "    )\n",
    "    plt.title(\"PCA解释方差比\")\n",
    "    plt.xlabel(\"主成分数量\")\n",
    "    plt.ylabel(\"解释方差比\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\n",
    "        os.path.join(output_dir, \"pca_explained_variance.png\"),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    # 使用选择的主成分数重新应用PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    features_pca = pca.fit_transform(features_scaled)\n",
    "    print(f\"降维后的特征形状: {features_pca.shape}\")\n",
    "\n",
    "    # 打印每个主成分对应的原始特征贡献\n",
    "    component_contributions = pd.DataFrame(\n",
    "        pca.components_,\n",
    "        columns=features.columns,  # 注意这里使用的是已清理的列\n",
    "        index=[f\"PC{i + 1}\" for i in range(n_components)],\n",
    "    )\n",
    "    print(\"\\n主成分与原始特征的关系:\")\n",
    "    print(component_contributions)\n",
    "\n",
    "    print(\"======== PCA降维分析完成 ========\")\n",
    "\n",
    "    # 返回结果\n",
    "    return {\n",
    "        \"pca\": pca,\n",
    "        \"scaler\": scaler,\n",
    "        \"n_components\": n_components,\n",
    "        \"component_contributions\": component_contributions,\n",
    "        \"features_clean\": features,\n",
    "        \"features_pca\": features_pca,\n",
    "        \"coords_clean\": coords_clean,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cef1cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对地震属性数据进行PCA降维\n",
    "pca_results = perform_pca_analysis(\n",
    "    data=data_H6_2_attr,\n",
    "    attribute_columns=H6_2_attr,\n",
    "    variance_threshold=0.95,\n",
    "    output_dir=output_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9000f723",
   "metadata": {},
   "source": [
    "## 函数：GMM (Gaussian Mixture Model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714a0309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gmm_clusters(features_pca, max_clusters=10, output_dir=\"output\"):\n",
    "    \"\"\"\n",
    "    对不同聚类数的GMM模型进行评估，绘制BIC和AIC曲线\n",
    "\n",
    "    参数:\n",
    "        features_pca (ndarray): PCA降维后的特征矩阵\n",
    "        max_clusters (int): 最大聚类数量，默认为10\n",
    "        output_dir (str): 输出图表的目录，默认为\"output\"\n",
    "\n",
    "    返回:\n",
    "        dict: 包含评估结果的字典\n",
    "    \"\"\"\n",
    "    print(\"======== GMM聚类数评估开始 ========\")\n",
    "\n",
    "    # 确定最佳GMM聚类数\n",
    "    n_components_range = range(1, max_clusters + 1)\n",
    "    models = []\n",
    "    bic_scores = []\n",
    "    aic_scores = []\n",
    "\n",
    "    for n_comp in n_components_range:\n",
    "        # 训练GMM模型\n",
    "        gmm = GaussianMixture(\n",
    "            n_components=n_comp,\n",
    "            covariance_type=\"full\",\n",
    "            random_state=42,\n",
    "            n_init=10,  # 多次初始化以获得更稳定的结果\n",
    "        )\n",
    "        gmm.fit(features_pca)\n",
    "\n",
    "        # 保存模型和得分\n",
    "        models.append(gmm)\n",
    "        bic_scores.append(gmm.bic(features_pca))\n",
    "        aic_scores.append(gmm.aic(features_pca))\n",
    "        print(f\"聚类数量 {n_comp}: BIC = {bic_scores[-1]:.2f}, AIC = {aic_scores[-1]:.2f}\")\n",
    "\n",
    "    # 绘制BIC和AIC曲线\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(n_components_range, bic_scores, \"o-\", label=\"BIC\")\n",
    "    plt.axvline(\n",
    "        np.argmin(bic_scores) + 1,\n",
    "        linestyle=\"--\",\n",
    "        color=\"r\",\n",
    "        label=f\"最佳聚类数 = {np.argmin(bic_scores) + 1}\",\n",
    "    )\n",
    "    plt.xlabel(\"聚类数量\")\n",
    "    plt.ylabel(\"BIC分数\")\n",
    "    plt.title(\"BIC评分曲线\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(n_components_range, aic_scores, \"o-\", label=\"AIC\")\n",
    "    plt.axvline(\n",
    "        np.argmin(aic_scores) + 1,\n",
    "        linestyle=\"--\",\n",
    "        color=\"r\",\n",
    "        label=f\"最佳聚类数 = {np.argmin(aic_scores) + 1}\",\n",
    "    )\n",
    "    plt.xlabel(\"聚类数量\")\n",
    "    plt.ylabel(\"AIC分数\")\n",
    "    plt.title(\"AIC评分曲线\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"gmm_bic_aic.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # 计算BIC和AIC的变化率\n",
    "    if len(bic_scores) > 1:\n",
    "        bic_changes = np.diff(bic_scores) / np.array(bic_scores[:-1])\n",
    "        aic_changes = np.diff(aic_scores) / np.array(aic_scores[:-1])\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(range(2, max_clusters + 1), bic_changes, \"o-\", label=\"BIC变化率\")\n",
    "        plt.plot(range(2, max_clusters + 1), aic_changes, \"s-\", label=\"AIC变化率\")\n",
    "        plt.axhline(y=0, linestyle=\"--\", color=\"gray\")\n",
    "        plt.xlabel(\"聚类数量\")\n",
    "        plt.ylabel(\"相对变化率\")\n",
    "        plt.title(\"BIC和AIC相对变化率\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(\n",
    "            os.path.join(output_dir, \"gmm_bic_aic_changes.png\"),\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "        # 尝试找出变化率变化最大的点(可能的\"肘部\")\n",
    "        if len(bic_changes) > 1:\n",
    "            bic_elbow = np.argmax(np.abs(np.diff(bic_changes))) + 2\n",
    "            aic_elbow = np.argmax(np.abs(np.diff(aic_changes))) + 2\n",
    "            print(f\"BIC变化率最大变化点: 聚类数 = {bic_elbow}\")\n",
    "            print(f\"AIC变化率最大变化点: 聚类数 = {aic_elbow}\")\n",
    "\n",
    "    # 选择最佳模型（基于BIC）\n",
    "    best_n_components = np.argmin(bic_scores) + 1\n",
    "    best_gmm = models[best_n_components - 1]\n",
    "    print(f\"\\n基于BIC的最佳聚类数: {best_n_components}\")\n",
    "    print(f\"基于AIC的最佳聚类数: {np.argmin(aic_scores) + 1}\")\n",
    "\n",
    "    print(\"======== GMM聚类数评估完成 ========\")\n",
    "\n",
    "    # 返回结果\n",
    "    return {\n",
    "        \"models\": models,\n",
    "        \"bic_scores\": bic_scores,\n",
    "        \"aic_scores\": aic_scores,\n",
    "        \"best_n_components\": best_n_components,\n",
    "        \"best_gmm\": best_gmm,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先评估最佳聚类数\n",
    "# gmm_evaluation = evaluate_gmm_clusters(\n",
    "#     features_pca=pca_results[\"features_pca\"], max_clusters=10, output_dir=output_dir\n",
    "# )\n",
    "\n",
    "# # 使用不同的聚类数执行GMM聚类\n",
    "# # 根据BIC/AIC结果选择的最佳聚类数\n",
    "# best_n = gmm_evaluation[\"best_n_components\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca4626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_gmm_clustering(features_pca, coords, n_clusters, output_dir=\"output\", random_state=42, prefix=\"\"):\n",
    "    \"\"\"\n",
    "    执行指定聚类数的GMM聚类并可视化结果\n",
    "\n",
    "    参数:\n",
    "        features_pca (ndarray): PCA降维后的特征矩阵\n",
    "        coords (DataFrame): 对应的坐标数据\n",
    "        n_clusters (int): 聚类数量\n",
    "        output_dir (str): 输出图表的目录，默认为\"output\"\n",
    "        random_state (int): 随机种子，默认为42\n",
    "        prefix (str): 输出文件名前缀，默认为\"\"\n",
    "\n",
    "    返回:\n",
    "        dict: 包含GMM聚类结果的字典\n",
    "    \"\"\"\n",
    "    print(f\"======== 执行 {n_clusters} 聚类的GMM分析 ========\")\n",
    "\n",
    "    # 训练GMM模型\n",
    "    gmm = GaussianMixture(\n",
    "        n_components=n_clusters,\n",
    "        covariance_type=\"full\",\n",
    "        random_state=random_state,\n",
    "        n_init=10,  # 多次初始化以获得更稳定的结果\n",
    "    )\n",
    "    gmm.fit(features_pca)\n",
    "\n",
    "    # 获取聚类标签和概率\n",
    "    cluster_labels = gmm.predict(features_pca)\n",
    "    cluster_probs = gmm.predict_proba(features_pca)\n",
    "\n",
    "    # 将聚类结果添加到原始数据中\n",
    "    result_df = pd.DataFrame(\n",
    "        {\n",
    "            \"X\": coords[\"X\"],\n",
    "            \"Y\": coords[\"Y\"],\n",
    "            \"Z\": coords[\"Z\"],\n",
    "            \"Cluster\": cluster_labels,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 文件名前缀\n",
    "    file_prefix = f\"{prefix}_{n_clusters}_clusters_\" if prefix else f\"{n_clusters}_clusters_\"\n",
    "\n",
    "    # 可视化1: 空间分布\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(\n",
    "        result_df[\"X\"],\n",
    "        result_df[\"Y\"],\n",
    "        c=result_df[\"Cluster\"],\n",
    "        cmap=\"viridis\",\n",
    "        s=30,\n",
    "        alpha=0.8,\n",
    "        linewidths=0.5,\n",
    "    )\n",
    "    plt.colorbar(scatter, label=\"聚类\")\n",
    "    plt.title(f\"样本点的聚类分布 (聚类数={n_clusters})\")\n",
    "    plt.xlabel(\"X坐标\")\n",
    "    plt.ylabel(\"Y坐标\")\n",
    "    plt.savefig(\n",
    "        os.path.join(output_dir, f\"{file_prefix}gmm_spatial.png\"),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    # 可视化2: PCA投影\n",
    "    if features_pca.shape[1] >= 2:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        scatter = plt.scatter(\n",
    "            features_pca[:, 0],\n",
    "            features_pca[:, 1],\n",
    "            c=cluster_labels,\n",
    "            cmap=\"viridis\",\n",
    "            s=30,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "        plt.colorbar(scatter, label=\"聚类\")\n",
    "        plt.title(f\"PCA投影下的聚类分布 (聚类数={n_clusters})\")\n",
    "        plt.xlabel(\"主成分1\")\n",
    "        plt.ylabel(\"主成分2\")\n",
    "        plt.savefig(\n",
    "            os.path.join(output_dir, f\"{file_prefix}gmm_pca_projection.png\"),\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "    # 统计各聚类的样本数\n",
    "    cluster_counts = result_df[\"Cluster\"].value_counts().sort_index()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cluster_counts.plot(kind=\"bar\")\n",
    "    plt.title(f\"各聚类样本数量分布 (聚类数={n_clusters})\")\n",
    "    plt.xlabel(\"聚类\")\n",
    "    plt.ylabel(\"样本数量\")\n",
    "    plt.grid(axis=\"y\")\n",
    "    plt.savefig(\n",
    "        os.path.join(output_dir, f\"{file_prefix}gmm_cluster_counts.png\"),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n各聚类样本数量:\")\n",
    "    for cluster, count in cluster_counts.items():\n",
    "        print(f\"聚类 {cluster}: {count} 样本 ({count / len(result_df) * 100:.2f}%)\")\n",
    "\n",
    "    print(f\"======== {n_clusters} 聚类的GMM分析完成 ========\")\n",
    "\n",
    "    # 返回结果\n",
    "    return {\n",
    "        \"gmm\": gmm,\n",
    "        \"cluster_labels\": cluster_labels,\n",
    "        \"cluster_probs\": cluster_probs,\n",
    "        \"result_df\": result_df,\n",
    "        \"cluster_counts\": cluster_counts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c77b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_6 = perform_gmm_clustering(\n",
    "    features_pca=pca_results[\"features_pca\"],\n",
    "    coords=pca_results[\"coords_clean\"],\n",
    "    n_clusters=6,\n",
    "    output_dir=output_dir,\n",
    ")\n",
    "gmm_6[\"result_df\"].to_csv(os.path.join(output_dir, \"gmm_6_clusters.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70fb19b",
   "metadata": {},
   "source": [
    "## 函数：cluster 中心混合扩充样本（于 PCA 空间）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a65fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_samples_by_pca_mixing(\n",
    "    well_data,\n",
    "    pca_model,\n",
    "    scaler,\n",
    "    cluster_results,\n",
    "    attribute_columns,\n",
    "    target_column=\"Thickness of LITHOLOGIES(1: sand)\",\n",
    "    min_samples_per_cluster=3,\n",
    "    augmentation_factor=2.0,\n",
    "    min_target_per_cluster=5,\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    在PCA空间中使用聚类均值混合法生成伪样本，并反投影到原始特征空间\n",
    "\n",
    "    参数:\n",
    "        well_data (DataFrame): 井点数据，包含地震属性和目标变量\n",
    "        pca_model: 训练好的PCA模型\n",
    "        scaler: 训练好的标准化模型\n",
    "        cluster_results (dict): GMM聚类结果，包含cluster_labels和cluster_probs\n",
    "        attribute_columns (list): 用于聚类和生成的属性列名\n",
    "        target_column (str): 回归目标列名，默认为\"Thickness of LITHOLOGIES(1: sand)\"\n",
    "        min_samples_per_cluster (int): 一个聚类内的最小样本数，默认为3\n",
    "        augmentation_factor (float): 样本扩增倍数，默认为2.0\n",
    "        min_target_per_cluster (int): 每个聚类的最小目标样本数，默认为5\n",
    "        random_state (int): 随机种子，默认为42\n",
    "        verbose (bool): 是否打印详细信息，默认为True\n",
    "\n",
    "    返回:\n",
    "        DataFrame: 扩增后的数据集，包含原始样本和伪样本\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # 复制原始数据\n",
    "    well_data = well_data.copy()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"使用 {len(attribute_columns)} 个属性列进行样本扩增\")\n",
    "        print(f\"原始样本数: {len(well_data)}\")\n",
    "        print(f\"目标扩增倍数: {augmentation_factor}倍\")\n",
    "\n",
    "    # 提取属性和目标\n",
    "    X_orig = well_data[attribute_columns].values\n",
    "    y_orig = well_data[target_column].values\n",
    "\n",
    "    # 标准化原始数据\n",
    "    X_scaled = scaler.transform(X_orig)\n",
    "\n",
    "    # 降维到PCA空间\n",
    "    X_pca = pca_model.transform(X_scaled)\n",
    "\n",
    "    # 获取聚类标签和概率\n",
    "    cluster_labels = cluster_results[\"cluster_labels\"]\n",
    "\n",
    "    # 将聚类标签添加到原始数据中\n",
    "    well_data[\"Cluster\"] = cluster_labels\n",
    "\n",
    "    # 创建一个空的列表用于存储伪样本\n",
    "    synthetic_samples = []\n",
    "\n",
    "    # 统计每个聚类的样本数，并获取唯一的聚类标签\n",
    "    cluster_counts = well_data[\"Cluster\"].value_counts().to_dict()\n",
    "    unique_clusters = sorted(cluster_counts.keys())  # 排序后的唯一聚类标签列表\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n各聚类样本数量:\")\n",
    "        for cluster in unique_clusters:\n",
    "            print(f\"聚类 {cluster}: {cluster_counts[cluster]} 样本\")\n",
    "\n",
    "    # 对每个存在的聚类进行处理\n",
    "    for cluster_id in unique_clusters:\n",
    "        # 获取当前聚类的样本\n",
    "        cluster_mask = cluster_labels == cluster_id\n",
    "        cluster_data = well_data[cluster_mask]\n",
    "\n",
    "        # 如果聚类样本数为0，跳过（这应该不会发生，但保留为安全检查）\n",
    "        if len(cluster_data) == 0:\n",
    "            if verbose:\n",
    "                print(f\"聚类 {cluster_id} 没有样本，跳过\")\n",
    "            continue\n",
    "\n",
    "        # 获取PCA空间中的数据点\n",
    "        X_pca_cluster = X_pca[cluster_mask]\n",
    "\n",
    "        # 计算当前聚类在PCA空间中的均值\n",
    "        pca_cluster_mean = np.mean(X_pca_cluster, axis=0)\n",
    "\n",
    "        # 计算目标值均值和标准差，用于生成新的目标值\n",
    "        target_mean = cluster_data[target_column].mean()\n",
    "        target_std = cluster_data[target_column].std()\n",
    "\n",
    "        # 如果标准差为0或NaN（只有一个样本或所有样本相同），使用一个小的默认值\n",
    "        if np.isnan(target_std) or target_std < 1e-6:\n",
    "            target_std = 0.1 * target_mean if target_mean > 0 else 0.1\n",
    "\n",
    "        # 基于扩增倍数计算目标样本数\n",
    "        target_samples = max(\n",
    "            min_target_per_cluster,  # 最小目标样本数\n",
    "            int(len(cluster_data) * augmentation_factor),  # 基于倍数的目标样本数\n",
    "        )\n",
    "\n",
    "        # 计算需要生成的样本数\n",
    "        n_samples = max(0, target_samples - len(cluster_data))\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"\\n聚类 {cluster_id}: 原始样本数 = {len(cluster_data)}, 目标样本数 = {target_samples}, 需要生成 {n_samples} 个伪样本\"\n",
    "            )\n",
    "\n",
    "        # 如果聚类样本数少于最小阈值，采用特殊处理\n",
    "        if len(cluster_data) < min_samples_per_cluster:\n",
    "            if verbose:\n",
    "                print(f\"  - 聚类 {cluster_id} 样本数过少，使用高权重的均值进行混合\")\n",
    "\n",
    "            # 对于样本过少的聚类，生成样本时更偏向均值\n",
    "            mean_weight_range = (0.6, 0.9)  # 均值的权重范围，偏向更高权重\n",
    "        else:\n",
    "            # 对于样本充足的聚类，使用较为均衡的混合比例\n",
    "            mean_weight_range = (0.3, 0.7)  # 均值的权重范围\n",
    "\n",
    "        # 生成伪样本\n",
    "        for i in range(n_samples):\n",
    "            # 随机选择一个原始样本索引\n",
    "            orig_idx = np.random.choice(np.where(cluster_mask)[0])\n",
    "            orig_sample = well_data.iloc[orig_idx]\n",
    "\n",
    "            # 获取原始样本在PCA空间中的表示\n",
    "            pca_orig_sample = X_pca[orig_idx]\n",
    "\n",
    "            # 确定均值的混合权重\n",
    "            mean_weight = np.random.uniform(mean_weight_range[0], mean_weight_range[1])\n",
    "\n",
    "            # 在PCA空间中混合生成新样本\n",
    "            pca_new_sample = mean_weight * pca_cluster_mean + (1 - mean_weight) * pca_orig_sample\n",
    "\n",
    "            # 反投影回原始空间\n",
    "            new_sample_scaled = pca_model.inverse_transform(pca_new_sample.reshape(1, -1))\n",
    "            new_sample_orig = scaler.inverse_transform(new_sample_scaled).flatten()\n",
    "\n",
    "            # 生成一个新的目标值，略微偏离原始样本的目标值\n",
    "            # 使用有约束的正态分布，确保目标值在合理范围内\n",
    "            orig_target = orig_sample[target_column]\n",
    "            new_target_delta = np.random.normal(0, 0.2 * target_std)\n",
    "\n",
    "            # 确保混合的目标值与原样本和聚类均值之间具有相似的关系\n",
    "            target_weight = mean_weight  # 可以使用与特征相同的权重，或单独设置\n",
    "            new_target_value = target_weight * target_mean + (1 - target_weight) * orig_target + new_target_delta\n",
    "\n",
    "            # 确保目标值非负（如果是砂厚等物理量）\n",
    "            new_target_value = max(0, new_target_value)\n",
    "\n",
    "            # 创建新样本字典\n",
    "            new_sample_dict = orig_sample.to_dict()\n",
    "            for j, col in enumerate(attribute_columns):\n",
    "                new_sample_dict[col] = new_sample_orig[j]\n",
    "            new_sample_dict[target_column] = new_target_value\n",
    "            new_sample_dict[\"Is_Synthetic\"] = 1\n",
    "            new_sample_dict[\"Cluster\"] = cluster_id  # 确保聚类标签也被添加到扩充数据中\n",
    "\n",
    "            synthetic_samples.append(new_sample_dict)\n",
    "\n",
    "    # 创建伪样本的DataFrame\n",
    "    if synthetic_samples:\n",
    "        synthetic_df = pd.DataFrame(synthetic_samples)\n",
    "\n",
    "        # 确保原始数据中有Is_Synthetic列\n",
    "        well_data[\"Is_Synthetic\"] = 0  # 标记为原始样本\n",
    "\n",
    "        # 合并原始数据和伪样本\n",
    "        augmented_data = pd.concat([well_data, synthetic_df], ignore_index=True)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n扩增后的样本数: {len(augmented_data)} (原始: {len(well_data)}, 合成: {len(synthetic_df)})\")\n",
    "            print(f\"扩增比例: {len(augmented_data) / len(well_data):.2f}倍\")\n",
    "    else:\n",
    "        well_data[\"Is_Synthetic\"] = 0\n",
    "        augmented_data = well_data\n",
    "        if verbose:\n",
    "            print(\"\\n没有生成伪样本\")\n",
    "\n",
    "    return augmented_data\n",
    "\n",
    "\n",
    "def encode_cluster_features(data, cluster_column=\"Cluster\", drop_original=True, prefix=\"Cluster_\"):\n",
    "    \"\"\"\n",
    "    对数据中的聚类标签进行One-Hot编码，并将编码后的特征添加到原始数据中\n",
    "\n",
    "    参数:\n",
    "        data (DataFrame): 包含聚类标签的数据框\n",
    "        cluster_column (str): 聚类标签所在的列名，默认为\"Cluster\"\n",
    "        drop_original (bool): 是否删除原始聚类列，默认为True\n",
    "        prefix (str): One-Hot编码列的前缀，默认为\"Cluster_\"\n",
    "\n",
    "    返回:\n",
    "        DataFrame: 添加了One-Hot编码特征的数据框\n",
    "    \"\"\"\n",
    "    if cluster_column not in data.columns:\n",
    "        raise ValueError(f\"列 '{cluster_column}' 不存在于数据中\")\n",
    "\n",
    "    # 复制原始数据，避免修改原始数据\n",
    "    result_df = data.copy()\n",
    "\n",
    "    # 获取唯一的聚类标签\n",
    "    unique_clusters = sorted(result_df[cluster_column].unique())\n",
    "\n",
    "    print(f\"对'{cluster_column}'列进行One-Hot编码，发现{len(unique_clusters)}个唯一的聚类标签\")\n",
    "\n",
    "    # 使用pandas的get_dummies函数进行One-Hot编码\n",
    "    cluster_dummies = pd.get_dummies(result_df[cluster_column], prefix=prefix)\n",
    "\n",
    "    # 合并One-Hot编码结果到原始数据\n",
    "    result_df = pd.concat([result_df, cluster_dummies], axis=1)\n",
    "\n",
    "    # 如果需要，删除原始聚类列\n",
    "    if drop_original:\n",
    "        result_df = result_df.drop(columns=[cluster_column])\n",
    "        print(f\"已删除原始'{cluster_column}'列\")\n",
    "\n",
    "    print(f\"One-Hot编码后数据形状: {result_df.shape}，新增了{len(unique_clusters)}个特征列\")\n",
    "\n",
    "    # 展示部分编码后的特征名\n",
    "    encoded_cols = [col for col in result_df.columns if col.startswith(prefix)]\n",
    "    print(f\"编码后的特征列: {', '.join(encoded_cols)}\")\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ea6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对井点数据进行PCA降维和GMM聚类\n",
    "# 注意：这里使用pca_results中的PCA模型和标准化器，但我们需要重新应用到井点数据上\n",
    "\n",
    "# 确定属性列 - 必须与PCA中使用的列完全一致\n",
    "attribute_columns = pca_results[\"features_clean\"].columns.tolist()\n",
    "\n",
    "# 不要只提取特征列，而是保留所有需要的列\n",
    "X_well = data_H6_2_well_selected.copy()\n",
    "\n",
    "# 然后对特征列进行预处理\n",
    "for col in attribute_columns:\n",
    "    if pd.isna(X_well[col].mean()):\n",
    "        print(f\"列 '{col}' 的均值为NaN，使用0填充\")\n",
    "        X_well[col] = X_well[col].fillna(0)\n",
    "    else:\n",
    "        # 使用列均值填充\n",
    "        X_well[col] = X_well[col].fillna(X_well[col].mean())\n",
    "\n",
    "# 确保特征列没有剩余的NaN值\n",
    "if X_well[attribute_columns].isna().any().any():\n",
    "    print(\"警告：特征列中仍然存在NaN值，将它们替换为0\")\n",
    "    for col in attribute_columns:\n",
    "        X_well[col] = X_well[col].fillna(0)\n",
    "\n",
    "# 对井点数据进行标准化 - 只标准化特征列\n",
    "X_well_scaled = pca_results[\"scaler\"].transform(X_well[attribute_columns])\n",
    "\n",
    "# 将井点数据投影到PCA空间\n",
    "X_well_pca = pca_results[\"pca\"].transform(X_well_scaled)\n",
    "\n",
    "# 使用已有的GMM模型对井点数据进行聚类\n",
    "cluster_labels_well = gmm_6[\"gmm\"].predict(X_well_pca)\n",
    "cluster_probs_well = gmm_6[\"gmm\"].predict_proba(X_well_pca)\n",
    "\n",
    "# 创建井点数据的聚类结果字典\n",
    "well_cluster_results = {\n",
    "    \"cluster_labels\": cluster_labels_well,\n",
    "    \"cluster_probs\": cluster_probs_well,\n",
    "}\n",
    "\n",
    "# 使用PCA空间中的中心混合法生成伪样本\n",
    "augmented_data = augment_samples_by_pca_mixing(\n",
    "    well_data=X_well,\n",
    "    pca_model=pca_results[\"pca\"],\n",
    "    scaler=pca_results[\"scaler\"],\n",
    "    cluster_results=well_cluster_results,\n",
    "    attribute_columns=attribute_columns,\n",
    "    target_column=\"Thickness of LITHOLOGIES(1: sand)\",\n",
    "    min_samples_per_cluster=2,  # 考虑井数据更少，设置更小的阈值\n",
    "    augmentation_factor=2.0,  # 目标是原始样本数的2倍\n",
    "    min_target_per_cluster=5,  # 每个聚类至少5个样本\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 保存扩增后的数据\n",
    "augmented_data.to_csv(os.path.join(output_dir, \"augmented_well_data_pca.csv\"), index=False)\n",
    "\n",
    "# 可视化原始样本和合成样本\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(\n",
    "    augmented_data[\"X\"],\n",
    "    augmented_data[\"Y\"],\n",
    "    c=augmented_data[\"Cluster\"],\n",
    "    s=50 * (1 + augmented_data[\"Is_Synthetic\"]),  # 合成样本点更大\n",
    "    alpha=0.7,\n",
    "    cmap=\"viridis\",\n",
    "    linewidths=0.5 * augmented_data[\"Is_Synthetic\"],  # 合成样本有轮廓\n",
    ")\n",
    "plt.colorbar(scatter, label=\"聚类\")\n",
    "plt.title(\"原始样本和PCA空间中合成的样本空间分布\")\n",
    "plt.xlabel(\"X坐标\")\n",
    "plt.ylabel(\"Y坐标\")\n",
    "plt.savefig(\n",
    "    os.path.join(output_dir, \"pca_augmented_samples_spatial.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# 可视化目标值分布\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(\n",
    "    data=augmented_data,\n",
    "    x=\"Thickness of LITHOLOGIES(1: sand)\",\n",
    "    hue=\"Is_Synthetic\",\n",
    "    element=\"step\",\n",
    "    bins=15,\n",
    "    kde=True,\n",
    ")\n",
    "plt.title(\"原始样本和PCA空间中合成样本的目标值分布\")\n",
    "plt.xlabel(\"砂厚\")\n",
    "plt.savefig(\n",
    "    os.path.join(output_dir, \"pca_augmented_samples_target_dist.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c751e5bf",
   "metadata": {},
   "source": [
    "## 函数：属性间相关性分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be36f919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attribute_correlations(\n",
    "    features_df,\n",
    "    method=\"pearson\",\n",
    "    corr_threshold=0.85,\n",
    "    output_dir=\"output\",\n",
    "    figsize=(14, 12),\n",
    "):\n",
    "    \"\"\"\n",
    "    分析属性间的相关性，将高度相关的属性分组，并可视化相关性矩阵\n",
    "\n",
    "    参数:\n",
    "        features_df (DataFrame): 包含属性列的数据框\n",
    "        method (str): 相关系数计算方法，'pearson'或'spearman'，默认为'pearson'\n",
    "        corr_threshold (float): 相关性阈值，高于此值的属性被视为高度相关，默认为0.85\n",
    "        output_dir (str): 输出图像的目录，默认为\"output\"\n",
    "        figsize (tuple): 图像尺寸，默认为(14, 12)\n",
    "\n",
    "    返回:\n",
    "        list: 相关属性组列表，每组包含高度相关的属性\n",
    "    \"\"\"\n",
    "    print(f\"======== 属性相关性分析开始 (方法: {method}) ========\")\n",
    "    print(f\"属性数量: {features_df.shape[1]}\")\n",
    "\n",
    "    # 计算相关系数矩阵\n",
    "    corr_matrix = features_df.corr(method=method)\n",
    "\n",
    "    # 可视化相关系数矩阵\n",
    "    plt.figure(figsize=figsize)\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # 创建上三角掩码\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)  # 使用发散色调色板\n",
    "\n",
    "    sns.heatmap(\n",
    "        corr_matrix,\n",
    "        mask=mask,\n",
    "        cmap=cmap,\n",
    "        vmax=1.0,\n",
    "        vmin=-1.0,\n",
    "        center=0,\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        annot=False,  # 不显示具体数值，避免过于拥挤\n",
    "        cbar_kws={\"shrink\": 0.8},\n",
    "    )\n",
    "\n",
    "    plt.title(f\"属性相关性矩阵 ({method}相关系数)\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(output_dir, f\"attribute_correlation_{method}.png\"),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    # 提取相关属性组\n",
    "    # 为避免重复，我们只考虑上三角矩阵(不包括对角线)\n",
    "    n_attrs = len(corr_matrix.columns)\n",
    "    correlated_groups = []\n",
    "    processed_attrs = set()  # 用于跟踪已经分配到组的属性\n",
    "\n",
    "    # 对每个属性，寻找与其高度相关的其他属性\n",
    "    for i, attr1 in enumerate(corr_matrix.columns):\n",
    "        if attr1 in processed_attrs:\n",
    "            continue\n",
    "\n",
    "        # 初始化一个新组，包含当前属性\n",
    "        correlated_group = {attr1}\n",
    "        processed_attrs.add(attr1)\n",
    "\n",
    "        # 查找与当前属性高度相关的所有其他属性\n",
    "        for j, attr2 in enumerate(corr_matrix.columns):\n",
    "            if i != j and attr2 not in processed_attrs:\n",
    "                corr_value = abs(corr_matrix.iloc[i, j])\n",
    "                if corr_value >= corr_threshold:\n",
    "                    correlated_group.add(attr2)\n",
    "                    processed_attrs.add(attr2)\n",
    "\n",
    "        # 如果组中有多个属性（即找到了相关属性），则将其添加到结果列表\n",
    "        if len(correlated_group) > 1:\n",
    "            correlated_groups.append(correlated_group)\n",
    "\n",
    "    # 添加未分组的单个属性\n",
    "    for attr in corr_matrix.columns:\n",
    "        if attr not in processed_attrs:\n",
    "            correlated_groups.append({attr})\n",
    "            processed_attrs.add(attr)\n",
    "\n",
    "    # 输出相关属性组\n",
    "    print(\"\\n高度相关的属性组 (相关系数阈值 >= {}):\".format(corr_threshold))\n",
    "    for i, group in enumerate(correlated_groups):\n",
    "        if len(group) > 1:\n",
    "            print(f\"\\n组 {i + 1} (包含 {len(group)} 个属性):\")\n",
    "            for attr in group:\n",
    "                print(f\"  - {attr}\")\n",
    "\n",
    "    # 输出独立属性（未分组）\n",
    "    independent_attrs = [list(group)[0] for group in correlated_groups if len(group) == 1]\n",
    "    print(f\"\\n独立属性 ({len(independent_attrs)} 个):\")\n",
    "    for i, attr in enumerate(independent_attrs):\n",
    "        print(f\"  - {attr}\")\n",
    "\n",
    "    # 计算每组内部的平均相关系数，用于评估组内相关程度\n",
    "    if len(correlated_groups) > 0 and any(len(group) > 1 for group in correlated_groups):\n",
    "        print(\"\\n各组内部平均相关系数:\")\n",
    "        for i, group in enumerate(correlated_groups):\n",
    "            if len(group) > 1:\n",
    "                group_list = list(group)\n",
    "                group_corr = corr_matrix.loc[group_list, group_list].values\n",
    "                # 计算组内非对角线元素的平均绝对值\n",
    "                mean_corr = np.sum(np.abs(group_corr)) / (len(group_list) ** 2 - len(group_list))\n",
    "                print(f\"  - 组 {i + 1}: {mean_corr:.4f}\")\n",
    "\n",
    "    print(f\"======== 属性相关性分析完成 ========\")\n",
    "\n",
    "    return correlated_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04a129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析原始特征的相关性\n",
    "correlated_attribute_groups = analyze_attribute_correlations(\n",
    "    features_df=pca_results[\"features_clean\"],  # 使用PCA结果中的清理后特征\n",
    "    method=\"pearson\",  # 使用Pearson相关系数\n",
    "    corr_threshold=0.85,  # 相关性阈值设为0.85\n",
    "    output_dir=output_dir,  # 输出目录\n",
    "    figsize=(16, 14),  # 图像尺寸\n",
    ")\n",
    "\n",
    "# 输出每个组的特征数量统计\n",
    "group_sizes = [len(group) for group in correlated_attribute_groups]\n",
    "print(f\"\\n相关属性组统计:\")\n",
    "print(f\"  - 总组数: {len(correlated_attribute_groups)}\")\n",
    "print(f\"  - 最大组大小: {max(group_sizes)} 属性\")\n",
    "print(f\"  - 包含多个属性的组数: {sum(1 for size in group_sizes if size > 1)}\")\n",
    "print(f\"  - 平均组大小: {sum(group_sizes) / len(group_sizes):.2f} 属性\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af06e6df",
   "metadata": {},
   "source": [
    "## 按组进行 RF importance 分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b42996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_rf_importance_by_group(\n",
    "    well_data,\n",
    "    attribute_groups,\n",
    "    target_column=\"Thickness of LITHOLOGIES(1: sand)\",\n",
    "    top_n=1,\n",
    "    test_size=0.3,\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    output_dir=\"output\",\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    对每组属性进行随机森林重要性分析，选择每组中最重要的特征\n",
    "\n",
    "    参数:\n",
    "        well_data (DataFrame): 井点数据，包含地震属性和目标变量\n",
    "        attribute_groups (list): 属性组列表，每组是一个包含相关属性的集合\n",
    "        target_column (str): 目标变量列名，默认为\"Thickness of LITHOLOGIES(1: sand)\"\n",
    "        top_n (int): 从每组中选择的最重要特征数量，默认为1\n",
    "        test_size (float): 测试集比例，默认为0.3\n",
    "        n_estimators (int): 随机森林中的树数量，默认为100\n",
    "        random_state (int): 随机种子，默认为42\n",
    "        output_dir (str): 输出目录，默认为\"output\"\n",
    "        verbose (bool): 是否输出详细信息，默认为True\n",
    "\n",
    "    返回:\n",
    "        list: 所有组中选出的最重要特征列表\n",
    "    \"\"\"\n",
    "    print(\"======== 按组进行随机森林特征重要性分析 ========\")\n",
    "\n",
    "    # 确保目标列存在\n",
    "    if target_column not in well_data.columns:\n",
    "        print(f\"错误: 目标列 '{target_column}' 不存在于数据中\")\n",
    "        return []\n",
    "\n",
    "    # 准备目标变量\n",
    "    y = well_data[target_column].values\n",
    "\n",
    "    # 初始化结果存储\n",
    "    selected_features = []\n",
    "    all_importances = {}\n",
    "\n",
    "    # 对每组属性进行分析\n",
    "    for i, group in enumerate(attribute_groups):\n",
    "        group_list = list(group)\n",
    "\n",
    "        # 如果组中只有一个属性，直接添加到结果中\n",
    "        if len(group_list) == 1:\n",
    "            selected_features.append(group_list[0])\n",
    "            if verbose:\n",
    "                print(f\"\\n组 {i + 1} 只有一个属性: {group_list[0]}，直接添加到选择列表\")\n",
    "            continue\n",
    "\n",
    "        # 打印当前组信息\n",
    "        if verbose:\n",
    "            print(f\"\\n分析组 {i + 1} (包含 {len(group_list)} 个属性):\")\n",
    "            for attr in group_list:\n",
    "                print(f\"  - {attr}\")\n",
    "\n",
    "        # 提取当前组的特征\n",
    "        X_group = well_data[group_list].values\n",
    "\n",
    "        # 划分训练集和测试集\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_group, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        # 训练随机森林模型\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,  # 使用所有CPU核心\n",
    "        )\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        # 评估模型性能\n",
    "        y_pred = rf.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"  组 {i + 1} 模型性能: MSE = {mse:.4f}, R² = {r2:.4f}\")\n",
    "\n",
    "        # 获取特征重要性\n",
    "        importances = rf.feature_importances_\n",
    "        importance_dict = dict(zip(group_list, importances))\n",
    "\n",
    "        # 按重要性排序\n",
    "        sorted_importances = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # 保存每个属性的重要性\n",
    "        for attr, imp in sorted_importances:\n",
    "            all_importances[attr] = imp\n",
    "\n",
    "        # 输出该组属性的重要性\n",
    "        if verbose:\n",
    "            print(f\"  组 {i + 1} 特征重要性:\")\n",
    "            for attr, imp in sorted_importances:\n",
    "                print(f\"    - {attr}: {imp:.6f}\")\n",
    "\n",
    "        # 选择 top_n 个重要性最高的特征\n",
    "        top_features = [attr for attr, _ in sorted_importances[: min(top_n, len(sorted_importances))]]\n",
    "        selected_features.extend(top_features)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"  从组 {i + 1} 中选择的特征: {', '.join(top_features)}\")\n",
    "\n",
    "        # 可视化特征重要性\n",
    "        plt.figure(figsize=(10, max(6, len(group_list) * 0.5)))\n",
    "        y_pos = np.arange(len(group_list))\n",
    "\n",
    "        # 排序后的重要性和属性名\n",
    "        sorted_attrs = [attr for attr, _ in sorted_importances]\n",
    "        sorted_imps = [imp for _, imp in sorted_importances]\n",
    "\n",
    "        # 绘制条形图\n",
    "        bars = plt.barh(y_pos, sorted_imps, align=\"center\")\n",
    "\n",
    "        # 为top_n特征着色\n",
    "        for j in range(min(top_n, len(bars))):\n",
    "            bars[j].set_color(\"red\")\n",
    "\n",
    "        plt.yticks(y_pos, sorted_attrs)\n",
    "        plt.xlabel(\"特征重要性\")\n",
    "        plt.title(f\"组 {i + 1} 的特征重要性\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            os.path.join(output_dir, f\"group_{i + 1}_feature_importance.png\"),\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "    # 可视化所有选择的特征及其重要性\n",
    "    selected_importances = {feat: all_importances.get(feat, 0) for feat in selected_features}\n",
    "    sorted_selected = sorted(selected_importances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    plt.figure(figsize=(12, max(6, len(selected_features) * 0.4)))\n",
    "    y_pos = np.arange(len(selected_features))\n",
    "\n",
    "    # 排序后的重要性和属性名\n",
    "    sorted_selected_attrs = [attr for attr, _ in sorted_selected]\n",
    "    sorted_selected_imps = [imp for _, imp in sorted_selected]\n",
    "\n",
    "    # 绘制条形图\n",
    "    plt.barh(y_pos, sorted_selected_imps, align=\"center\")\n",
    "    plt.yticks(y_pos, sorted_selected_attrs)\n",
    "    plt.xlabel(\"特征重要性\")\n",
    "    plt.title(\"所有选择特征的重要性\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(output_dir, \"selected_features_importance.png\"),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    # 输出最终选择的特征\n",
    "    print(\"\\n最终选择的特征列表:\")\n",
    "    for i, feature in enumerate(sorted_selected_attrs):\n",
    "        print(f\"{i + 1}. {feature}: {selected_importances[feature]:.6f}\")\n",
    "\n",
    "    print(f\"\\n共选择了 {len(selected_features)} 个特征用于建模\")\n",
    "    print(\"======== 随机森林特征重要性分析完成 ========\")\n",
    "\n",
    "    return sorted_selected_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c1606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在井点数据上应用随机森林重要性分析\n",
    "# 使用原始井点数据，不使用扩增数据\n",
    "selected_features = analyze_rf_importance_by_group(\n",
    "    well_data=data_H6_2_well_selected,  # 原始井点数据\n",
    "    attribute_groups=correlated_attribute_groups,  # 前面生成的属性组\n",
    "    target_column=\"Thickness of LITHOLOGIES(1: sand)\",\n",
    "    top_n=1,  # 每组选择1个最重要的特征\n",
    "    test_size=0.3,\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    output_dir=output_dir,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55293ae",
   "metadata": {},
   "source": [
    "## 函数：SVR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6187b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_svr_regression_model(\n",
    "    data,\n",
    "    selected_features,\n",
    "    target_column=\"Thickness of LITHOLOGIES(1: sand)\",\n",
    "    include_cluster_probs=True,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    output_dir=\"output\",\n",
    "    threshold_zero=0.1,  # 小于此值视为0\n",
    "    n_splits=5,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    使用选定特征和SVR构建回归模型\n",
    "\n",
    "    参数:\n",
    "        data (DataFrame): 包含特征和目标变量的数据框\n",
    "        selected_features (list): 选定的特征列表\n",
    "        target_column (str): 目标变量列名\n",
    "        include_cluster_probs (bool): 是否包含聚类概率作为附加特征\n",
    "        test_size (float): 测试集比例\n",
    "        random_state (int): 随机种子\n",
    "        output_dir (str): 输出目录\n",
    "        threshold_zero (float): 砂厚小于此阈值视为0\n",
    "        n_splits (int): 交叉验证折数\n",
    "        verbose (bool): 是否输出详细信息\n",
    "\n",
    "    返回:\n",
    "        dict: 包含模型、预测结果和评估指标的字典\n",
    "    \"\"\"\n",
    "    print(\"======== SVR回归建模开始 ========\")\n",
    "\n",
    "    # 复制数据，避免修改原始数据\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    # 准备特征和目标\n",
    "    X = data_copy[selected_features].values\n",
    "    y = data_copy[target_column].values\n",
    "\n",
    "    # 创建一个布尔变量，表示目标是否为0（或近似为0）\n",
    "    y_is_zero = y <= threshold_zero\n",
    "\n",
    "    # 如果需要，添加聚类概率作为附加特征\n",
    "    if include_cluster_probs and \"cluster_probs\" in data_copy.columns:\n",
    "        if verbose:\n",
    "            print(\"添加聚类概率作为附加特征\")\n",
    "\n",
    "        # 假设cluster_probs是一个存储概率向量的列\n",
    "        # 我们需要将其转换为单独的列\n",
    "        cluster_probs = np.array(data_copy[\"cluster_probs\"].tolist())\n",
    "        X = np.hstack((X, cluster_probs))\n",
    "\n",
    "    # 使用分层采样进行训练/测试集划分\n",
    "    # 我们使用y_is_zero作为分层依据，确保\"砂厚=0\"和\"砂厚>0\"样本的比例一致\n",
    "    X_train, X_test, y_train, y_test, y_train_zero, y_test_zero = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        y_is_zero,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y_is_zero,  # 使用是否为0作为分层依据\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"训练集大小: {X_train.shape[0]} 样本\")\n",
    "        print(f\"测试集大小: {X_test.shape[0]} 样本\")\n",
    "        print(f\"训练集中砂厚为0的样本比例: {np.mean(y_train_zero):.2%}\")\n",
    "        print(f\"测试集中砂厚为0的样本比例: {np.mean(y_test_zero):.2%}\")\n",
    "\n",
    "    # 标准化特征\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 使用GridSearchCV优化SVR参数\n",
    "    # 定义参数网格\n",
    "    param_grid = {\n",
    "        \"C\": [0.1, 1, 10, 100],\n",
    "        \"epsilon\": [0.01, 0.1, 0.2, 0.5],\n",
    "        \"kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
    "        \"gamma\": [\"scale\", \"auto\", 0.1, 0.01],\n",
    "    }\n",
    "\n",
    "    # 创建SVR模型\n",
    "    svr = SVR()\n",
    "\n",
    "    # 创建分层K折交叉验证，保持每折中\"砂厚=0\"和\"砂厚>0\"样本的比例一致\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # 创建一个GroupKFold用于保持砂厚为0和非0的分组\n",
    "    # 首先创建分组标签\n",
    "    groups = np.zeros(len(y_train))\n",
    "    groups[y_train > threshold_zero] = 1  # 非零样本分为一组\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        svr,\n",
    "        param_grid,\n",
    "        cv=kfold,  # 使用普通KFold\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        n_jobs=-1,\n",
    "        verbose=2 if verbose else 0,\n",
    "    )\n",
    "\n",
    "    # 训练模型\n",
    "    print(\"\\n正在训练SVR模型并优化参数...\")\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # 获取最佳模型\n",
    "    best_svr = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n最佳SVR参数:\")\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"  - {param}: {value}\")\n",
    "\n",
    "    # 在测试集上进行预测\n",
    "    y_pred = best_svr.predict(X_test_scaled)\n",
    "\n",
    "    # 计算评估指标\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    print(\"\\n模型评估指标:\")\n",
    "    print(f\"  - R²: {r2:.4f}\")\n",
    "    print(f\"  - MAE: {mae:.4f}\")\n",
    "    print(f\"  - RMSE: {rmse:.4f}\")\n",
    "\n",
    "    # 计算在\"真实值=0\"的样本中预测是否也为0的准确率\n",
    "    y_pred_is_zero = y_pred <= threshold_zero\n",
    "    zero_accuracy = accuracy_score(y_test_zero, y_pred_is_zero)\n",
    "\n",
    "    # 计算更详细的指标\n",
    "    # 真实为0，预测为0的比例 (真阴性率)\n",
    "    true_negative_rate = np.sum((y_test <= threshold_zero) & (y_pred <= threshold_zero)) / np.sum(\n",
    "        y_test <= threshold_zero\n",
    "    )\n",
    "    # 真实不为0，预测不为0的比例 (真阳性率)\n",
    "    true_positive_rate = np.sum((y_test > threshold_zero) & (y_pred > threshold_zero)) / np.sum(y_test > threshold_zero)\n",
    "\n",
    "    print(f\"\\n砂厚判别能力 (阈值 = {threshold_zero}):\")\n",
    "    print(f\"  - 总体准确率: {zero_accuracy:.4f}\")\n",
    "    print(f\"  - 真实为0时预测为0的比例: {true_negative_rate:.4f}\")\n",
    "    print(f\"  - 真实不为0时预测不为0的比例: {true_positive_rate:.4f}\")\n",
    "\n",
    "    # 创建真值vs预测值散点图\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # 绘制散点图，区分\"砂厚=0\"和\"砂厚>0\"的样本\n",
    "    plt.scatter(\n",
    "        y_test[~y_test_zero],\n",
    "        y_pred[~y_test_zero],\n",
    "        c=\"blue\",\n",
    "        alpha=0.6,\n",
    "        label=\"砂厚 > 0\",\n",
    "    )\n",
    "    plt.scatter(y_test[y_test_zero], y_pred[y_test_zero], c=\"red\", alpha=0.6, label=\"砂厚 ≈ 0\")\n",
    "\n",
    "    # 绘制对角线 (y=x)\n",
    "    max_val = max(np.max(y_test), np.max(y_pred))\n",
    "    plt.plot([0, max_val], [0, max_val], \"k--\", label=\"理想预测\")\n",
    "\n",
    "    # 绘制阈值线\n",
    "    plt.axhline(y=threshold_zero, color=\"green\", linestyle=\":\", label=f\"阈值 = {threshold_zero}\")\n",
    "    plt.axvline(x=threshold_zero, color=\"green\", linestyle=\":\")\n",
    "\n",
    "    plt.xlabel(\"真实砂厚\")\n",
    "    plt.ylabel(\"预测砂厚\")\n",
    "    plt.title(\"SVR回归：真实值 vs 预测值\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # 保存图像\n",
    "    plt.savefig(os.path.join(output_dir, \"svr_true_vs_pred.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # 创建误差直方图\n",
    "    errors = y_pred - y_test\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(errors, bins=30, alpha=0.7, color=\"blue\")\n",
    "    plt.axvline(x=0, color=\"red\", linestyle=\"--\")\n",
    "    plt.xlabel(\"预测误差 (预测值 - 真实值)\")\n",
    "    plt.ylabel(\"频数\")\n",
    "    plt.title(\"SVR回归预测误差分布\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(\n",
    "        os.path.join(output_dir, \"svr_error_distribution.png\"),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    # 将结果保存到CSV文件\n",
    "    results_df = pd.DataFrame(\n",
    "        {\n",
    "            \"True_Value\": y_test,\n",
    "            \"Predicted_Value\": y_pred,\n",
    "            \"Is_Zero_True\": y_test_zero,\n",
    "            \"Is_Zero_Pred\": y_pred_is_zero,\n",
    "            \"Error\": errors,\n",
    "        }\n",
    "    )\n",
    "    results_df.to_csv(os.path.join(output_dir, \"svr_prediction_results.csv\"), index=False)\n",
    "\n",
    "    print(\"======== SVR回归建模完成 ========\")\n",
    "\n",
    "    # 返回结果字典\n",
    "    return {\n",
    "        \"model\": best_svr,\n",
    "        \"scaler\": scaler,\n",
    "        \"best_params\": best_params,\n",
    "        \"metrics\": {\n",
    "            \"r2\": r2,\n",
    "            \"mae\": mae,\n",
    "            \"rmse\": rmse,\n",
    "            \"zero_accuracy\": zero_accuracy,\n",
    "            \"true_negative_rate\": true_negative_rate,\n",
    "            \"true_positive_rate\": true_positive_rate,\n",
    "        },\n",
    "        \"predictions\": {\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred,\n",
    "            \"y_test_zero\": y_test_zero,\n",
    "            \"y_pred_zero\": y_pred_is_zero,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef524b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用选择的特征构建SVR回归模型\n",
    "svr_results = build_svr_regression_model(\n",
    "    data=augmented_data,  # 可以使用扩增后的数据\n",
    "    selected_features=selected_features,  # 使用之前按组选择的特征\n",
    "    target_column=\"Thickness of LITHOLOGIES(1: sand)\",\n",
    "    include_cluster_probs=False,  # 是否包含聚类概率（如果有）\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    output_dir=output_dir,\n",
    "    threshold_zero=0.1,  # 砂厚小于0.1视为0\n",
    "    n_splits=5,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 使用原始井点数据构建SVR回归模型\n",
    "# svr_results = build_svr_regression_model(\n",
    "#     data=data_H6_2_well_selected,  # 使用原始井点数据\n",
    "#     selected_features=selected_features,\n",
    "#     target_column=\"Thickness of LITHOLOGIES(1: sand)\",\n",
    "#     include_cluster_probs=False,\n",
    "#     test_size=0.3,\n",
    "#     random_state=42,\n",
    "#     output_dir=output_dir,\n",
    "#     threshold_zero=0.1,\n",
    "#     n_splits=5,\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "# 可视化最重要的参数影响\n",
    "if svr_results[\"best_params\"][\"kernel\"] == \"rbf\":\n",
    "    print(\"\\n分析C和gamma参数对模型性能的影响...\")\n",
    "\n",
    "    # 创建不同C和gamma值的参数网格\n",
    "    C_range = np.logspace(-1, 2, 4)\n",
    "    gamma_range = [\"scale\", \"auto\"] + list(np.logspace(-2, 0, 3))\n",
    "\n",
    "    # 为了简化，使用最佳epsilon\n",
    "    epsilon = svr_results[\"best_params\"][\"epsilon\"]\n",
    "\n",
    "    # 创建结果表格\n",
    "    param_scores = []\n",
    "\n",
    "    # 重新提取特征和目标变量\n",
    "    X = data_H6_2_well_selected[selected_features].values\n",
    "    y = data_H6_2_well_selected[\"Thickness of LITHOLOGIES(1: sand)\"].values\n",
    "\n",
    "    # 再次划分训练集和测试集（使用相同的随机种子确保一致性）\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # 标准化特征\n",
    "    scaler = svr_results[\"scaler\"]  # 使用已有的标准化器\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    for C in C_range:\n",
    "        for gamma in gamma_range:\n",
    "            if isinstance(gamma, str):\n",
    "                gamma_label = gamma\n",
    "            else:\n",
    "                gamma_label = f\"{gamma:.3f}\"\n",
    "\n",
    "            svr = SVR(kernel=\"rbf\", C=C, epsilon=epsilon, gamma=gamma)\n",
    "            svr.fit(X_train_scaled, y_train)\n",
    "            score = svr.score(X_test_scaled, y_test)\n",
    "            param_scores.append({\"C\": C, \"gamma\": gamma_label, \"R²\": score})\n",
    "\n",
    "    # 转换为DataFrame并打印\n",
    "    param_df = pd.DataFrame(param_scores)\n",
    "    param_df = param_df.pivot(index=\"C\", columns=\"gamma\", values=\"R²\")\n",
    "    print(\"\\nC和gamma参数对R²的影响:\")\n",
    "    print(param_df)\n",
    "\n",
    "    # 将参数影响保存到CSV\n",
    "    param_df.to_csv(os.path.join(output_dir, \"svr_parameter_influence.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658f8080",
   "metadata": {},
   "source": [
    "## 预测\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1193c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对整个地震数据区域进行预测\n",
    "print(\"======== 开始预测整个地震区域的砂厚 ========\")\n",
    "\n",
    "# 准备地震属性数据进行预测\n",
    "# 首先对属性进行预处理（同样的方式，确保一致性）\n",
    "seismic_features = preprocess_features(\n",
    "    data=data_H6_2_attr, attribute_columns=selected_features, missing_values=[-999], verbose=True\n",
    ")\n",
    "\n",
    "# 提取坐标信息用于最终结果输出\n",
    "seismic_coords = data_H6_2_attr[[\"X\", \"Y\", \"Z\"]].copy()\n",
    "\n",
    "# 只保留选定的特征\n",
    "seismic_features_selected = seismic_features[selected_features]\n",
    "\n",
    "# 将特征转换为numpy数组\n",
    "X_seismic = seismic_features_selected.values\n",
    "\n",
    "# 使用训练好的标准化器对特征进行标准化\n",
    "X_seismic_scaled = svr_results[\"scaler\"].transform(X_seismic)\n",
    "\n",
    "# 使用训练好的SVR模型进行预测\n",
    "print(f\"使用SVR模型预测{len(X_seismic)}个点的砂厚...\")\n",
    "sand_thickness_pred = svr_results[\"model\"].predict(X_seismic_scaled)\n",
    "\n",
    "# 创建预测结果DataFrame\n",
    "prediction_results = pd.DataFrame(\n",
    "    {\n",
    "        \"X\": seismic_coords[\"X\"],\n",
    "        \"Y\": seismic_coords[\"Y\"],\n",
    "        \"Z\": seismic_coords[\"Z\"],\n",
    "        \"Predicted_Sand_Thickness\": sand_thickness_pred,\n",
    "    }\n",
    ")\n",
    "\n",
    "# 为负值预测结果置为0（如果有）\n",
    "if (prediction_results[\"Predicted_Sand_Thickness\"] < 0).any():\n",
    "    neg_count = (prediction_results[\"Predicted_Sand_Thickness\"] < 0).sum()\n",
    "    print(f\"有{neg_count}个点的预测砂厚为负值，已将其置为0\")\n",
    "    prediction_results[\"Predicted_Sand_Thickness\"] = prediction_results[\"Predicted_Sand_Thickness\"].clip(lower=0)\n",
    "\n",
    "# 统计预测结果\n",
    "pred_zero_count = (prediction_results[\"Predicted_Sand_Thickness\"] <= 0.1).sum()\n",
    "pred_nonzero_count = (prediction_results[\"Predicted_Sand_Thickness\"] > 0.1).sum()\n",
    "pred_mean = prediction_results[\"Predicted_Sand_Thickness\"].mean()\n",
    "pred_max = prediction_results[\"Predicted_Sand_Thickness\"].max()\n",
    "\n",
    "print(\"\\n预测结果统计:\")\n",
    "print(f\"  - 总点数: {len(prediction_results)}\")\n",
    "print(f\"  - 预测砂厚≤0.1的点数: {pred_zero_count} ({pred_zero_count / len(prediction_results):.2%})\")\n",
    "print(f\"  - 预测砂厚>0.1的点数: {pred_nonzero_count} ({pred_nonzero_count / len(prediction_results):.2%})\")\n",
    "print(f\"  - 平均预测砂厚: {pred_mean:.4f}\")\n",
    "print(f\"  - 最大预测砂厚: {pred_max:.4f}\")\n",
    "\n",
    "# 保存预测结果到CSV文件\n",
    "prediction_file = os.path.join(output_dir, \"H6-2_predicted_sand_thickness.csv\")\n",
    "prediction_results.to_csv(prediction_file, index=False)\n",
    "print(f\"\\n预测结果已保存到文件: {prediction_file}\")\n",
    "\n",
    "# 可视化预测结果 - 散点图\n",
    "plt.figure(figsize=(14, 12))\n",
    "scatter = plt.scatter(\n",
    "    prediction_results[\"X\"],\n",
    "    prediction_results[\"Y\"],\n",
    "    c=prediction_results[\"Predicted_Sand_Thickness\"],\n",
    "    cmap=\"viridis\",\n",
    "    s=2,  # 点大小较小，因为点数可能很多\n",
    "    alpha=0.7,\n",
    "    vmin=0,\n",
    "    vmax=min(pred_max, pred_mean * 3),  # 限制颜色范围，便于观察\n",
    ")\n",
    "plt.colorbar(scatter, label=\"预测砂厚\")\n",
    "plt.title(\"H6-2层砂厚预测结果空间分布\")\n",
    "plt.xlabel(\"X坐标\")\n",
    "plt.ylabel(\"Y坐标\")\n",
    "plt.savefig(os.path.join(output_dir, \"H6-2_predicted_sand_thickness_map.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# 可视化预测结果 - 直方图\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(prediction_results[\"Predicted_Sand_Thickness\"], bins=50, alpha=0.7, color=\"blue\")\n",
    "plt.axvline(x=0.1, color=\"red\", linestyle=\"--\", label=\"阈值 = 0.1\")\n",
    "plt.xlabel(\"预测砂厚\")\n",
    "plt.ylabel(\"频数\")\n",
    "plt.title(\"H6-2层砂厚预测结果分布\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_dir, \"H6-2_predicted_sand_thickness_histogram.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# 导出为Petrel可读的XYZ格式文件\n",
    "petrel_output_file = os.path.join(output_dir, \"H6-2_predicted_sand_thickness_petrel.txt\")\n",
    "with open(petrel_output_file, \"w\") as f:\n",
    "    # 写入标题\n",
    "    f.write(\"# H6-2层砂厚预测结果\\n\")\n",
    "    f.write(\"# X Y Z Predicted_Sand_Thickness\\n\")\n",
    "\n",
    "    # 写入数据\n",
    "    for _, row in prediction_results.iterrows():\n",
    "        f.write(f\"{row['X']:.6f} {row['Y']:.6f} {row['Z']:.6f} {row['Predicted_Sand_Thickness']:.6f}\\n\")\n",
    "\n",
    "print(f\"预测结果已保存为Petrel可导入的XYZ格式: {petrel_output_file}\")\n",
    "\n",
    "print(\"======== 砂厚预测完成 ========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40f0a2",
   "metadata": {},
   "source": [
    "## 排查问题\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a75585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_svr(\n",
    "    train_data,\n",
    "    predict_data,\n",
    "    selected_features,\n",
    "    target_column=\"Thickness of LITHOLOGIES(1: sand)\",\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    output_dir=\"output\",\n",
    "    threshold_zero=0.1,\n",
    "    n_splits=5,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    SVR模型训练和预测的整合函数，包含详细日志用于问题诊断\n",
    "\n",
    "    参数:\n",
    "        train_data (DataFrame): 用于训练的井点数据\n",
    "        predict_data (DataFrame): 用于预测的地震数据\n",
    "        selected_features (list): 选定的特征列表\n",
    "        target_column (str): 目标变量列名\n",
    "        test_size (float): 测试集比例\n",
    "        random_state (int): 随机种子\n",
    "        output_dir (str): 输出目录\n",
    "        threshold_zero (float): 砂厚小于此阈值视为0\n",
    "        n_splits (int): 交叉验证折数\n",
    "        verbose (bool): 是否输出详细信息\n",
    "\n",
    "    返回:\n",
    "        dict: 包含训练结果、预测结果和诊断信息的字典\n",
    "    \"\"\"\n",
    "    print(\"======== SVR回归模型训练与预测开始 ========\")\n",
    "    print(f\"训练数据: {train_data.shape}, 预测数据: {predict_data.shape}\")\n",
    "    print(f\"选定特征 ({len(selected_features)}): {selected_features}\")\n",
    "\n",
    "    # 提取训练特征和目标\n",
    "    X_train_raw = train_data[selected_features].values\n",
    "    y_train_all = train_data[target_column].values\n",
    "\n",
    "    # 诊断日志：训练数据基本统计信息\n",
    "    print(\"\\n训练数据统计信息:\")\n",
    "    for i, feature in enumerate(selected_features):\n",
    "        feature_values = X_train_raw[:, i]\n",
    "        print(\n",
    "            f\"  - {feature}: 均值={np.mean(feature_values):.4f}, 标准差={np.std(feature_values):.4f}, \"\n",
    "            f\"最小值={np.min(feature_values):.4f}, 最大值={np.max(feature_values):.4f}\"\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\"  - 目标变量: 均值={np.mean(y_train_all):.4f}, 标准差={np.std(y_train_all):.4f}, \"\n",
    "        f\"最小值={np.min(y_train_all):.4f}, 最大值={np.max(y_train_all):.4f}\"\n",
    "    )\n",
    "\n",
    "    # 是否为0（或近似为0）的标记\n",
    "    y_is_zero = y_train_all <= threshold_zero\n",
    "    print(f\"  - 训练集中砂厚≤{threshold_zero}的样本比例: {np.mean(y_is_zero):.2%}\")\n",
    "\n",
    "    # 预处理预测数据特征\n",
    "    seismic_features = preprocess_features(\n",
    "        data=predict_data,\n",
    "        attribute_columns=selected_features,\n",
    "        missing_values=[-999],\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    # 只保留选定特征\n",
    "    X_seismic_raw = seismic_features[selected_features].values\n",
    "\n",
    "    # 诊断日志：预测数据基本统计信息\n",
    "    print(\"\\n预测数据统计信息:\")\n",
    "    for i, feature in enumerate(selected_features):\n",
    "        feature_values = X_seismic_raw[:, i]\n",
    "        print(\n",
    "            f\"  - {feature}: 均值={np.mean(feature_values):.4f}, 标准差={np.std(feature_values):.4f}, \"\n",
    "            f\"最小值={np.min(feature_values):.4f}, 最大值={np.max(feature_values):.4f}\"\n",
    "        )\n",
    "\n",
    "    # 训练/测试集划分\n",
    "    X_train, X_test, y_train, y_test, y_train_zero, y_test_zero = train_test_split(\n",
    "        X_train_raw,\n",
    "        y_train_all,\n",
    "        y_is_zero,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y_is_zero,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n训练集大小: {X_train.shape[0]} 样本, 测试集大小: {X_test.shape[0]} 样本\")\n",
    "\n",
    "    # 标准化特征\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 诊断日志：标准化后的统计信息\n",
    "    print(\"\\n标准化后的训练数据统计信息:\")\n",
    "    for i, feature in enumerate(selected_features):\n",
    "        feature_values = X_train_scaled[:, i]\n",
    "        print(\n",
    "            f\"  - {feature}: 均值={np.mean(feature_values):.4f}, 标准差={np.std(feature_values):.4f}, \"\n",
    "            f\"最小值={np.min(feature_values):.4f}, 最大值={np.max(feature_values):.4f}\"\n",
    "        )\n",
    "\n",
    "    # 标准化预测数据\n",
    "    X_seismic_scaled = scaler.transform(X_seismic_raw)\n",
    "\n",
    "    # 诊断日志：标准化后的预测数据\n",
    "    print(\"\\n标准化后的预测数据统计信息:\")\n",
    "    for i, feature in enumerate(selected_features):\n",
    "        feature_values = X_seismic_scaled[:, i]\n",
    "        print(\n",
    "            f\"  - {feature}: 均值={np.mean(feature_values):.4f}, 标准差={np.std(feature_values):.4f}, \"\n",
    "            f\"最小值={np.min(feature_values):.4f}, 最大值={np.max(feature_values):.4f}\"\n",
    "        )\n",
    "\n",
    "    # 定义参数网格\n",
    "    param_grid = {\n",
    "        \"C\": [0.1, 1, 10, 100],\n",
    "        \"epsilon\": [0.01, 0.1, 0.2, 0.5],\n",
    "        \"kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
    "        \"gamma\": [\"scale\", \"auto\", 0.1, 0.01],\n",
    "    }\n",
    "\n",
    "    # 创建SVR模型\n",
    "    svr = SVR()\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        svr,\n",
    "        param_grid,\n",
    "        cv=kfold,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        n_jobs=-1,\n",
    "        verbose=2 if verbose else 0,\n",
    "    )\n",
    "\n",
    "    # 训练模型\n",
    "    print(\"\\n正在训练SVR模型并优化参数...\")\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # 获取最佳模型\n",
    "    best_svr = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    print(\"\\n最佳SVR参数:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  - {param}: {value}\")\n",
    "\n",
    "    # 测试集上评估\n",
    "    y_pred_test = best_svr.predict(X_test_scaled)\n",
    "\n",
    "    # 计算测试集评估指标\n",
    "    r2 = r2_score(y_test, y_pred_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "    print(\"\\n测试集评估指标:\")\n",
    "    print(f\"  - R²: {r2:.4f}\")\n",
    "    print(f\"  - MAE: {mae:.4f}\")\n",
    "    print(f\"  - RMSE: {rmse:.4f}\")\n",
    "\n",
    "    # 使用最佳模型在训练集上预测（检查过拟合）\n",
    "    y_pred_train = best_svr.predict(X_train_scaled)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "\n",
    "    print(f\"\\n训练集 R²: {r2_train:.4f}（用于检查过拟合）\")\n",
    "\n",
    "    # 训练/测试集预测值统计\n",
    "    print(\"\\n训练集预测值统计:\")\n",
    "    print(f\"  - 均值: {np.mean(y_pred_train):.4f}\")\n",
    "    print(f\"  - 标准差: {np.std(y_pred_train):.4f}\")\n",
    "    print(f\"  - 最小值: {np.min(y_pred_train):.4f}\")\n",
    "    print(f\"  - 最大值: {np.max(y_pred_train):.4f}\")\n",
    "    print(f\"  - 预测为0的比例: {np.mean(y_pred_train <= threshold_zero):.2%}\")\n",
    "\n",
    "    print(\"\\n测试集预测值统计:\")\n",
    "    print(f\"  - 均值: {np.mean(y_pred_test):.4f}\")\n",
    "    print(f\"  - 标准差: {np.std(y_pred_test):.4f}\")\n",
    "    print(f\"  - 最小值: {np.min(y_pred_test):.4f}\")\n",
    "    print(f\"  - 最大值: {np.max(y_pred_test):.4f}\")\n",
    "    print(f\"  - 预测为0的比例: {np.mean(y_pred_test <= threshold_zero):.2%}\")\n",
    "\n",
    "    # 调试：检查特征对模型影响\n",
    "    if hasattr(best_svr, \"coef_\"):\n",
    "        # 线性核心有系数可供查看\n",
    "        print(\"\\n特征系数（重要性）:\")\n",
    "        for feature, coef in zip(selected_features, best_svr.coef_[0]):\n",
    "            print(f\"  - {feature}: {coef:.6f}\")\n",
    "\n",
    "    # 使用前10个样本进行预测，输出详细结果\n",
    "    print(\"\\n预测数据前10个样本的详细结果:\")\n",
    "    for i in range(min(10, X_seismic_scaled.shape[0])):\n",
    "        pred_value = best_svr.predict([X_seismic_scaled[i]])[0]\n",
    "        print(f\"样本 {i + 1}: 预测值 = {pred_value:.4f}\")\n",
    "        print(\"  特征值:\")\n",
    "        for j, feature in enumerate(selected_features):\n",
    "            print(f\"    - {feature}: 原始值={X_seismic_raw[i, j]:.4f}, 标准化值={X_seismic_scaled[i, j]:.4f}\")\n",
    "\n",
    "    # 对整个地震数据进行预测\n",
    "    print(f\"\\n使用SVR模型预测{len(X_seismic_raw)}个点的砂厚...\")\n",
    "    sand_thickness_pred = best_svr.predict(X_seismic_scaled)\n",
    "\n",
    "    # 预测值统计分析\n",
    "    unique_values = np.unique(sand_thickness_pred)\n",
    "    print(f\"预测结果中的唯一值数量: {len(unique_values)}\")\n",
    "    if len(unique_values) <= 10:\n",
    "        print(f\"所有唯一预测值: {unique_values}\")\n",
    "    else:\n",
    "        print(f\"前10个唯一预测值: {unique_values[:10]}\")\n",
    "\n",
    "    # 检查是否所有预测值都相同\n",
    "    if len(unique_values) == 1:\n",
    "        print(\"\\n警告: 所有点的预测值都是相同的!\")\n",
    "\n",
    "        # 尝试使用不同的模型参数预测一小部分数据\n",
    "        print(\"\\n尝试使用不同参数的SVR模型:\")\n",
    "        test_params = [\n",
    "            {\"kernel\": \"linear\", \"C\": 1.0, \"epsilon\": 0.1},\n",
    "            {\"kernel\": \"rbf\", \"C\": 10.0, \"gamma\": 0.01, \"epsilon\": 0.1},\n",
    "            {\"kernel\": \"poly\", \"C\": 1.0, \"degree\": 2, \"epsilon\": 0.1},\n",
    "        ]\n",
    "\n",
    "        for params in test_params:\n",
    "            test_svr = SVR(**params)\n",
    "            test_svr.fit(X_train_scaled, y_train)\n",
    "            test_pred = test_svr.predict(X_seismic_scaled[:100])  # 只预测前100个样本\n",
    "            unique_test = np.unique(test_pred)\n",
    "            print(\n",
    "                f\"  - SVR参数 {params}: 唯一值数量={len(unique_test)}, \"\n",
    "                f\"范围=[{np.min(test_pred):.4f}, {np.max(test_pred):.4f}]\"\n",
    "            )\n",
    "\n",
    "    # 创建预测结果DataFrame\n",
    "    prediction_results = pd.DataFrame(\n",
    "        {\n",
    "            \"X\": predict_data[\"X\"],\n",
    "            \"Y\": predict_data[\"Y\"],\n",
    "            \"Z\": predict_data[\"Z\"],\n",
    "            \"Predicted_Sand_Thickness\": sand_thickness_pred,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 为负值预测结果置为0（如果有）\n",
    "    if (prediction_results[\"Predicted_Sand_Thickness\"] < 0).any():\n",
    "        neg_count = (prediction_results[\"Predicted_Sand_Thickness\"] < 0).sum()\n",
    "        print(f\"有{neg_count}个点的预测砂厚为负值，已将其置为0\")\n",
    "        prediction_results[\"Predicted_Sand_Thickness\"] = prediction_results[\"Predicted_Sand_Thickness\"].clip(lower=0)\n",
    "\n",
    "    # 统计预测结果\n",
    "    pred_zero_count = (prediction_results[\"Predicted_Sand_Thickness\"] <= threshold_zero).sum()\n",
    "    pred_nonzero_count = (prediction_results[\"Predicted_Sand_Thickness\"] > threshold_zero).sum()\n",
    "    pred_mean = prediction_results[\"Predicted_Sand_Thickness\"].mean()\n",
    "    pred_max = prediction_results[\"Predicted_Sand_Thickness\"].max()\n",
    "\n",
    "    print(\"\\n预测结果统计:\")\n",
    "    print(f\"  - 总点数: {len(prediction_results)}\")\n",
    "    print(f\"  - 预测砂厚≤{threshold_zero}的点数: {pred_zero_count} ({pred_zero_count / len(prediction_results):.2%})\")\n",
    "    print(\n",
    "        f\"  - 预测砂厚>{threshold_zero}的点数: {pred_nonzero_count} ({pred_nonzero_count / len(prediction_results):.2%})\"\n",
    "    )\n",
    "    print(f\"  - 平均预测砂厚: {pred_mean:.4f}\")\n",
    "    print(f\"  - 最大预测砂厚: {pred_max:.4f}\")\n",
    "\n",
    "    # 保存预测结果到CSV文件\n",
    "    prediction_file = os.path.join(output_dir, \"H6-2_predicted_sand_thickness.csv\")\n",
    "    prediction_results.to_csv(prediction_file, index=False)\n",
    "    print(f\"\\n预测结果已保存到文件: {prediction_file}\")\n",
    "\n",
    "    print(\"======== SVR回归模型训练与预测完成 ========\")\n",
    "\n",
    "    return {\n",
    "        \"model\": best_svr,\n",
    "        \"best_params\": best_params,\n",
    "        \"scaler\": scaler,\n",
    "        \"metrics\": {\n",
    "            \"r2_test\": r2,\n",
    "            \"r2_train\": r2_train,\n",
    "            \"mae\": mae,\n",
    "            \"rmse\": rmse,\n",
    "        },\n",
    "        \"predictions\": {\n",
    "            \"train\": y_pred_train,\n",
    "            \"test\": y_pred_test,\n",
    "            \"seismic\": sand_thickness_pred,\n",
    "        },\n",
    "        \"prediction_results\": prediction_results,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db524dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 使用整合的函数进行SVR模型训练与预测\n",
    "# svr_analysis = train_and_predict_svr(\n",
    "#     train_data=data_H6_2_well_selected,  # 训练数据：井点数据\n",
    "#     predict_data=data_H6_2_attr,  # 预测数据：地震属性数据\n",
    "#     selected_features=selected_features,  # 选定的特征\n",
    "#     target_column=\"Thickness of LITHOLOGIES(1: sand)\",\n",
    "#     test_size=0.3,\n",
    "#     random_state=42,\n",
    "#     output_dir=output_dir,\n",
    "#     threshold_zero=0.1,\n",
    "#     n_splits=5,\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "# # 预测结果可视化部分保持不变\n",
    "# # 可视化预测结果 - 散点图\n",
    "# plt.figure(figsize=(14, 12))\n",
    "# scatter = plt.scatter(\n",
    "#     svr_analysis[\"prediction_results\"][\"X\"],\n",
    "#     svr_analysis[\"prediction_results\"][\"Y\"],\n",
    "#     c=svr_analysis[\"prediction_results\"][\"Predicted_Sand_Thickness\"],\n",
    "#     cmap=\"viridis\",\n",
    "#     s=2,  # 点大小较小，因为点数可能很多\n",
    "#     alpha=0.7,\n",
    "#     vmin=0,\n",
    "#     vmax=min(\n",
    "#         svr_analysis[\"prediction_results\"][\"Predicted_Sand_Thickness\"].max(),\n",
    "#         svr_analysis[\"prediction_results\"][\"Predicted_Sand_Thickness\"].mean() * 3,\n",
    "#     ),  # 限制颜色范围\n",
    "# )\n",
    "# plt.colorbar(scatter, label=\"预测砂厚\")\n",
    "# plt.title(\"H6-2层砂厚预测结果空间分布\")\n",
    "# plt.xlabel(\"X坐标\")\n",
    "# plt.ylabel(\"Y坐标\")\n",
    "# plt.savefig(\n",
    "#     os.path.join(output_dir, \"H6-2_predicted_sand_thickness_map.png\"),\n",
    "#     dpi=300,\n",
    "#     bbox_inches=\"tight\",\n",
    "# )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d149095",
   "metadata": {},
   "source": [
    "### 结论：训练和预测的数据，尺度不一致！\n",
    "\n",
    "上个月用的是尺度不一致的数据！训练数据（井点数据）和预测数据（地震属性数据）来自不同的数据处理流程或使用了不同的单位。在机器学习中，这种情况会导致模型在新数据上彻底失效。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attr_fusion_20250428",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
