{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6f1839e",
   "metadata": {},
   "source": [
    "# 井点属性自动提取 + 伪样本生成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb1d689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保src目录在Python路径中\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LassoCV, LinearRegression, RidgeCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "# 导入模块\n",
    "from src.data_utils import (\n",
    "    extract_seismic_attributes_for_wells,\n",
    "    extract_uniform_seismic_samples,\n",
    "    filter_anomalous_attributes,\n",
    "    filter_outlier_wells,\n",
    "    filter_seismic_by_wells,\n",
    "    identify_attributes,\n",
    "    parse_petrel_file,\n",
    "    preprocess_features,\n",
    ")\n",
    "from src.feature_selection import select_best_features\n",
    "from src.visualization import visualize_attribute_map\n",
    "\n",
    "data_dir = \"..\\\\data\"\n",
    "output_dir = \"output\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = \"SimHei\"  # 黑体 SimHei 支持中文\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False  # 正常显示负号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a288f2bd",
   "metadata": {},
   "source": [
    "## 导入地震数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_seismic_attr = parse_petrel_file(os.path.join(data_dir, \"H6-2_attr\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f5fc91",
   "metadata": {},
   "source": [
    "## 导入井点位置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4608109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_well_position = pd.read_excel(os.path.join(data_dir, \"well_without_attr.xlsx\"))\n",
    "\n",
    "# 选择对应层位的行，丢弃砂厚为 NaN 的行\n",
    "data_well_purpose_surface_position = (\n",
    "    data_well_position[data_well_position[\"Surface\"] == \"H6-2\"]\n",
    "    .replace(-999, np.nan)  # 将-999替换为NaN\n",
    "    .dropna(subset=[\"Sand Thickness\"])  # 丢弃砂厚为NaN的行\n",
    "    .reset_index(drop=True)  # 重置索引\n",
    ")\n",
    "data_well_purpose_surface_position.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6426e7",
   "metadata": {},
   "source": [
    "## 筛除离群井\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb175ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选离群井\n",
    "data_well_purpose_surface_filtered = filter_outlier_wells(data_well_purpose_surface_position, method=\"iqr\")\n",
    "\n",
    "# 显示筛选前后的井点数量\n",
    "print(f\"筛选前井点数量: {len(data_well_purpose_surface_position)}\")\n",
    "print(f\"筛选后井点数量: {len(data_well_purpose_surface_filtered)}\")\n",
    "\n",
    "# 可视化筛选前后的井点分布\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# 计算坐标范围（使用所有井点的数据来确定范围）\n",
    "x_min = data_well_purpose_surface_position[\"X\"].min()\n",
    "x_max = data_well_purpose_surface_position[\"X\"].max()\n",
    "y_min = data_well_purpose_surface_position[\"Y\"].min()\n",
    "y_max = data_well_purpose_surface_position[\"Y\"].max()\n",
    "\n",
    "# 可选：添加一些边距使图更美观\n",
    "margin = 0.05  # 5%的边距\n",
    "x_range = x_max - x_min\n",
    "y_range = y_max - y_min\n",
    "x_min -= x_range * margin\n",
    "x_max += x_range * margin\n",
    "y_min -= y_range * margin\n",
    "y_max += y_range * margin\n",
    "\n",
    "# 绘制筛选前的井点分布\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(data_well_purpose_surface_position[\"X\"], data_well_purpose_surface_position[\"Y\"], c=\"blue\")\n",
    "plt.title(\"筛选前井点分布\")\n",
    "plt.xlabel(\"X坐标\")\n",
    "plt.ylabel(\"Y坐标\")\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "# 绘制筛选后的井点分布\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(data_well_purpose_surface_filtered[\"X\"], data_well_purpose_surface_filtered[\"Y\"], c=\"red\")\n",
    "plt.title(\"筛选后井点分布\")\n",
    "plt.xlabel(\"X坐标\")\n",
    "plt.ylabel(\"Y坐标\")\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"well_filtering_comparison.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef27b05f",
   "metadata": {},
   "source": [
    "## 处理属性缺失值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b33e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先获取地震属性列表\n",
    "attribute_names, _ = identify_attributes(os.path.join(data_dir, \"H6-2_attr\"))\n",
    "\n",
    "# 使用preprocess_features处理地震数据\n",
    "processed_seismic, attr_stats = preprocess_features(\n",
    "    data=data_seismic_attr,\n",
    "    attribute_columns=attribute_names,\n",
    "    missing_values=[-999],\n",
    "    missing_threshold=0.6,  # 缺失值超过60%的列将被删除\n",
    "    outlier_method=\"iqr\",\n",
    "    outlier_threshold=1.5,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 提取筛选后的属性\n",
    "attribute_names_filtered = [col for col in processed_seismic.columns]\n",
    "\n",
    "# 将处理后的属性数据与原始坐标数据合并\n",
    "processed_seismic_full = data_seismic_attr[[\"X\", \"Y\"]].copy()\n",
    "for col in processed_seismic.columns:\n",
    "    processed_seismic_full[col] = processed_seismic[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65f889d",
   "metadata": {},
   "source": [
    "## 根据井点分布，缩小工区范围\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a098a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 限制工区范围\n",
    "seismic_attr_filtered, area_bounds = filter_seismic_by_wells(\n",
    "    seismic_data=processed_seismic_full,\n",
    "    well_data=data_well_purpose_surface_filtered,\n",
    "    expansion_factor=1.5,  # 扩展50%\n",
    "    plot=True,\n",
    "    output_dir=output_dir,\n",
    ")\n",
    "\n",
    "# 后续可以直接使用area_bounds中的边界信息\n",
    "print(\"区域边界信息:\")\n",
    "for key, value in area_bounds.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011dd836",
   "metadata": {},
   "source": [
    "## 提取井点处地震属性\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8c65e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为筛选前的井点提取地震属性\n",
    "well_attr = extract_seismic_attributes_for_wells(\n",
    "    well_data=data_well_purpose_surface_position,\n",
    "    seismic_data=processed_seismic_full,\n",
    "    max_distance=50,\n",
    "    num_points=5,\n",
    ")\n",
    "\n",
    "# 为筛选后的井点提取地震属性\n",
    "well_attr_filtered = extract_seismic_attributes_for_wells(\n",
    "    well_data=data_well_purpose_surface_filtered, seismic_data=processed_seismic_full, max_distance=50, num_points=5\n",
    ")\n",
    "\n",
    "# 保存处理结果\n",
    "well_attr.to_excel(os.path.join(data_dir, \"wells_attr.xlsx\"), index=False)\n",
    "print(\"筛选前井点的地震属性已保存到 wells_attr.xlsx\")\n",
    "well_attr_filtered.to_excel(os.path.join(data_dir, \"wells_attr_filtered.xlsx\"), index=False)\n",
    "print(\"筛选后井点的地震属性已保存到 wells_attr_filtered.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240a7d50",
   "metadata": {},
   "source": [
    "## 生成统计摘要\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ead7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选出质量良好的属性\n",
    "good_attributes, anomalous_attributes, attribute_stats = filter_anomalous_attributes(\n",
    "    seismic_data=seismic_attr_filtered,\n",
    "    well_data=well_attr_filtered,\n",
    "    common_attributes=attribute_names_filtered,\n",
    "    ratio_threshold=5.0,  # 均值比值阈值\n",
    "    range_ratio_threshold=10.0,  # 数值范围比值阈值\n",
    "    std_ratio_threshold=10.0,  # 标准差比值阈值\n",
    "    output_dir=None,  # 输出图表目录\n",
    "    verbose=True,  # 打印详细信息\n",
    ")\n",
    "\n",
    "print(\"\\n筛选后保留的质量良好属性:\")\n",
    "for attr in good_attributes:\n",
    "    print(f\"- {attr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2b75d7",
   "metadata": {},
   "source": [
    "## 随机森林重要性和相关性分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1e0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用随机森林评估特征重要性并移除冗余特征\n",
    "selected_features = select_best_features(\n",
    "    well_data=well_attr_filtered,\n",
    "    attribute_columns=good_attributes,\n",
    "    target_column=\"Sand Thickness\",\n",
    "    n_features=3,\n",
    "    corr_threshold=0.85,\n",
    "    output_dir=output_dir,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 输出特征选择结果\n",
    "print(\"\\n基于随机森林重要性和相关性分析的最佳特征:\")\n",
    "for i, feature in enumerate(selected_features):\n",
    "    print(f\"{i + 1}. {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84e2806",
   "metadata": {},
   "source": [
    "## 提取样本，准备设置虚拟井\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8418b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用筛选后的地震数据区域提取等间距样本\n",
    "seismic_samples = extract_uniform_seismic_samples(\n",
    "    seismic_data=seismic_attr_filtered,\n",
    "    n_rows=40,\n",
    "    n_cols=40,\n",
    "    area_bounds=area_bounds,  # 直接传入边界字典\n",
    ")\n",
    "\n",
    "# 可视化真实井点和采样点\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 绘制地震数据点（使用抽样）\n",
    "sample_ratio = min(1.0, 5000 / len(seismic_attr_filtered))\n",
    "seismic_sample = seismic_attr_filtered.sample(frac=sample_ratio)\n",
    "plt.scatter(seismic_sample[\"X\"], seismic_sample[\"Y\"], color=\"lightgray\", alpha=0.3, s=10, label=\"地震数据(抽样)\")\n",
    "\n",
    "# 绘制真实井点位置\n",
    "plt.scatter(well_attr_filtered[\"X\"], well_attr_filtered[\"Y\"], color=\"red\", s=100, marker=\"^\", label=\"真实井点\")\n",
    "\n",
    "# 绘制等间距采样点位置\n",
    "plt.scatter(seismic_samples[\"X\"], seismic_samples[\"Y\"], color=\"blue\", s=50, marker=\"o\", label=\"等间距采样点\")\n",
    "\n",
    "# 添加标题和图例\n",
    "plt.title(\"真实井点与等间距采样点分布\", fontsize=16)\n",
    "plt.xlabel(\"X坐标\", fontsize=14)\n",
    "plt.ylabel(\"Y坐标\", fontsize=14)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# 保存图片\n",
    "plt.savefig(os.path.join(output_dir, \"real_wells_and_seismic_samples.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 保存提取的样本数据\n",
    "seismic_samples.to_csv(os.path.join(output_dir, \"seismic_samples.csv\"), index=False)\n",
    "print(f\"等间距地震样本数据已保存至 {os.path.join(output_dir, 'seismic_samples.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98260556",
   "metadata": {},
   "source": [
    "## 多线性模型一致性预测设置虚拟井\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c72cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建融合属性和多模型预测\n",
    "print(\"======== 创建融合属性和多模型预测 ========\")\n",
    "target_column = \"Sand Thickness\"\n",
    "min_corr_threshold = 0.3  # 最小相关性阈值，低于此值的属性将被排除\n",
    "\n",
    "# 准备训练数据\n",
    "X_labeled = well_attr_filtered[selected_features].values\n",
    "y_labeled = well_attr_filtered[target_column].values\n",
    "\n",
    "# 准备未标记数据（地震样本点）\n",
    "X_unlabeled = seismic_samples[selected_features].dropna().values\n",
    "unlabeled_indices = seismic_samples[selected_features].dropna().index\n",
    "\n",
    "# 1. 融合属性线性加权预测\n",
    "print(\"\\n=== 模型1: 融合属性线性加权 ===\")\n",
    "\n",
    "# 计算相关性权重\n",
    "correlation_weights = {}\n",
    "for i, feature in enumerate(selected_features):\n",
    "    corr, _ = pearsonr(well_attr_filtered[feature], well_attr_filtered[target_column])\n",
    "    if abs(corr) >= min_corr_threshold:\n",
    "        correlation_weights[feature] = corr\n",
    "        print(f\"属性 '{feature}' 与砂厚的 Pearson 相关性: {corr:.4f}\")\n",
    "    else:\n",
    "        print(f\"属性 '{feature}' 与砂厚的相关性过低 ({corr:.4f})，不纳入融合\")\n",
    "\n",
    "# 如果没有有效属性，使用所有属性且权重相等\n",
    "if len(correlation_weights) == 0:\n",
    "    print(\"警告: 没有属性满足相关性阈值，将使用所有属性且权重相等\")\n",
    "    for feature in selected_features:\n",
    "        correlation_weights[feature] = 1.0\n",
    "        print(f\"属性 '{feature}' 使用默认权重: 1.0\")\n",
    "\n",
    "# 标准化数据\n",
    "scaler = StandardScaler()\n",
    "X_labeled_scaled = scaler.fit_transform(X_labeled)\n",
    "X_unlabeled_scaled = scaler.transform(X_unlabeled)\n",
    "\n",
    "\n",
    "# 创建融合属性函数\n",
    "def create_fused_attribute(X_scaled, features, weights):\n",
    "    \"\"\"\n",
    "    基于选定特征和权重创建融合属性\n",
    "\n",
    "    参数:\n",
    "        X_scaled (ndarray): 标准化后的特征矩阵\n",
    "        features (list): 特征列表\n",
    "        weights (dict): 每个特征的权重\n",
    "\n",
    "    返回:\n",
    "        ndarray: 融合属性\n",
    "    \"\"\"\n",
    "    # 初始化融合属性\n",
    "    fused_attr = np.zeros(X_scaled.shape[0])\n",
    "    weight_sum = 0\n",
    "\n",
    "    # 对每个特征进行加权融合\n",
    "    for i, feature in enumerate(features):\n",
    "        if feature in weights:\n",
    "            weight = weights[feature]\n",
    "            fused_attr += X_scaled[:, i] * weight\n",
    "            weight_sum += abs(weight)\n",
    "\n",
    "    # 归一化融合结果\n",
    "    if weight_sum > 0:\n",
    "        fused_attr /= weight_sum\n",
    "\n",
    "    return fused_attr\n",
    "\n",
    "\n",
    "# 在训练数据上创建融合属性\n",
    "fused_attr_labeled = create_fused_attribute(X_labeled_scaled, selected_features, correlation_weights)\n",
    "\n",
    "# 在未标记数据上创建融合属性\n",
    "fused_attr_unlabeled = create_fused_attribute(X_unlabeled_scaled, selected_features, correlation_weights)\n",
    "\n",
    "# 使用融合属性拟合线性回归\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(fused_attr_labeled.reshape(-1, 1), y_labeled)\n",
    "\n",
    "# 预测\n",
    "fused_pred_labeled = linear_model.predict(fused_attr_labeled.reshape(-1, 1))\n",
    "fused_pred_unlabeled = linear_model.predict(fused_attr_unlabeled.reshape(-1, 1))\n",
    "\n",
    "# 评估融合属性预测效果\n",
    "fused_corr = np.corrcoef(fused_pred_labeled, y_labeled)[0, 1]\n",
    "print(f\"融合属性预测结果与真实砂厚的相关性: {fused_corr:.4f}\")\n",
    "\n",
    "# 2. Lasso回归 + Bootstrap\n",
    "print(\"\\n=== 模型2: LassoCV + Bootstrap ===\")\n",
    "\n",
    "n_bootstrap = 100  # Bootstrap重采样次数\n",
    "alpha_values = np.logspace(-4, 1, 30)  # alpha候选值\n",
    "\n",
    "# 主Lasso模型\n",
    "lasso_model = LassoCV(alphas=alpha_values, cv=5, max_iter=10000, tol=1e-3)\n",
    "lasso_model.fit(X_labeled_scaled, y_labeled)\n",
    "print(f\"Lasso最优alpha值: {lasso_model.alpha_:.6f}\")\n",
    "\n",
    "# 在训练数据上的预测\n",
    "lasso_pred_labeled = lasso_model.predict(X_labeled_scaled)\n",
    "lasso_corr = np.corrcoef(lasso_pred_labeled, y_labeled)[0, 1]\n",
    "print(f\"Lasso预测结果与真实砂厚的相关性: {lasso_corr:.4f}\")\n",
    "\n",
    "# Bootstrap重采样预测\n",
    "lasso_bootstrap_preds = np.zeros((n_bootstrap, X_unlabeled_scaled.shape[0]))\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    # Bootstrap重采样\n",
    "    indices = np.random.choice(len(X_labeled_scaled), len(X_labeled_scaled), replace=True)\n",
    "    X_boot, y_boot = X_labeled_scaled[indices], y_labeled[indices]\n",
    "\n",
    "    # 拟合模型\n",
    "    lasso_boot = LassoCV(alphas=alpha_values, cv=5, max_iter=10000, tol=1e-3)\n",
    "    lasso_boot.fit(X_boot, y_boot)\n",
    "\n",
    "    # 预测\n",
    "    lasso_bootstrap_preds[i, :] = lasso_boot.predict(X_unlabeled_scaled)\n",
    "\n",
    "# 计算预测均值和置信区间\n",
    "lasso_pred_unlabeled = np.mean(lasso_bootstrap_preds, axis=0)\n",
    "lasso_lower_ci = np.percentile(lasso_bootstrap_preds, 2.5, axis=0)\n",
    "lasso_upper_ci = np.percentile(lasso_bootstrap_preds, 97.5, axis=0)\n",
    "\n",
    "# 3. Ridge回归 + Bootstrap\n",
    "print(\"\\n=== 模型3: RidgeCV + Bootstrap ===\")\n",
    "\n",
    "alpha_values = np.logspace(-3, 3, 30)  # alpha候选值\n",
    "\n",
    "# 主Ridge模型\n",
    "ridge_model = RidgeCV(alphas=alpha_values, cv=5)\n",
    "ridge_model.fit(X_labeled_scaled, y_labeled)\n",
    "print(f\"Ridge最优alpha值: {ridge_model.alpha_:.6f}\")\n",
    "\n",
    "# 在训练数据上的预测\n",
    "ridge_pred_labeled = ridge_model.predict(X_labeled_scaled)\n",
    "ridge_corr = np.corrcoef(ridge_pred_labeled, y_labeled)[0, 1]\n",
    "print(f\"Ridge预测结果与真实砂厚的相关性: {ridge_corr:.4f}\")\n",
    "\n",
    "# Bootstrap重采样预测\n",
    "ridge_bootstrap_preds = np.zeros((n_bootstrap, X_unlabeled_scaled.shape[0]))\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    # Bootstrap重采样\n",
    "    indices = np.random.choice(len(X_labeled_scaled), len(X_labeled_scaled), replace=True)\n",
    "    X_boot, y_boot = X_labeled_scaled[indices], y_labeled[indices]\n",
    "\n",
    "    # 拟合模型\n",
    "    ridge_boot = RidgeCV(alphas=alpha_values, cv=5)\n",
    "    ridge_boot.fit(X_boot, y_boot)\n",
    "\n",
    "    # 预测\n",
    "    ridge_bootstrap_preds[i, :] = ridge_boot.predict(X_unlabeled_scaled)\n",
    "\n",
    "# 计算预测均值和置信区间\n",
    "ridge_pred_unlabeled = np.mean(ridge_bootstrap_preds, axis=0)\n",
    "ridge_lower_ci = np.percentile(ridge_bootstrap_preds, 2.5, axis=0)\n",
    "ridge_upper_ci = np.percentile(ridge_bootstrap_preds, 97.5, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa09ba1",
   "metadata": {},
   "source": [
    "## 平衡区间分布与质量控制的伪样本筛选\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9559f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quality_score(mean_pred, max_diff, rel_diff, lasso_std, ridge_std, thresholds):\n",
    "    \"\"\"\n",
    "    计算样本的质量分数（0-1，越高越好）\n",
    "\n",
    "    参数:\n",
    "        mean_pred: 平均预测值\n",
    "        max_diff: 最大模型差异\n",
    "        rel_diff: 相对差异\n",
    "        lasso_std: Lasso模型标准差\n",
    "        ridge_std: Ridge模型标准差\n",
    "        thresholds: 包含各阈值的字典\n",
    "    \"\"\"\n",
    "    # 基础权重\n",
    "    w_consistency = thresholds[\"w_consistency\"]  # 一致性权重\n",
    "    w_uncertainty = thresholds[\"w_uncertainty\"]  # 不确定性权重\n",
    "\n",
    "    # 计算一致性得分 (0-1)\n",
    "    abs_score = max(0, 1 - max_diff / thresholds[\"abs_threshold\"])\n",
    "    rel_score = max(0, 1 - rel_diff / thresholds[\"rel_threshold\"])\n",
    "    consistency_score = 0.5 * abs_score + 0.5 * rel_score\n",
    "\n",
    "    # 计算不确定性得分 (0-1)\n",
    "    lasso_std_score = max(0, 1 - lasso_std / thresholds[\"std_threshold\"])\n",
    "    ridge_std_score = max(0, 1 - ridge_std / thresholds[\"std_threshold\"])\n",
    "    uncertainty_score = 0.5 * lasso_std_score + 0.5 * ridge_std_score\n",
    "\n",
    "    # 组合得分\n",
    "    final_score = w_consistency * consistency_score + w_uncertainty * uncertainty_score\n",
    "\n",
    "    return final_score\n",
    "\n",
    "\n",
    "def calculate_j_score(selected_indices, quality_scores, mean_preds, target_dist, bins, thresholds):\n",
    "    \"\"\"计算当前选择的J评分\"\"\"\n",
    "    if len(selected_indices) == 0:\n",
    "        return 0\n",
    "\n",
    "    # 1. 计算质量分数\n",
    "    selected_quality = quality_scores[selected_indices]\n",
    "    Q_total = np.mean(selected_quality)\n",
    "\n",
    "    # 2. 计算分布相似度\n",
    "    selected_values = mean_preds[selected_indices]\n",
    "    hist, _ = np.histogram(selected_values, bins=bins)\n",
    "    current_dist = hist / (np.sum(hist) + 1e-10)\n",
    "\n",
    "    # Jensen-Shannon散度\n",
    "    from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "    jsd = jensenshannon(current_dist, target_dist)\n",
    "    dist_similarity = 1 - jsd\n",
    "\n",
    "    # 3. 计算样本数量得分\n",
    "    n_samples = len(selected_indices)\n",
    "    min_samples = 200\n",
    "    max_samples = 1000\n",
    "    quantity_score = min(1.0, max(0.0, (n_samples - min_samples) / (max_samples - min_samples)))\n",
    "\n",
    "    # 计算J值 - 使用动态权重\n",
    "    J = (\n",
    "        thresholds[\"j_quality_weight\"] * Q_total\n",
    "        + thresholds[\"j_dist_weight\"] * dist_similarity\n",
    "        + thresholds[\"j_quantity_weight\"] * quantity_score\n",
    "    )\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb7c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dynamic_quality_threshold(bin_idx, sample_ratio, base_threshold):\n",
    "    \"\"\"\n",
    "    基于区间特性和充足率的精细调整\n",
    "\n",
    "    参数:\n",
    "        bin_idx: 区间索引\n",
    "        sample_ratio: 区间的当前样本充足率\n",
    "        base_threshold: 当前基础质量阈值\n",
    "    \"\"\"\n",
    "    # 1. 区间特性调整\n",
    "    if bin_idx >= 2:  # 高砂厚区间对质量更加宽容\n",
    "        adjusted = max(0.3, base_threshold - 0.1)\n",
    "    else:\n",
    "        adjusted = base_threshold\n",
    "\n",
    "    # 2. 样本充足率调整\n",
    "    if sample_ratio < 0.3:  # 严重不足\n",
    "        return max(0.3, adjusted - 0.1)\n",
    "    elif sample_ratio < 0.6:  # 较为不足\n",
    "        return max(0.35, adjusted - 0.05)\n",
    "    else:  # 接近或超过目标\n",
    "        return adjusted\n",
    "\n",
    "\n",
    "def adjust_thresholds(thresholds, iteration, bin_coverage):\n",
    "    \"\"\"\n",
    "    根据迭代情况全局调整下一轮迭代的阈值\n",
    "\n",
    "    参数:\n",
    "        thresholds: 当前阈值字典\n",
    "        iteration: 当前迭代次数\n",
    "        bin_coverage: 各区间的样本覆盖率\n",
    "    \"\"\"\n",
    "    new_thresholds = thresholds.copy()\n",
    "\n",
    "    # 1. 根据迭代次数调整模型差异容忍度参数\n",
    "    iteration_factor = min(1.5, 1.0 + iteration * 0.05)  # 每次迭代增加5%，最多增加50%\n",
    "    new_thresholds[\"abs_threshold\"] *= iteration_factor\n",
    "    new_thresholds[\"rel_threshold\"] *= iteration_factor\n",
    "    new_thresholds[\"std_threshold\"] *= iteration_factor\n",
    "\n",
    "    # 2. 根据迭代次数调整基础质量阈值\n",
    "    # 随着迭代进行，逐渐降低基础质量要求\n",
    "    if iteration > 0:\n",
    "        new_quality = max(0.3, thresholds[\"min_quality\"] * (1 - iteration * 0.05))\n",
    "        new_thresholds[\"min_quality\"] = new_quality\n",
    "\n",
    "    # 3. 根据总体覆盖情况调整J评分权重\n",
    "    min_coverage = np.min(bin_coverage)\n",
    "    if min_coverage < 0.3:  # 覆盖率极低\n",
    "        # 降低分布权重，更关注质量和数量\n",
    "        new_thresholds[\"j_dist_weight\"] *= 0.85\n",
    "        new_thresholds[\"j_quality_weight\"] = 1 - new_thresholds[\"j_dist_weight\"] - new_thresholds[\"j_quantity_weight\"]\n",
    "\n",
    "    # 保证权重和为1\n",
    "    total_weight = (\n",
    "        new_thresholds[\"j_quality_weight\"] + new_thresholds[\"j_dist_weight\"] + new_thresholds[\"j_quantity_weight\"]\n",
    "    )\n",
    "    if total_weight != 1.0:\n",
    "        new_thresholds[\"j_quality_weight\"] /= total_weight\n",
    "        new_thresholds[\"j_dist_weight\"] /= total_weight\n",
    "        new_thresholds[\"j_quantity_weight\"] /= total_weight\n",
    "\n",
    "    # 4. 调整J评分容忍度\n",
    "    new_thresholds[\"j_tolerance\"] = min(0.1, 0.01 + iteration * 0.02)  # 每次迭代增加0.02，最大0.1\n",
    "\n",
    "    return new_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271600af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 平衡分布与质量控制的优化采样\n",
    "print(\"\\n=== 平衡分布与质量控制的优化采样 ===\")\n",
    "\n",
    "# 将三个模型的预测结果整合\n",
    "predictions = np.column_stack([fused_pred_unlabeled, lasso_pred_unlabeled, ridge_pred_unlabeled])\n",
    "\n",
    "# 计算每个点三个预测值的最大差异\n",
    "max_diffs = np.max(predictions, axis=1) - np.min(predictions, axis=1)\n",
    "\n",
    "# 计算平均预测值用于相对差异计算\n",
    "mean_preds = np.mean(predictions, axis=1)\n",
    "\n",
    "# 预先处理预测值，将负值替换为0\n",
    "mean_preds = np.maximum(mean_preds, 0)\n",
    "fused_pred_unlabeled = np.maximum(fused_pred_unlabeled, 0)\n",
    "lasso_pred_unlabeled = np.maximum(lasso_pred_unlabeled, 0)\n",
    "ridge_pred_unlabeled = np.maximum(ridge_pred_unlabeled, 0)\n",
    "\n",
    "# 计算bootstrap结果的标准差\n",
    "lasso_std = np.std(lasso_bootstrap_preds, axis=0)\n",
    "ridge_std = np.std(ridge_bootstrap_preds, axis=0)\n",
    "\n",
    "# 定义砂厚区间\n",
    "bins = [0, 0.1, 10, np.inf]\n",
    "bin_labels = [\"0-0.1\", \"0.1-10\", \">10\"]\n",
    "\n",
    "# 排除靠近真实井点且砂厚差异大的点\n",
    "print(\"\\n预处理：排除靠近真实井点且砂厚差异大的点...\")\n",
    "valid_unlabeled_mask = np.ones(len(unlabeled_indices), dtype=bool)\n",
    "proximity_radius = 100  # 米，设置排除半径\n",
    "max_thickness_diff = 0.1  # 米，最大允许砂厚差异\n",
    "\n",
    "# 获取真实井点的坐标和砂厚\n",
    "real_well_x = well_attr_filtered[\"X\"].values\n",
    "real_well_y = well_attr_filtered[\"Y\"].values\n",
    "real_well_thickness = well_attr_filtered[\"Sand Thickness\"].values\n",
    "\n",
    "# 获取未标记点的坐标\n",
    "unlabeled_x = seismic_samples.loc[unlabeled_indices, \"X\"].values\n",
    "unlabeled_y = seismic_samples.loc[unlabeled_indices, \"Y\"].values\n",
    "\n",
    "excluded_count = 0\n",
    "excluded_reasons = []\n",
    "\n",
    "# 逐个检查未标记点与真实井点的距离和砂厚差异\n",
    "for i in range(len(unlabeled_indices)):\n",
    "    for j in range(len(real_well_x)):\n",
    "        # 计算距离\n",
    "        distance = np.sqrt((unlabeled_x[i] - real_well_x[j]) ** 2 + (unlabeled_y[i] - real_well_y[j]) ** 2)\n",
    "\n",
    "        # 如果距离小于排除半径\n",
    "        if distance <= proximity_radius:\n",
    "            # 计算砂厚差异\n",
    "            thickness_diff = abs(mean_preds[i] - real_well_thickness[j])\n",
    "\n",
    "            # 如果差异超过阈值，排除该点\n",
    "            if thickness_diff > max_thickness_diff:\n",
    "                valid_unlabeled_mask[i] = False\n",
    "                excluded_count += 1\n",
    "                excluded_reasons.append(\n",
    "                    f\"点{i}(预测砂厚={mean_preds[i]:.2f})与井点{j}(真实砂厚={real_well_thickness[j]:.2f})距离{distance:.1f}米，差异{thickness_diff:.2f}米\"\n",
    "                )\n",
    "                break  # 一旦与任何真实井点冲突，就排除该点\n",
    "\n",
    "if excluded_count > 0:\n",
    "    print(f\"  排除了 {excluded_count} 个靠近真实井点且砂厚差异大的点\")\n",
    "    if excluded_count <= 10:  # 只显示少量详情以避免日志过长\n",
    "        for reason in excluded_reasons:\n",
    "            print(f\"  - {reason}\")\n",
    "    else:\n",
    "        print(f\"  - 显示前10个排除原因:\")\n",
    "        for reason in excluded_reasons[:10]:\n",
    "            print(f\"    {reason}\")\n",
    "\n",
    "# 更新有效的未标记索引和预测\n",
    "valid_unlabeled_indices = unlabeled_indices[valid_unlabeled_mask]\n",
    "valid_mean_preds = mean_preds[valid_unlabeled_mask]\n",
    "valid_max_diffs = max_diffs[valid_unlabeled_mask]\n",
    "valid_lasso_std = lasso_std[valid_unlabeled_mask]\n",
    "valid_ridge_std = ridge_std[valid_unlabeled_mask]\n",
    "valid_rel_diffs = valid_max_diffs / (valid_mean_preds + 1e-10)  # 避免除零\n",
    "\n",
    "print(f\"  预处理后剩余 {len(valid_unlabeled_indices)} 个有效点 (排除了 {excluded_count} 个点)\")\n",
    "\n",
    "# 将预测结果分配到各个区间\n",
    "bin_indices = []\n",
    "for i in range(len(bins) - 1):\n",
    "    mask = (valid_mean_preds >= bins[i]) & (valid_mean_preds < bins[i + 1])\n",
    "    bin_indices.append(np.where(mask)[0])\n",
    "\n",
    "# 计算各区间样本数量\n",
    "bin_counts = [len(indices) for indices in bin_indices]\n",
    "print(\"\\n各砂厚区间原始分布:\")\n",
    "for i, count in enumerate(bin_counts):\n",
    "    print(f\"  {bin_labels[i]}: {count}个 ({count / len(valid_mean_preds) * 100:.1f}%)\")\n",
    "\n",
    "# 设定目标分布\n",
    "target_distribution = np.array([0.4, 0.4, 0.20])\n",
    "\n",
    "# 目标样本总数（最小200个）\n",
    "target_total_samples = max(200, int(0.2 * len(valid_mean_preds)))\n",
    "min_samples_per_bin = 15  # 每个区间的最小样本数\n",
    "\n",
    "# 计算每个区间的目标样本数\n",
    "target_bin_samples = np.round(target_distribution * target_total_samples).astype(int)\n",
    "# 确保每个区间至少有最小样本数\n",
    "target_bin_samples = np.maximum(target_bin_samples, min_samples_per_bin)\n",
    "# 调整以匹配总目标\n",
    "target_bin_samples = np.round(target_bin_samples / sum(target_bin_samples) * target_total_samples).astype(int)\n",
    "\n",
    "print(\"\\n目标样本分布:\")\n",
    "for i, count in enumerate(target_bin_samples):\n",
    "    print(f\"  {bin_labels[i]}: 目标{count}个样本 ({count / sum(target_bin_samples) * 100:.1f}%)\")\n",
    "\n",
    "# 初始阈值设置\n",
    "base_thresholds = {\n",
    "    \"abs_threshold\": 3.0,  # 绝对差异阈值\n",
    "    \"rel_threshold\": 0.2,  # 相对差异阈值\n",
    "    \"std_threshold\": 2.5,  # 标准差阈值\n",
    "    \"w_consistency\": 0.6,  # 一致性权重\n",
    "    \"w_uncertainty\": 0.4,  # 不确定性权重\n",
    "    \"min_quality\": 0.6,  # 最低质量阈值\n",
    "    \"j_quality_weight\": 0.6,  # J评分中质量的权重\n",
    "    \"j_dist_weight\": 0.25,  # J评分中分布的权重\n",
    "    \"j_quantity_weight\": 0.15,  # J评分中数量的权重\n",
    "    \"j_tolerance\": 0.01,  # J评分容忍度\n",
    "}\n",
    "\n",
    "# 计算所有样本的初始质量分数\n",
    "quality_scores = np.zeros(len(valid_mean_preds))\n",
    "\n",
    "for i in range(len(valid_mean_preds)):\n",
    "    quality_scores[i] = calculate_quality_score(\n",
    "        valid_mean_preds[i],\n",
    "        valid_max_diffs[i],\n",
    "        valid_rel_diffs[i],\n",
    "        valid_lasso_std[i],\n",
    "        valid_ridge_std[i],\n",
    "        base_thresholds,\n",
    "    )\n",
    "\n",
    "# 迭代优化样本选择\n",
    "print(\"\\n开始迭代优化样本选择...\")\n",
    "\n",
    "# 初始化选择的样本\n",
    "selected_indices = np.array([], dtype=int)\n",
    "max_iterations = 10\n",
    "current_j = 0\n",
    "current_thresholds = base_thresholds.copy()\n",
    "min_iterations = 3  # 至少运行的迭代次数\n",
    "\n",
    "# 记录每次迭代的状态，用于调试和分析\n",
    "iteration_history = []\n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    print(f\"\\n迭代 {iteration + 1}/{max_iterations}\")\n",
    "\n",
    "    # 打印当前阈值\n",
    "    print(\"当前阈值设置:\")\n",
    "    for key, value in current_thresholds.items():\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "    # 1. 重新计算质量分数(使用当前阈值)\n",
    "    for i in range(len(valid_mean_preds)):\n",
    "        quality_scores[i] = calculate_quality_score(\n",
    "            valid_mean_preds[i],\n",
    "            valid_max_diffs[i],\n",
    "            valid_rel_diffs[i],\n",
    "            valid_lasso_std[i],\n",
    "            valid_ridge_std[i],\n",
    "            current_thresholds,\n",
    "        )\n",
    "\n",
    "    # 2. 计算当前各区间已选样本数\n",
    "    current_bin_samples = [0] * len(bin_labels)\n",
    "    if len(selected_indices) > 0:\n",
    "        selected_values = valid_mean_preds[selected_indices]\n",
    "        for i in range(len(bins) - 1):\n",
    "            current_bin_samples[i] = np.sum((selected_values >= bins[i]) & (selected_values < bins[i + 1]))\n",
    "\n",
    "    # 3. 计算各区间的覆盖率\n",
    "    bin_ratios = np.zeros(len(bins) - 1)\n",
    "    for i in range(len(bins) - 1):\n",
    "        bin_ratios[i] = current_bin_samples[i] / target_bin_samples[i] if target_bin_samples[i] > 0 else 1.0\n",
    "\n",
    "    # 按照样本充足率排序区间（优先处理最不足的区间）\n",
    "    bin_priority = np.argsort(bin_ratios)\n",
    "\n",
    "    # 记录本次迭代前的状态\n",
    "    pre_iteration_state = {\n",
    "        \"selected_count\": len(selected_indices),\n",
    "        \"j_score\": current_j,\n",
    "        \"bin_ratios\": bin_ratios.copy(),\n",
    "        \"thresholds\": current_thresholds.copy(),\n",
    "    }\n",
    "\n",
    "    improved = False\n",
    "\n",
    "    # 4. 逐个处理各区间\n",
    "    for bin_idx in bin_priority:\n",
    "        # 如果该区间已经足够，并且不是第一轮迭代，可以跳过\n",
    "        if bin_ratios[bin_idx] >= 1.0 and iteration > 0:\n",
    "            print(f\"  区间 {bin_labels[bin_idx]} 已达到目标覆盖率 ({bin_ratios[bin_idx] * 100:.1f}%)，跳过\")\n",
    "            continue\n",
    "\n",
    "        # 5. 动态调整质量阈值\n",
    "        quality_threshold = get_dynamic_quality_threshold(\n",
    "            bin_idx=bin_idx, sample_ratio=bin_ratios[bin_idx], base_threshold=current_thresholds[\"min_quality\"]\n",
    "        )\n",
    "\n",
    "        # 打印原始阈值和动态调整后的阈值，增强可理解性\n",
    "        print(\n",
    "            f\"  处理区间 {bin_labels[bin_idx]} - 原始质量阈值={current_thresholds['min_quality']:.2f}, \"\n",
    "            f\"动态调整后={quality_threshold:.2f}, 覆盖率={bin_ratios[bin_idx] * 100:.1f}%\"\n",
    "        )\n",
    "\n",
    "        # 6. 获取该区间的候选样本\n",
    "        candidates = bin_indices[bin_idx]\n",
    "\n",
    "        # 排除已选择的样本\n",
    "        candidates = np.setdiff1d(candidates, selected_indices)\n",
    "\n",
    "        if len(candidates) == 0:\n",
    "            print(f\"  区间 {bin_labels[bin_idx]} 没有更多可用样本\")\n",
    "            continue\n",
    "\n",
    "        # 7. 按质量排序\n",
    "        candidates = candidates[np.argsort(-quality_scores[candidates])]\n",
    "\n",
    "        # 8. 筛选符合质量要求的样本\n",
    "        qualified_candidates = candidates[quality_scores[candidates] >= quality_threshold]\n",
    "\n",
    "        if len(qualified_candidates) == 0:\n",
    "            print(f\"  区间 {bin_labels[bin_idx]} 没有符合质量要求(>{quality_threshold:.2f})的样本，跳过该区间\")\n",
    "            continue  # 不再尝试降低质量标准\n",
    "\n",
    "        # 9. 计算需要添加的样本数\n",
    "        needed = target_bin_samples[bin_idx] - current_bin_samples[bin_idx]\n",
    "        to_add = min(needed, len(qualified_candidates))\n",
    "\n",
    "        if to_add <= 0:\n",
    "            print(f\"  区间 {bin_labels[bin_idx]} 不需要添加更多样本\")\n",
    "            continue\n",
    "\n",
    "        print(f\"  尝试向区间 {bin_labels[bin_idx]} 添加 {to_add} 个样本 (质量阈值>{quality_threshold:.2f})\")\n",
    "\n",
    "        # 10. 添加样本并评估J值变化\n",
    "        new_selected = (\n",
    "            np.concatenate([selected_indices, qualified_candidates[:to_add]])\n",
    "            if len(selected_indices) > 0\n",
    "            else qualified_candidates[:to_add]\n",
    "        )\n",
    "\n",
    "        new_j = calculate_j_score(\n",
    "            new_selected, quality_scores, valid_mean_preds, target_distribution, bins, current_thresholds\n",
    "        )\n",
    "\n",
    "        # 11. 评估J值变化\n",
    "        j_tolerance = current_thresholds[\"j_tolerance\"]\n",
    "        j_change = new_j - current_j\n",
    "\n",
    "        # 如果J值提高或在容忍范围内，接受新样本\n",
    "        if new_j > current_j - j_tolerance or len(selected_indices) == 0:\n",
    "            selected_indices = new_selected\n",
    "            current_j = new_j\n",
    "            improved = True\n",
    "\n",
    "            print(f\"  【接受】添加了 {to_add} 个样本，新J值: {current_j:.4f}\")\n",
    "            if j_change < 0:\n",
    "                print(f\"  注意: 接受了J值轻微下降({j_change:.4f})以获取更多样本\")\n",
    "        else:\n",
    "            print(f\"  【拒绝】J值下降超出容忍范围，不添加样本\")\n",
    "\n",
    "    # 记录本次迭代后的状态\n",
    "    post_iteration_state = {\"selected_count\": len(selected_indices), \"j_score\": current_j, \"improved\": improved}\n",
    "\n",
    "    iteration_history.append({**pre_iteration_state, **post_iteration_state})\n",
    "\n",
    "    # 调整阈值，为下一轮迭代做准备\n",
    "    current_thresholds = adjust_thresholds(current_thresholds, iteration + 1, bin_ratios)\n",
    "\n",
    "    # 如果达到最小迭代次数且本轮迭代没有改进，考虑提前结束\n",
    "    if iteration >= min_iterations and not improved:\n",
    "        print(f\"  已完成{iteration + 1}轮迭代且本轮无改进，提前结束\")\n",
    "        break\n",
    "\n",
    "    # 打印当前状态\n",
    "    if len(selected_indices) > 0:\n",
    "        selected_values = valid_mean_preds[selected_indices]\n",
    "        current_hist, _ = np.histogram(selected_values, bins=bins)\n",
    "        print(f\"  当前已选择 {len(selected_indices)} 个样本，J值 = {current_j:.4f}\")\n",
    "        for i in range(len(bins) - 1):\n",
    "            coverage = current_hist[i] / target_bin_samples[i] * 100 if target_bin_samples[i] > 0 else 0\n",
    "            print(\n",
    "                f\"    {bin_labels[i]}: {current_hist[i]}个 ({current_hist[i] / len(selected_indices) * 100:.1f}%), 目标覆盖率: {coverage:.1f}%\"\n",
    "            )\n",
    "\n",
    "# 将选中的索引映射回原始索引\n",
    "final_selected_indices_orig = valid_unlabeled_indices[selected_indices]\n",
    "\n",
    "# 创建最终掩码\n",
    "final_mask = np.zeros(len(unlabeled_indices), dtype=bool)\n",
    "final_mask[np.where(np.isin(unlabeled_indices, final_selected_indices_orig))[0]] = True\n",
    "\n",
    "# 统计最终结果\n",
    "print(\"\\n=== 最终样本选择结果 ===\")\n",
    "final_hist, _ = np.histogram(mean_preds[final_mask], bins=bins)\n",
    "print(f\"总计选择样本: {np.sum(final_mask)}个\")\n",
    "for i in range(len(bins) - 1):\n",
    "    original_count = bin_counts[i]\n",
    "    final_count = final_hist[i]\n",
    "    coverage = final_count / target_bin_samples[i] * 100 if target_bin_samples[i] > 0 else 0\n",
    "    print(\n",
    "        f\"砂厚范围 {bin_labels[i]}: {final_count}个 ({final_count / np.sum(final_mask) * 100:.1f}%) | \"\n",
    "        f\"原始: {original_count}个 | 目标覆盖率: {coverage:.1f}%\"\n",
    "    )\n",
    "\n",
    "# 统计最终质量分数\n",
    "final_quality = quality_scores[selected_indices]\n",
    "print(f\"\\n样本质量统计:\")\n",
    "print(f\"  平均质量分数: {np.mean(final_quality):.4f}\")\n",
    "print(f\"  最低质量分数: {np.min(final_quality):.4f}\")\n",
    "print(f\"  最高质量分数: {np.max(final_quality):.4f}\")\n",
    "\n",
    "# 打印迭代历史摘要\n",
    "print(\"\\n迭代历史摘要:\")\n",
    "for i, hist in enumerate(iteration_history):\n",
    "    print(f\"  迭代 {i + 1}: 样本数 {hist['selected_count']}, J值 {hist['j_score']:.4f}\")\n",
    "\n",
    "# 创建伪标记数据\n",
    "X_pseudo = X_unlabeled[final_mask]\n",
    "y_pseudo = mean_preds[final_mask]\n",
    "\n",
    "# 获取优化样本在原始seismic_samples中的索引\n",
    "optimized_orig_indices = unlabeled_indices[final_mask]\n",
    "\n",
    "# 添加预测结果到seismic_samples\n",
    "seismic_samples[\"Fused_Pred\"] = np.nan\n",
    "seismic_samples[\"Lasso_Pred\"] = np.nan\n",
    "seismic_samples[\"Ridge_Pred\"] = np.nan\n",
    "seismic_samples[\"Mean_Pred\"] = np.nan\n",
    "seismic_samples[\"Max_Diff\"] = np.nan\n",
    "seismic_samples[\"Quality_Score\"] = np.nan\n",
    "seismic_samples[\"Is_Selected\"] = False\n",
    "seismic_samples[\"Excluded_By_Well\"] = False  # 新增列：被真实井点排除\n",
    "\n",
    "# 填充预测结果\n",
    "seismic_samples.loc[unlabeled_indices, \"Fused_Pred\"] = fused_pred_unlabeled\n",
    "seismic_samples.loc[unlabeled_indices, \"Lasso_Pred\"] = lasso_pred_unlabeled\n",
    "seismic_samples.loc[unlabeled_indices, \"Ridge_Pred\"] = ridge_pred_unlabeled\n",
    "seismic_samples.loc[unlabeled_indices, \"Mean_Pred\"] = mean_preds\n",
    "seismic_samples.loc[unlabeled_indices, \"Max_Diff\"] = max_diffs\n",
    "seismic_samples.loc[unlabeled_indices, \"Quality_Score\"] = np.nan  # 先设为NaN\n",
    "seismic_samples.loc[valid_unlabeled_indices, \"Quality_Score\"] = quality_scores  # 只填充有效点的质量分数\n",
    "\n",
    "# 标记被真实井点排除的点\n",
    "excluded_indices = unlabeled_indices[~np.isin(unlabeled_indices, valid_unlabeled_indices)]\n",
    "seismic_samples.loc[excluded_indices, \"Excluded_By_Well\"] = True\n",
    "\n",
    "# 标记最终选择的点\n",
    "seismic_samples.loc[optimized_orig_indices, \"Is_Selected\"] = True\n",
    "\n",
    "# 统计并报告负值数量\n",
    "neg_count = (seismic_samples[\"Mean_Pred\"] < 0).sum()\n",
    "if neg_count > 0:\n",
    "    print(f\"\\n注意: 有 {neg_count} 个负的砂厚预测值已被替换为0\")\n",
    "\n",
    "# 将负数的Mean_Pred值置为0\n",
    "seismic_samples[\"Mean_Pred\"] = seismic_samples[\"Mean_Pred\"].clip(lower=0)\n",
    "\n",
    "# 可视化优化前后的分布变化\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# 原始分布\n",
    "bin_percents = np.array(bin_counts) / sum(bin_counts) * 100\n",
    "plt.bar(np.arange(len(bin_labels)) - 0.2, bin_percents, width=0.4, label=\"原始分布\", color=\"lightblue\")\n",
    "\n",
    "# 优化后分布\n",
    "optimized_percents = final_hist / sum(final_hist) * 100\n",
    "plt.bar(np.arange(len(bin_labels)) + 0.2, optimized_percents, width=0.4, label=\"优化后分布\", color=\"orange\")\n",
    "\n",
    "# 添加目标分布线\n",
    "target_percents = target_distribution / sum(target_distribution) * 100\n",
    "plt.plot(np.arange(len(bin_labels)), target_percents, \"r--\", label=\"目标分布\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"砂厚区间\")\n",
    "plt.ylabel(\"百分比 (%)\")\n",
    "plt.title(\"样本分布优化前后对比\")\n",
    "plt.xticks(np.arange(len(bin_labels)), bin_labels)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "plt.savefig(os.path.join(output_dir, \"distribution_optimization.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# 保存预测结果\n",
    "seismic_samples.to_csv(os.path.join(output_dir, \"seismic_samples_with_predictions.csv\"), index=False)\n",
    "print(f\"\\n预测结果已保存至 {os.path.join(output_dir, 'seismic_samples_with_predictions.csv')}\")\n",
    "\n",
    "# 保存优化后的虚拟井数据\n",
    "optimized_samples = seismic_samples[seismic_samples[\"Is_Selected\"] == True].copy()\n",
    "optimized_samples.to_csv(os.path.join(output_dir, \"optimized_pseudo_wells.csv\"), index=False)\n",
    "print(f\"优化后的虚拟井数据已保存至 {os.path.join(output_dir, 'optimized_pseudo_wells.csv')}\")\n",
    "\n",
    "# 计算质量-分布的J指标\n",
    "final_j = calculate_j_score(\n",
    "    selected_indices, quality_scores, valid_mean_preds, target_distribution, bins, current_thresholds\n",
    ")\n",
    "print(f\"\\n最终优化J评分: {final_j:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3aebc5",
   "metadata": {},
   "source": [
    "## 虚拟井展示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5592059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备小工区地震数据（使用已筛选的data_H6_2_attr_filtered）\n",
    "# 1. 首先创建融合属性\n",
    "print(\"为整个小工区地震数据创建融合属性...\")\n",
    "\n",
    "# 标准化数据\n",
    "scaler = StandardScaler()\n",
    "attr_data = seismic_attr_filtered[selected_features].copy()\n",
    "attr_data_scaled = scaler.fit_transform(attr_data)\n",
    "\n",
    "# 使用之前定义的create_fused_attribute函数创建融合属性\n",
    "fused_attr = create_fused_attribute(attr_data_scaled, selected_features, correlation_weights)\n",
    "\n",
    "# 将融合属性添加到地震数据中\n",
    "seismic_attr_filtered[\"Fused_Attribute\"] = fused_attr\n",
    "\n",
    "# 2. 准备真实井点数据\n",
    "real_wells = well_attr_filtered.copy()\n",
    "\n",
    "# 3. 准备虚拟井点数据（使用之前优化选择的虚拟井）\n",
    "pseudo_wells = seismic_samples[seismic_samples[\"Is_Selected\"] == True].copy()\n",
    "\n",
    "# 可视化融合属性分布与井点位置\n",
    "visualize_attribute_map(\n",
    "    data_points=seismic_attr_filtered,\n",
    "    attribute_name=\"Fused_Attribute\",\n",
    "    attribute_label=\"地震融合属性值\",\n",
    "    real_wells=real_wells,\n",
    "    pseudo_wells=pseudo_wells,\n",
    "    target_column=\"Sand Thickness\",\n",
    "    output_dir=output_dir,\n",
    "    filename_prefix=\"fused_attribute\",\n",
    "    class_thresholds=[0.1, 10],\n",
    "    figsize=(16, 14),\n",
    "    dpi=300,\n",
    "    cmap=\"viridis\",\n",
    "    point_size=140,\n",
    "    well_size=200,\n",
    ")\n",
    "\n",
    "print(\"融合属性分布与井点位置可视化完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745533f2",
   "metadata": {},
   "source": [
    "## SVR、随机森林（限制树深）、XGBoost（控制学习率和复杂度）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ebe53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子以确保结果可复现\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"======== 三模型联合预测开始 ========\")\n",
    "\n",
    "# 步骤1: 准备数据 - 加载所有井点数据和虚拟井点数据\n",
    "# 使用所有井点数据(未筛选)\n",
    "all_wells = well_attr.copy()  # 使用所有井点，而不是筛选后的井点\n",
    "print(f\"全部井点数据数量: {len(all_wells)}\")\n",
    "\n",
    "# 加载虚拟井点数据\n",
    "pseudo_wells = pd.read_csv(os.path.join(output_dir, \"optimized_pseudo_wells.csv\"))\n",
    "print(f\"加载了 {len(pseudo_wells)} 个虚拟井点数据\")\n",
    "\n",
    "# 确定共同的特征列\n",
    "common_features = selected_features.copy()\n",
    "print(f\"使用的特征: {common_features}\")\n",
    "\n",
    "# 确保井点数据有所需的特征列\n",
    "all_wells_valid = all_wells.dropna(subset=common_features + [\"Sand Thickness\"])\n",
    "print(f\"有效井点数据数量: {len(all_wells_valid)}\")\n",
    "\n",
    "# 从真实井点中提取特征和目标\n",
    "X_real = all_wells_valid[common_features].values\n",
    "y_real = all_wells_valid[\"Sand Thickness\"].values\n",
    "\n",
    "# 从虚拟井点中提取特征和目标\n",
    "X_pseudo = pseudo_wells[common_features].values\n",
    "y_pseudo = pseudo_wells[\"Mean_Pred\"].values  # 使用平均预测作为目标值\n",
    "\n",
    "# 提取虚拟井的质量分数作为权重\n",
    "pseudo_quality_weights = pseudo_wells[\"Quality_Score\"].values\n",
    "\n",
    "# 确保权重在合理范围内\n",
    "if np.any(np.isnan(pseudo_quality_weights)):\n",
    "    print(\"警告：虚拟井质量分数中存在NaN值，将使用均值填充\")\n",
    "    pseudo_quality_weights = np.nan_to_num(pseudo_quality_weights, nan=np.nanmean(pseudo_quality_weights))\n",
    "\n",
    "# 确保权重为正数且进行适当缩放\n",
    "pseudo_quality_weights = np.clip(pseudo_quality_weights, 0.2, 1.0)  # 限制最小权重为0.2\n",
    "\n",
    "print(f\"虚拟井质量分数统计：\")\n",
    "print(f\"  - 平均值: {np.mean(pseudo_quality_weights):.4f}\")\n",
    "print(f\"  - 最小值: {np.min(pseudo_quality_weights):.4f}\")\n",
    "print(f\"  - 最大值: {np.max(pseudo_quality_weights):.4f}\")\n",
    "\n",
    "# 标准化真实井点数据(用于留一法)\n",
    "scaler_real = StandardScaler()\n",
    "X_real_scaled = scaler_real.fit_transform(X_real)\n",
    "\n",
    "# 创建留一法交叉验证对象\n",
    "loocv = LeaveOneOut()\n",
    "\n",
    "# 步骤2: 设置模型参数网格搜索(保留原有参数)\n",
    "# SVR参数网格\n",
    "param_grid_svr = {\"C\": [0.1, 1, 10], \"gamma\": [\"scale\", 0.01, 0.1], \"epsilon\": [0.1, 0.2], \"kernel\": [\"rbf\"]}\n",
    "\n",
    "# 随机森林参数网格\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"min_samples_leaf\": [3, 5],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"],\n",
    "}\n",
    "\n",
    "# XGBoost参数网格\n",
    "param_grid_xgb = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"subsample\": [0.8],\n",
    "    \"colsample_bytree\": [0.8],\n",
    "    \"min_child_weight\": [3, 5],\n",
    "}\n",
    "\n",
    "# 使用网格搜索找到最佳参数\n",
    "print(\"\\n执行SVR参数网格搜索...\")\n",
    "svr = SVR()\n",
    "grid_search_svr = GridSearchCV(svr, param_grid_svr, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "# 使用所有真实井点数据进行参数调优\n",
    "grid_search_svr.fit(X_real_scaled, y_real)\n",
    "best_svr_params = grid_search_svr.best_params_\n",
    "print(f\"SVR最佳参数: {best_svr_params}\")\n",
    "\n",
    "print(\"\\n执行随机森林参数网格搜索...\")\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "grid_search_rf.fit(X_real_scaled, y_real)\n",
    "best_rf_params = grid_search_rf.best_params_\n",
    "print(f\"随机森林最佳参数: {best_rf_params}\")\n",
    "\n",
    "print(\"\\n执行XGBoost参数网格搜索...\")\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "grid_search_xgb = GridSearchCV(xgb, param_grid_xgb, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "grid_search_xgb.fit(X_real_scaled, y_real)\n",
    "best_xgb_params = grid_search_xgb.best_params_\n",
    "print(f\"XGBoost最佳参数: {best_xgb_params}\")\n",
    "\n",
    "\n",
    "# 步骤3: 使用留一法交叉验证评估模型性能\n",
    "# 创建SVR预测函数\n",
    "def loocv_predict_with_pseudo(X_real, y_real, X_pseudo, y_pseudo, pseudo_weights, model_creator, model_params):\n",
    "    \"\"\"使用留一法交叉验证评估模型，同时利用虚拟井点进行训练\"\"\"\n",
    "    loocv = LeaveOneOut()\n",
    "    predictions = np.zeros_like(y_real)\n",
    "\n",
    "    for train_idx, test_idx in loocv.split(X_real):\n",
    "        # 分割数据\n",
    "        X_real_train, X_real_test = X_real[train_idx], X_real[test_idx]\n",
    "        y_real_train, y_real_test = y_real[train_idx], y_real[test_idx]\n",
    "\n",
    "        # 合并真实训练样本和虚拟样本\n",
    "        X_train = np.vstack((X_real_train, X_pseudo))\n",
    "        y_train = np.concatenate((y_real_train, y_pseudo))\n",
    "\n",
    "        # 标准化数据\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_real_test)\n",
    "\n",
    "        # 构建权重\n",
    "        train_weights = np.concatenate(\n",
    "            [\n",
    "                np.ones(len(X_real_train)) * 2.0,  # 真实样本权重为2\n",
    "                pseudo_weights,  # 虚拟样本使用质量分数作为权重\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 创建并训练模型\n",
    "        if model_creator == \"svr\":\n",
    "            # SVR特征数量限制\n",
    "            svr_features = min(3, X_train_scaled.shape[1])\n",
    "            X_train_svr = X_train_scaled[:, :svr_features]\n",
    "            X_test_svr = X_test_scaled[:, :svr_features]\n",
    "\n",
    "            model = SVR(**model_params)\n",
    "            model.fit(X_train_svr, y_train, sample_weight=train_weights)\n",
    "            predictions[test_idx] = model.predict(X_test_svr)\n",
    "        else:\n",
    "            model = model_creator(**model_params)\n",
    "            model.fit(X_train_scaled, y_train, sample_weight=train_weights)\n",
    "            predictions[test_idx] = model.predict(X_test_scaled)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "print(\"\\n执行留一法交叉验证...\")\n",
    "\n",
    "# SVR模型预测\n",
    "print(\"留一法评估SVR模型...\")\n",
    "# 注意SVR处理特征数量的限制\n",
    "svr_features = min(3, len(common_features))\n",
    "X_real_svr = X_real[:, :svr_features]\n",
    "X_pseudo_svr = X_pseudo[:, :svr_features]\n",
    "\n",
    "# SVR预测\n",
    "svr_pred = loocv_predict_with_pseudo(\n",
    "    X_real_svr, y_real, X_pseudo_svr, y_pseudo, pseudo_quality_weights, \"svr\", best_svr_params\n",
    ")\n",
    "\n",
    "# 随机森林预测\n",
    "print(\"留一法评估随机森林模型...\")\n",
    "rf_params = best_rf_params.copy()\n",
    "rf_params[\"random_state\"] = 42\n",
    "rf_pred = loocv_predict_with_pseudo(\n",
    "    X_real, y_real, X_pseudo, y_pseudo, pseudo_quality_weights, RandomForestRegressor, rf_params\n",
    ")\n",
    "\n",
    "# XGBoost预测\n",
    "print(\"留一法评估XGBoost模型...\")\n",
    "xgb_params = best_xgb_params.copy()\n",
    "xgb_params[\"random_state\"] = 42\n",
    "xgb_pred = loocv_predict_with_pseudo(\n",
    "    X_real, y_real, X_pseudo, y_pseudo, pseudo_quality_weights, XGBRegressor, xgb_params\n",
    ")\n",
    "\n",
    "\n",
    "# 步骤4: 评估留一法交叉验证结果\n",
    "def evaluate_loocv_results(y_true, y_pred, model_name):\n",
    "    \"\"\"评估留一法交叉验证结果\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\n{model_name} 留一法交叉验证评估:\")\n",
    "    print(f\"  - RMSE: {rmse:.4f}\")\n",
    "    print(f\"  - R²: {r2:.4f}\")\n",
    "    print(f\"  - MAE: {mae:.4f}\")\n",
    "\n",
    "    # 绘制真实值vs预测值散点图\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.7)\n",
    "    plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], \"r--\")\n",
    "    plt.xlabel(\"真实值\")\n",
    "    plt.ylabel(\"预测值\")\n",
    "    plt.title(f\"{model_name}: 留一法交叉验证 真实值 vs 预测值\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(\n",
    "        os.path.join(output_dir, f\"{model_name.lower().replace(' ', '_')}_loocv_pred_vs_true.png\"),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    return {\"rmse\": rmse, \"r2\": r2, \"mae\": mae}\n",
    "\n",
    "\n",
    "# 评估各模型留一法结果\n",
    "svr_results = evaluate_loocv_results(y_real, svr_pred, \"SVR\")\n",
    "rf_results = evaluate_loocv_results(y_real, rf_pred, \"随机森林\")\n",
    "xgb_results = evaluate_loocv_results(y_real, xgb_pred, \"XGBoost\")\n",
    "\n",
    "# 比较模型性能\n",
    "loocv_models_comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"模型\": [\"SVR\", \"随机森林\", \"XGBoost\"],\n",
    "        \"RMSE\": [svr_results[\"rmse\"], rf_results[\"rmse\"], xgb_results[\"rmse\"]],\n",
    "        \"R²\": [svr_results[\"r2\"], rf_results[\"r2\"], xgb_results[\"r2\"]],\n",
    "        \"MAE\": [svr_results[\"mae\"], rf_results[\"mae\"], xgb_results[\"mae\"]],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n留一法交叉验证模型性能比较:\")\n",
    "print(loocv_models_comparison)\n",
    "\n",
    "# 保存比较结果\n",
    "loocv_models_comparison.to_csv(os.path.join(output_dir, \"loocv_model_comparison.csv\"), index=False)\n",
    "\n",
    "# 步骤5: 训练最终模型\n",
    "print(\"\\n训练最终模型...\")\n",
    "\n",
    "# 选择性能最佳的模型类型\n",
    "best_model_idx = loocv_models_comparison[\"R²\"].idxmax()\n",
    "best_model_name = loocv_models_comparison.loc[best_model_idx, \"模型\"]\n",
    "print(f\"最佳模型: {best_model_name}\")\n",
    "\n",
    "# 合并所有数据用于训练最终模型\n",
    "X_combined = np.vstack((X_real, X_pseudo))\n",
    "y_combined = np.concatenate((y_real, y_pseudo))\n",
    "\n",
    "# 创建权重数组 - 真实井点权重为2.0，虚拟井点使用质量分数\n",
    "combined_weights = np.concatenate(\n",
    "    [\n",
    "        np.ones(len(X_real)) * 2.0,  # 真实井点权重为2\n",
    "        pseudo_quality_weights,  # 虚拟井点权重为质量分数\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 标准化特征\n",
    "scaler_final = StandardScaler()\n",
    "X_combined_scaled = scaler_final.fit_transform(X_combined)\n",
    "\n",
    "# 根据最佳模型类型训练最终模型\n",
    "if best_model_name == \"SVR\":\n",
    "    # SVR特征限制\n",
    "    svr_features = min(3, len(common_features))\n",
    "    X_combined_svr = X_combined_scaled[:, :svr_features]\n",
    "\n",
    "    final_model = SVR(**best_svr_params)\n",
    "    final_model.fit(X_combined_svr, y_combined, sample_weight=combined_weights)\n",
    "elif best_model_name == \"随机森林\":\n",
    "    rf_params = best_rf_params.copy()\n",
    "    rf_params[\"random_state\"] = 42\n",
    "    final_model = RandomForestRegressor(**rf_params)\n",
    "    final_model.fit(X_combined_scaled, y_combined, sample_weight=combined_weights)\n",
    "else:  # XGBoost\n",
    "    xgb_params = best_xgb_params.copy()\n",
    "    xgb_params[\"random_state\"] = 42\n",
    "    final_model = XGBRegressor(**xgb_params)\n",
    "    final_model.fit(X_combined_scaled, y_combined, sample_weight=combined_weights)\n",
    "\n",
    "# 步骤6: 为整个工区生成预测\n",
    "print(\"\\n使用最终模型为整个工区生成预测...\")\n",
    "seismic_data = seismic_attr_filtered.copy()\n",
    "X_seismic = seismic_data[common_features].fillna(seismic_data[common_features].mean())\n",
    "X_seismic_scaled = scaler_final.transform(X_seismic)\n",
    "\n",
    "# 根据模型类型生成预测\n",
    "if best_model_name == \"SVR\":\n",
    "    X_seismic_svr = X_seismic_scaled[:, :svr_features]\n",
    "    predictions = final_model.predict(X_seismic_svr)\n",
    "else:\n",
    "    predictions = final_model.predict(X_seismic_scaled)\n",
    "\n",
    "# 将预测结果添加到地震数据\n",
    "seismic_data[\"LOOCV_Predicted_Sand_Thickness\"] = predictions\n",
    "\n",
    "# 将负值预测设为0\n",
    "if (predictions < 0).any():\n",
    "    neg_count = (predictions < 0).sum()\n",
    "    print(f\"注意: {neg_count} 个负的砂厚预测值已被替换为0\")\n",
    "    seismic_data[\"LOOCV_Predicted_Sand_Thickness\"] = seismic_data[\"LOOCV_Predicted_Sand_Thickness\"].clip(lower=0)\n",
    "\n",
    "# 保存预测结果\n",
    "seismic_data.to_csv(os.path.join(output_dir, \"seismic_with_loocv_predictions.csv\"), index=False)\n",
    "\n",
    "# 可视化最终预测结果\n",
    "visualize_attribute_map(\n",
    "    data_points=seismic_data,\n",
    "    attribute_name=\"LOOCV_Predicted_Sand_Thickness\",\n",
    "    attribute_label=f\"砂厚预测值(米) - {best_model_name}模型\",\n",
    "    real_wells=well_attr_filtered,\n",
    "    pseudo_wells=None,\n",
    "    target_column=\"Sand Thickness\",\n",
    "    output_dir=output_dir,\n",
    "    filename_prefix=f\"loocv_final_prediction_{best_model_name.lower()}\",\n",
    "    class_thresholds=[0.1, 10],\n",
    "    figsize=(14, 14),\n",
    "    dpi=300,\n",
    "    cmap=\"viridis\",\n",
    "    point_size=150,\n",
    "    well_size=200,\n",
    ")\n",
    "\n",
    "# 如果是树模型，可视化特征重要性\n",
    "if best_model_name in [\"随机森林\", \"XGBoost\"]:\n",
    "    print(\"\\n分析特征重要性...\")\n",
    "\n",
    "    # 获取特征重要性\n",
    "    importances = final_model.feature_importances_\n",
    "\n",
    "    # 创建特征重要性DataFrame\n",
    "    feature_importance_df = pd.DataFrame({\"特征\": common_features, \"重要性\": importances})\n",
    "    feature_importance_df = feature_importance_df.sort_values(\"重要性\", ascending=False)\n",
    "\n",
    "    # 绘制特征重要性\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance_df[\"特征\"], feature_importance_df[\"重要性\"])\n",
    "    plt.xlabel(\"特征重要性\")\n",
    "    plt.title(f\"{best_model_name}模型特征重要性\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(output_dir, f\"loocv_{best_model_name.lower()}_feature_importance.png\"),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    # 保存特征重要性\n",
    "    feature_importance_df.to_csv(\n",
    "        os.path.join(output_dir, f\"loocv_{best_model_name.lower()}_feature_importance.csv\"), index=False\n",
    "    )\n",
    "\n",
    "print(\"\\n留一法交叉验证与质量分数加权建模完成。所有结果已保存到输出目录。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-af",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
