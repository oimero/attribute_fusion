{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c25782a",
   "metadata": {},
   "source": [
    "# 伪样本 + SVR 进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52717610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保src目录在Python路径中\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "from math import comb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "# 导入模块\n",
    "from src.data_utils import (\n",
    "    extract_seismic_attributes_for_wells,\n",
    "    identify_attributes,\n",
    "    parse_petrel_file,\n",
    "    preprocess_features,\n",
    ")\n",
    "from src.feature_selection import group_features_by_correlation, select_features_from_groups\n",
    "from src.visualization import (\n",
    "    visualize_attribute_map,\n",
    ")\n",
    "\n",
    "data_dir = \"..\\\\data\"\n",
    "data_tmp_dir = \"data_tmp\"\n",
    "output_dir = \"H5_2_ps_output\"\n",
    "if not os.path.exists(data_tmp_dir):\n",
    "    os.makedirs(data_tmp_dir)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = \"SimHei\"  # 黑体 SimHei 支持中文\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False  # 正常显示负号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474aaecf",
   "metadata": {},
   "source": [
    "## 导入地震数据\n",
    "\n",
    "修改 data_seismic_url = os.path.join(data_dir, \"6.2\") 这一行可以更换数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deef7e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_seismic_path = os.path.join(data_dir, \"H5-2\")\n",
    "\n",
    "data_seismic_attr = parse_petrel_file(data_seismic_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5417bfe2",
   "metadata": {},
   "source": [
    "## 处理属性缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425febc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先获取地震属性列表\n",
    "attribute_names, _ = identify_attributes(data_seismic_path)\n",
    "\n",
    "processed_features, stats, report = preprocess_features(\n",
    "    data=data_seismic_attr,\n",
    "    attribute_columns=attribute_names,\n",
    "    missing_values=[-999],\n",
    "    missing_threshold=0.6,\n",
    "    outlier_method=\"iqr\",\n",
    "    outlier_threshold=2.0,\n",
    "    outlier_treatment=\"clip\",  # 边界截断\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 提取筛选后的属性\n",
    "attribute_names_filtered = [col for col in processed_features.columns]\n",
    "\n",
    "# 将处理后的属性数据与原始坐标数据合并\n",
    "processed_seismic_full = data_seismic_attr[[\"X\", \"Y\"]].copy()  # type: ignore\n",
    "for col in processed_features.columns:\n",
    "    processed_seismic_full[col] = processed_features[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c1b7e0",
   "metadata": {},
   "source": [
    "## 导入真实井、虚拟井"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3dcdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_well_path = os.path.join(data_dir, \"well_without_attr.xlsx\")\n",
    "\n",
    "data_well_position = pd.read_excel(data_well_path)\n",
    "\n",
    "# 选择对应层位的行，丢弃砂厚为 NaN 的行\n",
    "data_well_purpose_surface_position = (\n",
    "    data_well_position[data_well_position[\"Surface\"] == \"H5-2\"]\n",
    "    .replace(-999, np.nan)  # 将-999替换为NaN\n",
    "    .dropna(subset=[\"Sand Thickness\"])  # 丢弃砂厚为NaN的行\n",
    "    .reset_index(drop=True)  # 重置索引\n",
    ")\n",
    "\n",
    "print(f\"井点数据导入完成，共 {len(data_well_purpose_surface_position)} 口井\")\n",
    "data_well_purpose_surface_position.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a81fa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为井点提取地震属性\n",
    "well_attr = extract_seismic_attributes_for_wells(\n",
    "    well_data=data_well_purpose_surface_position,\n",
    "    seismic_data=processed_seismic_full,\n",
    "    max_distance=50,\n",
    "    num_points=5,\n",
    ")\n",
    "\n",
    "print(f\"井点地震属性提取完成，共 {len(well_attr)} 口井\")\n",
    "\n",
    "# 保存处理结果\n",
    "well_attr.to_excel(os.path.join(data_tmp_dir, \"wells_attr.xlsx\"), index=False)\n",
    "print(\"井点的地震属性已保存到 wells_attr.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46606c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_wells_path = os.path.join(data_tmp_dir, \"optimized_pseudo_wells.csv\")\n",
    "if os.path.exists(pseudo_wells_path):\n",
    "    pseudo_wells = pd.read_csv(pseudo_wells_path)\n",
    "    print(f\"虚拟井数据导入完成，共 {len(pseudo_wells)} 个虚拟井点\")\n",
    "\n",
    "    # 检查虚拟井数据中的砂厚列名\n",
    "    if \"Predicted_Sand_Thickness\" in pseudo_wells.columns:\n",
    "        pseudo_thickness_col = \"Predicted_Sand_Thickness\"\n",
    "    elif \"Mean_Pred\" in pseudo_wells.columns:\n",
    "        pseudo_thickness_col = \"Mean_Pred\"\n",
    "    else:\n",
    "        # 查找可能的砂厚列\n",
    "        possible_cols = [col for col in pseudo_wells.columns if \"thick\" in col.lower() or \"pred\" in col.lower()]\n",
    "        if possible_cols:\n",
    "            pseudo_thickness_col = possible_cols[0]\n",
    "            print(f\"使用列 '{pseudo_thickness_col}' 作为虚拟井砂厚\")\n",
    "        else:\n",
    "            raise ValueError(\"无法找到虚拟井砂厚列\")\n",
    "\n",
    "    print(f\"虚拟井砂厚统计:\")\n",
    "    print(\n",
    "        f\"  - 范围: {pseudo_wells[pseudo_thickness_col].min():.2f} - {pseudo_wells[pseudo_thickness_col].max():.2f} m\"\n",
    "    )\n",
    "    print(f\"  - 平均值: {pseudo_wells[pseudo_thickness_col].mean():.2f} m\")\n",
    "\n",
    "    # 显示虚拟井数据的前几行\n",
    "    print(\"\\n虚拟井数据预览:\")\n",
    "    display(pseudo_wells.head())\n",
    "\n",
    "else:\n",
    "    raise FileNotFoundError(f\"未找到虚拟井数据文件: {pseudo_wells_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1096b2b1",
   "metadata": {},
   "source": [
    "## 根据自相关性对特征进行分组\n",
    "\n",
    "经验公式：1个特征对应10个样本。现在有30个真实井，所以最多选3个特征。\n",
    "\n",
    "先根据自相关性对特征进行分组，然后再遍历**从多个组中选三个组**的特征组合（如果组里只有一个特征，选这个特征即可；有多个特征则随机选择）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8935ce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行特征分组\n",
    "feature_groups, correlation_matrix = group_features_by_correlation(\n",
    "    data=processed_seismic_full,\n",
    "    feature_columns=attribute_names_filtered,\n",
    "    correlation_threshold=0.9,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(f\"\\n特征分组完成，共分成 {len(feature_groups)} 组\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aadad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算组合数\n",
    "n_groups = len(feature_groups)\n",
    "n_select = 3  # 选择3组特征\n",
    "\n",
    "if n_groups < n_select:\n",
    "    print(f\"警告: 特征组数 ({n_groups}) 少于需要选择的组数 ({n_select})\")\n",
    "    print(\"将使用所有组进行建模\")\n",
    "    n_select = n_groups\n",
    "\n",
    "\n",
    "total_combinations = comb(n_groups, n_select)\n",
    "print(f\"\\n=== SVR模型训练计划 ===\")\n",
    "print(f\"特征组数: {n_groups}\")\n",
    "print(f\"每次选择组数: {n_select}\")\n",
    "print(f\"总组合数: C({n_groups}, {n_select}) = {total_combinations}\")\n",
    "print(f\"预计训练 {total_combinations} 个SVR模型\")\n",
    "\n",
    "# 估算样本数\n",
    "real_samples = len(well_attr)\n",
    "pseudo_samples = len(pseudo_wells)\n",
    "total_samples = real_samples + pseudo_samples\n",
    "print(f\"\\n训练样本数: {real_samples} 个真实井 + {pseudo_samples} 个虚拟井 = {total_samples} 个样本\")\n",
    "\n",
    "# 调整SVR参数网格以减少过拟合\n",
    "param_grid = [\n",
    "    # RBF核参数\n",
    "    {\"C\": [0.01, 0.1, 1], \"gamma\": [\"scale\", 0.001, 0.01, 0.1], \"epsilon\": [0.1, 0.2, 0.5], \"kernel\": [\"rbf\"]},\n",
    "    # 线性核参数（线性核不需要gamma参数）\n",
    "    {\"C\": [0.01, 0.1, 1, 10], \"epsilon\": [0.1, 0.2, 0.5], \"kernel\": [\"linear\"]},\n",
    "]\n",
    "\n",
    "print(f\"\\nSVR参数网格设置:\")\n",
    "print(f\"  RBF核: C={[0.01, 0.1, 1]}, gamma={['scale', 0.001, 0.01, 0.1]}, epsilon={[0.1, 0.2, 0.5]}\")\n",
    "print(f\"  线性核: C={[0.01, 0.1, 1, 10]}, epsilon={[0.1, 0.2, 0.5]}\")\n",
    "\n",
    "# 重新计算网格大小\n",
    "rbf_grid_size = 3 * 4 * 3  # C * gamma * epsilon\n",
    "linear_grid_size = 4 * 3  # C * epsilon\n",
    "total_grid_size = rbf_grid_size + linear_grid_size\n",
    "\n",
    "print(f\"RBF核参数组合数: {rbf_grid_size}\")\n",
    "print(f\"线性核参数组合数: {linear_grid_size}\")\n",
    "print(f\"总参数组合数: {total_grid_size}\")\n",
    "print(f\"总计需要训练: {total_combinations} × {total_grid_size} = {total_combinations * total_grid_size} 个SVR模型\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c41f821",
   "metadata": {},
   "source": [
    "## 数据增强 & 权重设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a813b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_data_augmentation(X_real, y_real, X_pseudo, y_pseudo, target_samples_per_bin=10, noise_factor=0.05):\n",
    "    \"\"\"\n",
    "    动态数据增强\n",
    "\n",
    "    参数:\n",
    "        X_real: 真实井特征数据\n",
    "        y_real: 真实井砂厚数据\n",
    "        X_pseudo: 虚拟井特征数据\n",
    "        y_pseudo: 虚拟井砂厚数据\n",
    "        target_samples_per_bin: 每个砂厚区间目标样本数（阈值）\n",
    "        noise_factor: 噪声因子（相对于特征标准差）\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 动态数据增强策略 ===\")\n",
    "\n",
    "    # 定义砂厚区间\n",
    "    thickness_bins = [0, 1, 10, 20, np.inf]\n",
    "    bin_labels = [\"0-1m\", \"1-10m\", \"10-20m\", \">20m\"]\n",
    "\n",
    "    # 分析当前真实井样本分布\n",
    "    real_bin_counts = []\n",
    "    real_bin_masks = []\n",
    "\n",
    "    print(\"真实井样本分布分析:\")\n",
    "    for i in range(len(thickness_bins) - 1):\n",
    "        mask = (y_real >= thickness_bins[i]) & (y_real < thickness_bins[i + 1])\n",
    "        count = np.sum(mask)\n",
    "        real_bin_counts.append(count)\n",
    "        real_bin_masks.append(mask)\n",
    "        print(f\"  {bin_labels[i]}: {count} 个真实样本\")\n",
    "\n",
    "    # 分析虚拟井样本分布\n",
    "    pseudo_bin_counts = []\n",
    "    pseudo_bin_masks = []\n",
    "\n",
    "    print(\"\\n虚拟井样本分布分析:\")\n",
    "    for i in range(len(thickness_bins) - 1):\n",
    "        mask = (y_pseudo >= thickness_bins[i]) & (y_pseudo < thickness_bins[i + 1])\n",
    "        count = np.sum(mask)\n",
    "        pseudo_bin_counts.append(count)\n",
    "        pseudo_bin_masks.append(mask)\n",
    "        print(f\"  {bin_labels[i]}: {count} 个虚拟样本\")\n",
    "\n",
    "    # 初始化增强后的数据\n",
    "    X_augmented = X_real.copy()\n",
    "    y_augmented = y_real.copy()\n",
    "    augmentation_sources = [\"real\"] * len(y_real)  # 记录样本来源\n",
    "\n",
    "    # 计算特征标准差（用于噪声生成）\n",
    "    all_features = np.vstack([X_real, X_pseudo])\n",
    "    feature_stds = np.std(all_features, axis=0)\n",
    "\n",
    "    print(f\"\\n动态增强决策（阈值: {target_samples_per_bin}）:\")\n",
    "\n",
    "    # 对每个区间进行动态增强\n",
    "    for i in range(len(thickness_bins) - 1):\n",
    "        real_count = real_bin_counts[i]\n",
    "        pseudo_count = pseudo_bin_counts[i]\n",
    "\n",
    "        print(f\"\\n{bin_labels[i]} 区间:\")\n",
    "        print(f\"  真实样本: {real_count}, 虚拟样本: {pseudo_count}\")\n",
    "\n",
    "        if real_count >= target_samples_per_bin:\n",
    "            print(f\"  ✓ 真实样本充足（>={target_samples_per_bin}），无需增强\")\n",
    "            continue\n",
    "\n",
    "        samples_needed = target_samples_per_bin - real_count\n",
    "        print(f\"  需要增强 {samples_needed} 个样本\")\n",
    "\n",
    "        # 策略1：如果有真实样本，优先基于真实样本增强\n",
    "        if real_count > 0:\n",
    "            print(f\"  策略: 基于 {real_count} 个真实样本进行噪声增强\")\n",
    "\n",
    "            # 提取该区间的真实样本\n",
    "            X_real_bin = X_real[real_bin_masks[i]]\n",
    "            y_real_bin = y_real[real_bin_masks[i]]\n",
    "\n",
    "            # 生成增强样本\n",
    "            for j in range(samples_needed):\n",
    "                # 随机选择一个真实样本作为基础\n",
    "                base_idx = np.random.randint(0, len(X_real_bin))\n",
    "                base_x = X_real_bin[base_idx].copy()\n",
    "                base_y = y_real_bin[base_idx]\n",
    "\n",
    "                # 添加适应性噪声\n",
    "                # 根据砂厚大小调整噪声强度\n",
    "                if base_y <= 1:  # 薄层，使用小噪声\n",
    "                    adaptive_noise_factor = noise_factor * 0.5\n",
    "                elif base_y <= 10:  # 中等厚度\n",
    "                    adaptive_noise_factor = noise_factor * 1.0\n",
    "                else:  # 厚层，使用稍大噪声\n",
    "                    adaptive_noise_factor = noise_factor * 1.5\n",
    "\n",
    "                noise = np.random.normal(0, feature_stds * adaptive_noise_factor)\n",
    "                new_x = base_x + noise\n",
    "\n",
    "                # 砂厚也加入轻微扰动，但保持在区间内\n",
    "                if i == 0:  # 0-1m区间\n",
    "                    thickness_noise = np.random.uniform(-0.1, 0.1)\n",
    "                    new_y = np.clip(base_y + thickness_noise, 0, 0.99)\n",
    "                elif i == 1:  # 1-10m区间\n",
    "                    thickness_noise = np.random.uniform(-0.5, 0.5)\n",
    "                    new_y = np.clip(base_y + thickness_noise, 1, 9.99)\n",
    "                elif i == 2:  # 10-20m区间\n",
    "                    thickness_noise = np.random.uniform(-1.0, 1.0)\n",
    "                    new_y = np.clip(base_y + thickness_noise, 10, 19.99)\n",
    "                else:  # >20m区间\n",
    "                    thickness_noise = np.random.uniform(-2.0, 2.0)\n",
    "                    new_y = max(20, base_y + thickness_noise)\n",
    "\n",
    "                X_augmented = np.vstack([X_augmented, new_x.reshape(1, -1)])\n",
    "                y_augmented = np.append(y_augmented, new_y)\n",
    "                augmentation_sources.append(\"real_augmented\")\n",
    "\n",
    "        # 策略2：如果没有真实样本但有虚拟样本，从虚拟样本采样\n",
    "        elif pseudo_count > 0:\n",
    "            print(f\"  策略: 从 {pseudo_count} 个虚拟样本中采样\")\n",
    "\n",
    "            # 提取该区间的虚拟样本\n",
    "            X_pseudo_bin = X_pseudo[pseudo_bin_masks[i]]\n",
    "            y_pseudo_bin = y_pseudo[pseudo_bin_masks[i]]\n",
    "\n",
    "            # 从虚拟样本中随机采样\n",
    "            sample_indices = np.random.choice(\n",
    "                len(X_pseudo_bin),\n",
    "                size=min(samples_needed, len(X_pseudo_bin)),\n",
    "                replace=True if samples_needed > len(X_pseudo_bin) else False,\n",
    "            )\n",
    "\n",
    "            for idx in sample_indices:\n",
    "                # 对虚拟样本也添加轻微噪声以增加多样性\n",
    "                base_x = X_pseudo_bin[idx].copy()\n",
    "                base_y = y_pseudo_bin[idx]\n",
    "\n",
    "                # 添加小幅噪声\n",
    "                noise = np.random.normal(0, feature_stds * noise_factor * 0.3)\n",
    "                new_x = base_x + noise\n",
    "\n",
    "                X_augmented = np.vstack([X_augmented, new_x.reshape(1, -1)])\n",
    "                y_augmented = np.append(y_augmented, base_y)\n",
    "                augmentation_sources.append(\"pseudo_sampled\")\n",
    "\n",
    "        else:\n",
    "            print(f\"  警告: 该区间既无真实样本也无虚拟样本，跳过增强\")\n",
    "\n",
    "    # 统计增强结果\n",
    "    original_count = len(y_real)\n",
    "    augmented_count = len(y_augmented)\n",
    "\n",
    "    print(f\"\\n=== 数据增强完成 ===\")\n",
    "    print(f\"原始真实井样本: {original_count}\")\n",
    "    print(f\"增强后总样本: {augmented_count}\")\n",
    "    print(f\"新增样本: {augmented_count - original_count}\")\n",
    "\n",
    "    # 按来源统计\n",
    "    sources_count = {}\n",
    "    for source in set(augmentation_sources):\n",
    "        sources_count[source] = augmentation_sources.count(source)\n",
    "\n",
    "    print(f\"\\n样本来源统计:\")\n",
    "    for source, count in sources_count.items():\n",
    "        if source == \"real\":\n",
    "            print(f\"  原始真实井: {count}\")\n",
    "        elif source == \"real_augmented\":\n",
    "            print(f\"  真实井增强: {count}\")\n",
    "        elif source == \"pseudo_sampled\":\n",
    "            print(f\"  虚拟井采样: {count}\")\n",
    "\n",
    "    # 最终分布检查\n",
    "    print(f\"\\n增强后各区间真实样本分布:\")\n",
    "    for i in range(len(thickness_bins) - 1):\n",
    "        mask = (y_augmented >= thickness_bins[i]) & (y_augmented < thickness_bins[i + 1])\n",
    "        count = np.sum(mask)\n",
    "        print(f\"  {bin_labels[i]}: {count} 个样本\")\n",
    "\n",
    "    return X_augmented, y_augmented, augmentation_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并真实井和虚拟井数据\n",
    "print(\"=== 准备训练数据 ===\")\n",
    "\n",
    "# 确保虚拟井数据包含所需的特征列\n",
    "common_features = [col for col in attribute_names_filtered if col in pseudo_wells.columns]\n",
    "missing_features = [col for col in attribute_names_filtered if col not in pseudo_wells.columns]\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"警告: 虚拟井数据中缺少以下特征: {missing_features}\")\n",
    "    print(\"将只使用两个数据集中都存在的特征\")\n",
    "    # 更新特征组，只保留共同特征\n",
    "    updated_feature_groups = []\n",
    "    for group in feature_groups:\n",
    "        updated_group = [f for f in group if f in common_features]\n",
    "        if updated_group:  # 只保留非空组\n",
    "            updated_feature_groups.append(updated_group)\n",
    "    feature_groups = updated_feature_groups\n",
    "    print(f\"更新后的特征组数: {len(feature_groups)}\")\n",
    "\n",
    "# 准备真实井数据\n",
    "X_real = well_attr[common_features].values\n",
    "y_real = well_attr[\"Sand Thickness\"].values\n",
    "\n",
    "# 准备虚拟井数据\n",
    "X_pseudo = pseudo_wells[common_features].values  # type: ignore\n",
    "y_pseudo = pseudo_wells[pseudo_thickness_col].values\n",
    "\n",
    "# 执行动态数据增强\n",
    "X_real_augmented, y_real_augmented, aug_sources = dynamic_data_augmentation(\n",
    "    X_real,\n",
    "    y_real,\n",
    "    X_pseudo,\n",
    "    y_pseudo,\n",
    "    target_samples_per_bin=10,  # 阈值设为10\n",
    "    noise_factor=0.03,\n",
    ")\n",
    "\n",
    "# 重新合并数据（用增强后的真实井数据）\n",
    "X_combined_aug = np.vstack([X_real_augmented, X_pseudo])\n",
    "y_combined_aug = np.concatenate([y_real_augmented, y_pseudo])  # type: ignore\n",
    "\n",
    "# 动态调整样本权重\n",
    "original_real_count = len(X_real)\n",
    "real_augmented_count = aug_sources.count(\"real_augmented\")\n",
    "pseudo_sampled_count = aug_sources.count(\"pseudo_sampled\")\n",
    "\n",
    "sample_weights_aug = np.concatenate(\n",
    "    [\n",
    "        # 原始真实井权重最高\n",
    "        np.ones(original_real_count) * 15.0,\n",
    "        # 真实井增强样本权重较高\n",
    "        np.ones(real_augmented_count) * 8.0,\n",
    "        # 虚拟井采样权重中等\n",
    "        np.ones(pseudo_sampled_count) * 3.0,\n",
    "        # 原始虚拟井权重最低\n",
    "        np.ones(len(X_pseudo)) * 1.0,\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"\\n动态权重分配:\")\n",
    "print(f\"  - 原始真实井权重: 15.0 (共{original_real_count}个)\")\n",
    "print(f\"  - 真实井增强权重: 8.0 (共{real_augmented_count}个)\")\n",
    "print(f\"  - 虚拟井采样权重: 3.0 (共{pseudo_sampled_count}个)\")\n",
    "print(f\"  - 原始虚拟井权重: 1.0 (共{len(X_pseudo)}个)\")\n",
    "\n",
    "# 更新训练数据\n",
    "X_combined = X_combined_aug\n",
    "y_combined = y_combined_aug\n",
    "sample_weights = sample_weights_aug\n",
    "\n",
    "print(f\"\\n最终训练数据:\")\n",
    "print(f\"  - 总样本数: {len(X_combined)}\")\n",
    "print(f\"  - 目标变量范围: {y_combined.min():.2f} - {y_combined.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a44153",
   "metadata": {},
   "source": [
    "## 遍历所有特征组合训练SVR模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0c71fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历所有特征组合训练SVR模型\n",
    "all_combinations = list(combinations(range(len(feature_groups)), n_select))\n",
    "print(f\"=== 开始训练 {len(all_combinations)} 个SVR模型 ===\")\n",
    "\n",
    "# 存储所有模型结果\n",
    "model_results = []\n",
    "best_models = []  # 存储最佳模型信息\n",
    "\n",
    "# 设置随机种子确保可重现性\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i, combination in enumerate(all_combinations):\n",
    "    print(f\"\\n训练模型 {i + 1}/{len(all_combinations)}: 组合 {combination}\")\n",
    "\n",
    "    # 从选定组中选择特征\n",
    "    selected_features = select_features_from_groups(feature_groups, combination, random_seed=42 + i)\n",
    "    print(f\"  选择的特征: {selected_features}\")\n",
    "\n",
    "    # 获取特征索引\n",
    "    feature_indices = [common_features.index(f) for f in selected_features if f in common_features]\n",
    "\n",
    "    if len(feature_indices) != len(selected_features):\n",
    "        print(f\"  警告: 部分特征不在数据中，跳过此组合\")\n",
    "        continue\n",
    "\n",
    "    # 提取特征数据\n",
    "    X_train = X_combined[:, feature_indices]\n",
    "\n",
    "    # 标准化特征\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    try:\n",
    "        svr = SVR(kernel=\"rbf\")\n",
    "\n",
    "        # 调整交叉验证策略\n",
    "        cv_folds = min(3, len(X_train) // 3)  # 减少CV折数，每折至少有样本数的1/3\n",
    "        if cv_folds < 2:\n",
    "            cv_folds = 2  # 最少2折\n",
    "\n",
    "        grid_search = GridSearchCV(\n",
    "            svr,\n",
    "            param_grid,\n",
    "            cv=cv_folds,\n",
    "            scoring=\"r2\",\n",
    "            n_jobs=-1,\n",
    "            return_train_score=True,  # 返回训练分数以便分析过拟合\n",
    "        )\n",
    "\n",
    "        print(f\"  使用{cv_folds}折交叉验证\")\n",
    "\n",
    "        grid_search.fit(X_train_scaled, y_combined, sample_weight=sample_weights)\n",
    "\n",
    "        best_svr = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "        best_cv_score = grid_search.best_score_\n",
    "\n",
    "        # 在训练集上的表现\n",
    "        train_pred = best_svr.predict(X_train_scaled)\n",
    "        train_r2 = r2_score(y_combined, train_pred)\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_combined, train_pred))\n",
    "\n",
    "        # 在存储结果之前添加过拟合检测\n",
    "        overfitting_score = train_r2 - best_cv_score\n",
    "        if overfitting_score > 0.5:  # 如果训练集和验证集R²差异过大\n",
    "            print(f\"  警告: 检测到严重过拟合 (差异: {overfitting_score:.3f})\")\n",
    "\n",
    "        result = {\n",
    "            \"combination_index\": i,\n",
    "            \"group_combination\": combination,\n",
    "            \"selected_features\": selected_features,\n",
    "            \"best_params\": best_params,\n",
    "            \"cv_r2\": best_cv_score,\n",
    "            \"train_r2\": train_r2,\n",
    "            \"train_rmse\": train_rmse,\n",
    "            \"overfitting_score\": overfitting_score,  # 添加过拟合指标\n",
    "            \"model\": best_svr,\n",
    "            \"scaler\": scaler,\n",
    "            \"feature_indices\": feature_indices,\n",
    "        }\n",
    "\n",
    "        model_results.append(result)\n",
    "        print(f\"  最佳参数: {best_params}\")\n",
    "        print(f\"  CV R^2: {best_cv_score:.4f}\")\n",
    "        print(f\"  训练集 R^2: {train_r2:.4f}\")\n",
    "        print(f\"  训练集 RMSE: {train_rmse:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  训练失败: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "    # 每10个模型报告一次进度\n",
    "    if (i + 1) % 10 == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        avg_time_per_model = elapsed_time / (i + 1)\n",
    "        remaining_time = avg_time_per_model * (len(all_combinations) - i - 1)\n",
    "        print(f\"\\n进度: {i + 1}/{len(all_combinations)}, 已用时: {elapsed_time:.1f}s, 预计剩余: {remaining_time:.1f}s\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n=== 所有模型训练完成 ===\")\n",
    "print(f\"成功训练: {len(model_results)}/{len(all_combinations)} 个模型\")\n",
    "print(f\"总用时: {total_time:.1f} 秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74e4d13",
   "metadata": {},
   "source": [
    "## 选择最佳模型并进行全区预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b946d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按CV R²排序，选择前5个模型\n",
    "model_results.sort(key=lambda x: x[\"cv_r2\"], reverse=True)\n",
    "top_models = model_results[:5]\n",
    "\n",
    "print(\"=== 前5个最佳模型 ===\")\n",
    "for i, result in enumerate(top_models):\n",
    "    print(f\"\\n模型 {i + 1}:\")\n",
    "    print(f\"  特征组合: {result['group_combination']}\")\n",
    "    print(f\"  选择特征: {result['selected_features']}\")\n",
    "    print(f\"  最佳参数: {result['best_params']}\")\n",
    "    print(f\"  CV R^2: {result['cv_r2']:.4f}\")\n",
    "    print(f\"  训练集 R^2: {result['train_r2']:.4f}\")\n",
    "    print(f\"  训练集 RMSE: {result['train_rmse']:.4f}\")\n",
    "\n",
    "# 为整个工区生成预测\n",
    "print(\"\\n=== 开始全区预测 ===\")\n",
    "\n",
    "# 准备全区数据\n",
    "seismic_data = processed_seismic_full.copy()\n",
    "seismic_features = seismic_data[common_features].fillna(seismic_data[common_features].mean())\n",
    "\n",
    "# 为每个最佳模型生成预测\n",
    "for i, result in enumerate(top_models):\n",
    "    print(f\"\\n使用模型 {i + 1} 进行全区预测...\")\n",
    "\n",
    "    # 提取对应特征\n",
    "    X_seismic = seismic_features.iloc[:, result[\"feature_indices\"]].values\n",
    "\n",
    "    # 标准化\n",
    "    X_seismic_scaled = result[\"scaler\"].transform(X_seismic)\n",
    "\n",
    "    # 预测\n",
    "    predictions = result[\"model\"].predict(X_seismic_scaled)\n",
    "\n",
    "    # 将负值设为0\n",
    "    predictions = np.maximum(predictions, 0)\n",
    "\n",
    "    # 添加到地震数据\n",
    "    column_name = f\"SVR_Model_{i + 1}_Prediction\"\n",
    "    seismic_data[column_name] = predictions\n",
    "\n",
    "    print(f\"  预测完成，预测值范围: {predictions.min():.2f} - {predictions.max():.2f}\")\n",
    "    print(f\"  平均预测值: {predictions.mean():.2f}\")\n",
    "\n",
    "# 创建综合预测（前5个模型的平均）\n",
    "ensemble_predictions = np.mean(\n",
    "    [seismic_data[f\"SVR_Model_{i + 1}_Prediction\"].values for i in range(len(top_models))],  # type: ignore\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "seismic_data[\"SVR_Ensemble_Prediction\"] = ensemble_predictions\n",
    "\n",
    "print(f\"\\n集成预测完成:\")\n",
    "print(f\"  预测值范围: {ensemble_predictions.min():.2f} - {ensemble_predictions.max():.2f}\")\n",
    "print(f\"  平均预测值: {ensemble_predictions.mean():.2f}\")\n",
    "\n",
    "# 保存结果\n",
    "output_file = os.path.join(output_dir, \"svr_predictions_all_models.csv\")\n",
    "seismic_data.to_csv(output_file, index=False)\n",
    "print(f\"\\n全区预测结果已保存到: {output_file}\")\n",
    "\n",
    "# 生成Petrel格式的txt文件\n",
    "print(\"\\n=== 生成Petrel格式文件 ===\")\n",
    "\n",
    "# 提取基础坐标数据\n",
    "base_coords = seismic_data[[\"X\", \"Y\"]].copy()\n",
    "\n",
    "# 为每个最佳模型生成txt文件\n",
    "for i, result in enumerate(top_models):\n",
    "    model_num = i + 1\n",
    "    column_name = f\"SVR_Model_{model_num}_Prediction\"\n",
    "\n",
    "    # 准备数据：X, Y, Sand Thickness（砂厚预测值）\n",
    "    petrel_data = base_coords.copy()\n",
    "    petrel_data[\"Sand Thickness\"] = seismic_data[column_name]\n",
    "\n",
    "    # 生成文件名\n",
    "    txt_filename = f\"SVR_Model_{model_num}_Prediction.txt\"\n",
    "    txt_filepath = os.path.join(output_dir, txt_filename)\n",
    "\n",
    "    # 保存为txt文件，使用空格分隔\n",
    "    petrel_data.to_csv(\n",
    "        txt_filepath,\n",
    "        sep=\" \",  # 使用空格分隔\n",
    "        index=False,  # 不保存索引\n",
    "        header=True,  # 保存列名\n",
    "        float_format=\"%.6f\",  # 保留6位小数\n",
    "    )\n",
    "\n",
    "    print(f\"  模型{model_num}已保存: {txt_filename}\")\n",
    "    print(f\"    预测点数: {len(petrel_data):,}\")\n",
    "    print(f\"    砂厚范围: {petrel_data['Sand Thickness'].min():.2f} - {petrel_data['Sand Thickness'].max():.2f} m\")\n",
    "\n",
    "# 生成集成预测的txt文件\n",
    "ensemble_petrel_data = base_coords.copy()\n",
    "ensemble_petrel_data[\"Sand Thickness\"] = ensemble_predictions\n",
    "\n",
    "ensemble_txt_filename = \"SVR_Ensemble_Prediction.txt\"\n",
    "ensemble_txt_filepath = os.path.join(output_dir, ensemble_txt_filename)\n",
    "\n",
    "ensemble_petrel_data.to_csv(\n",
    "    ensemble_txt_filepath,\n",
    "    sep=\" \",  # 使用空格分隔\n",
    "    index=False,  # 不保存索引\n",
    "    header=True,  # 保存列名\n",
    "    float_format=\"%.6f\",  # 保留6位小数\n",
    ")\n",
    "\n",
    "print(f\"  集成模型已保存: {ensemble_txt_filename}\")\n",
    "print(f\"    预测点数: {len(ensemble_petrel_data):,}\")\n",
    "print(f\"    砂厚范围: {ensemble_predictions.min():.2f} - {ensemble_predictions.max():.2f} m\")\n",
    "\n",
    "print(f\"\\n=== Petrel格式文件生成完成 ===\")\n",
    "print(f\"生成的文件列表:\")\n",
    "for i in range(len(top_models)):\n",
    "    print(f\"  • SVR_Model_{i + 1}_Prediction.txt\")\n",
    "print(f\"  • SVR_Ensemble_Prediction.txt\")\n",
    "print(f\"\\n所有文件均保存在目录: {output_dir}\")\n",
    "print(f\"文件格式: X Y Sand_Thickness (空格分隔)\")\n",
    "print(f\"可直接导入Petrel进行可视化和分析\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0acda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化预测结果\n",
    "\n",
    "print(\"\\n=== 开始生成预测结果可视化图 ===\")\n",
    "\n",
    "# 设置可视化参数\n",
    "class_thresholds = [1.0, 10.0]  # 砂厚分类阈值，可根据实际情况调整\n",
    "figsize = (14, 14)\n",
    "point_size = 10\n",
    "well_size = 60\n",
    "# vrange = (0, max(ensemble_predictions.max(), 15))  # 设置色彩范围，最大值取集成预测最大值和15的较大者\n",
    "vrange = (0, 30)  # 设置色彩范围，最大值取20\n",
    "\n",
    "# 为每个最佳模型生成可视化图\n",
    "for i, result in enumerate(top_models):\n",
    "    model_num = i + 1\n",
    "    column_name = f\"SVR_Model_{model_num}_Prediction\"\n",
    "\n",
    "    print(f\"\\n生成模型 {model_num} 预测结果可视化图...\")\n",
    "\n",
    "    # 构建模型描述信息\n",
    "    features_str = \", \".join(result[\"selected_features\"])\n",
    "    cv_r2 = result[\"cv_r2\"]\n",
    "\n",
    "    # 生成可视化图\n",
    "    visualize_attribute_map(\n",
    "        data_points=seismic_data,\n",
    "        attribute_name=column_name,\n",
    "        attribute_label=f\"模型{model_num}砂厚预测值(m)\\nCV R^2={cv_r2:.3f}\\n特征: {features_str}\",\n",
    "        real_wells=well_attr,  # 使用真实井数据\n",
    "        pseudo_wells=None,\n",
    "        target_column=\"Sand Thickness\",\n",
    "        output_dir=output_dir,\n",
    "        filename_prefix=f\"svr_model_{model_num}_prediction\",\n",
    "        class_thresholds=class_thresholds,\n",
    "        figsize=figsize,\n",
    "        point_size=point_size,\n",
    "        well_size=well_size,\n",
    "        vrange=vrange,\n",
    "        cmap=\"viridis\",\n",
    "    )\n",
    "\n",
    "# 生成集成预测可视化图\n",
    "print(f\"\\n生成集成预测结果可视化图...\")\n",
    "\n",
    "# 计算集成模型的平均CV R²\n",
    "avg_cv_r2 = np.mean([result[\"cv_r2\"] for result in top_models])\n",
    "\n",
    "visualize_attribute_map(\n",
    "    data_points=seismic_data,\n",
    "    attribute_name=\"SVR_Ensemble_Prediction\",\n",
    "    attribute_label=f\"SVR集成砂厚预测值(m)\\n平均CV R^2={avg_cv_r2:.3f}\\n(前5个最佳模型平均)\",\n",
    "    real_wells=well_attr,  # 使用真实井数据\n",
    "    pseudo_wells=None,\n",
    "    target_column=\"Sand Thickness\",\n",
    "    output_dir=output_dir,\n",
    "    filename_prefix=\"svr_ensemble_prediction\",\n",
    "    class_thresholds=class_thresholds,\n",
    "    figsize=figsize,\n",
    "    point_size=point_size,\n",
    "    well_size=well_size,\n",
    "    vrange=vrange,\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "\n",
    "print(\"\\n=== 预测结果可视化完成 ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386d4687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型结果摘要\n",
    "results_summary = []\n",
    "for i, result in enumerate(model_results):\n",
    "    summary = {\n",
    "        \"model_rank\": i + 1,\n",
    "        \"combination_index\": result[\"combination_index\"],\n",
    "        \"group_combination\": str(result[\"group_combination\"]),\n",
    "        \"selected_features\": str(result[\"selected_features\"]),\n",
    "        \"cv_r2\": result[\"cv_r2\"],\n",
    "        \"train_r2\": result[\"train_r2\"],\n",
    "        \"train_rmse\": result[\"train_rmse\"],\n",
    "        \"best_params\": str(result[\"best_params\"]),\n",
    "    }\n",
    "    results_summary.append(summary)\n",
    "\n",
    "summary_df = pd.DataFrame(results_summary)\n",
    "summary_file = os.path.join(output_dir, \"svr_model_results_summary.csv\")\n",
    "summary_df.to_csv(summary_file, index=False)\n",
    "print(f\"模型结果摘要已保存到: {summary_file}\")\n",
    "\n",
    "# 生成预测统计报告\n",
    "print(\"\\n=== 预测结果统计报告 ===\")\n",
    "\n",
    "# 统计各模型预测结果\n",
    "prediction_stats = []\n",
    "for i, result in enumerate(top_models):\n",
    "    model_num = i + 1\n",
    "    column_name = f\"SVR_Model_{model_num}_Prediction\"\n",
    "    predictions = seismic_data[column_name]\n",
    "\n",
    "    stats = {\n",
    "        \"模型\": f\"模型{model_num}\",\n",
    "        \"特征组合\": str(result[\"group_combination\"]),\n",
    "        \"选择特征\": str(result[\"selected_features\"]),\n",
    "        \"CV_R2\": result[\"cv_r2\"],\n",
    "        \"预测最小值\": predictions.min(),\n",
    "        \"预测最大值\": predictions.max(),\n",
    "        \"预测平均值\": predictions.mean(),\n",
    "        \"预测标准差\": predictions.std(),\n",
    "        \"预测中位数\": predictions.median(),\n",
    "        \"大于1m的点数\": (predictions > 1.0).sum(),\n",
    "        \"大于5m的点数\": (predictions > 5.0).sum(),\n",
    "        \"大于10m的点数\": (predictions > 10.0).sum(),\n",
    "    }\n",
    "    prediction_stats.append(stats)\n",
    "\n",
    "# 添加集成预测统计\n",
    "ensemble_stats = {\n",
    "    \"模型\": \"集成预测\",\n",
    "    \"特征组合\": \"前5个模型平均\",\n",
    "    \"选择特征\": \"多模型组合\",\n",
    "    \"CV_R2\": avg_cv_r2,\n",
    "    \"预测最小值\": ensemble_predictions.min(),\n",
    "    \"预测最大值\": ensemble_predictions.max(),\n",
    "    \"预测平均值\": ensemble_predictions.mean(),\n",
    "    \"预测标准差\": ensemble_predictions.std(),\n",
    "    \"预测中位数\": np.median(ensemble_predictions),\n",
    "    \"大于1m的点数\": (ensemble_predictions > 1.0).sum(),\n",
    "    \"大于5m的点数\": (ensemble_predictions > 5.0).sum(),\n",
    "    \"大于10m的点数\": (ensemble_predictions > 10.0).sum(),\n",
    "}\n",
    "prediction_stats.append(ensemble_stats)\n",
    "\n",
    "# 转换为DataFrame并保存\n",
    "stats_df = pd.DataFrame(prediction_stats)\n",
    "stats_file = os.path.join(output_dir, \"prediction_statistics.csv\")\n",
    "stats_df.to_csv(stats_file, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"预测统计信息已保存到: {stats_file}\")\n",
    "\n",
    "# 打印统计摘要\n",
    "print(\"\\n预测结果统计摘要:\")\n",
    "for i, stats in enumerate(prediction_stats):\n",
    "    print(f\"\\n{stats['模型']}:\")\n",
    "    if i < len(top_models):\n",
    "        print(f\"  CV R^2: {stats['CV_R2']:.4f}\")\n",
    "    print(f\"  预测范围: {stats['预测最小值']:.2f} - {stats['预测最大值']:.2f} m\")\n",
    "    print(f\"  平均值: {stats['预测平均值']:.2f} m\")\n",
    "    print(f\"  标准差: {stats['预测标准差']:.2f} m\")\n",
    "    print(f\"  厚度>1m的点数: {stats['大于1m的点数']} ({stats['大于1m的点数'] / len(seismic_data) * 100:.1f}%)\")\n",
    "    print(f\"  厚度>5m的点数: {stats['大于5m的点数']} ({stats['大于5m的点数'] / len(seismic_data) * 100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n=== 所有结果已保存到目录: {output_dir} ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-af",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
