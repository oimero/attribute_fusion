{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15195d7",
   "metadata": {},
   "source": [
    "# 虚拟井生成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090bca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保src目录在Python路径中\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LassoCV, LinearRegression, RidgeCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "# 导入模块\n",
    "from src.data_utils import (\n",
    "    extract_uniform_seismic_samples,\n",
    "    filter_anomalous_attributes,\n",
    "    filter_seismic_by_wells,\n",
    "    identify_attributes,\n",
    "    parse_petrel_file,\n",
    ")\n",
    "from src.feature_selection import select_best_features\n",
    "from src.visualization import visualize_attribute_map\n",
    "\n",
    "output_dir = \"output\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = \"SimHei\"  # 黑体 SimHei 支持中文\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False  # 正常显示负号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db20df",
   "metadata": {},
   "source": [
    "## 导入地震数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb9fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_H6_2_attr = parse_petrel_file(\"../data/H6-2_attr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4dc3e2",
   "metadata": {},
   "source": [
    "## 导入井震数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_H6_2_well = \"../data/well_processed.xlsx\"\n",
    "data_H6_2_well = pd.read_excel(file_H6_2_well, sheet_name=\"Sheet1\")\n",
    "\n",
    "# 只选择层位（Surface）为 H6-2 的行，并丢弃砂厚为 NaN 的行\n",
    "data_H6_2_well_selected = (\n",
    "    data_H6_2_well[data_H6_2_well[\"Surface\"] == \"H6-2\"]\n",
    "    .query(\"Well != 'PH6' and Well != 'PH8' and Well != 'PH3' and Well != 'PH2'\")\n",
    "    .replace(-999, np.nan)  # 将-999替换为NaN（通常-999是缺失值的代码）\n",
    "    .dropna(subset=[\"Thickness of facies(1: Fine sand)\"])  # 丢弃砂厚为NaN的行\n",
    "    .reset_index(drop=True)  # 重置索引\n",
    ")\n",
    "\n",
    "# 显示筛选后的前几行数据\n",
    "data_H6_2_well_selected.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5756f454",
   "metadata": {},
   "source": [
    "## 提取共同属性\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cf0842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取地震属性列表\n",
    "seismic_attr, _ = identify_attributes(\"../data/H6-2_attr\")\n",
    "\n",
    "# 提取Excel的属性列表（从第8列开始的所有列）\n",
    "well_seismic_attr = data_H6_2_well.columns[7:].tolist()\n",
    "\n",
    "# 计算两个列表的交集\n",
    "common_attributes = list(set(seismic_attr) & set(well_seismic_attr))\n",
    "\n",
    "# 打印结果\n",
    "print(f\"地震属性数量: {len(seismic_attr)}\")\n",
    "print(f\"Excel属性数量: {len(well_seismic_attr)}\")\n",
    "print(f\"共同属性数量: {len(common_attributes)}\")\n",
    "print(\"\\n共同属性列表:\")\n",
    "for attr in common_attributes:\n",
    "    print(f\"- {attr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80684df4",
   "metadata": {},
   "source": [
    "## 根据井点分布，缩小工区范围\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77821d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 限制工区范围\n",
    "data_H6_2_attr_filtered, area_bounds = filter_seismic_by_wells(\n",
    "    seismic_data=data_H6_2_attr,\n",
    "    well_data=data_H6_2_well_selected,\n",
    "    expansion_factor=1.5,  # 扩展50%\n",
    "    plot=True,\n",
    "    output_dir=output_dir,\n",
    ")\n",
    "\n",
    "# 后续可以直接使用area_bounds中的边界信息\n",
    "print(\"区域边界信息:\")\n",
    "for key, value in area_bounds.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bff370",
   "metadata": {},
   "source": [
    "## 生成统计摘要\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e80ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选出质量良好的属性\n",
    "good_attributes, anomalous_attributes, attribute_stats = filter_anomalous_attributes(\n",
    "    seismic_data=data_H6_2_attr_filtered,\n",
    "    well_data=data_H6_2_well_selected,\n",
    "    common_attributes=common_attributes,\n",
    "    ratio_threshold=5.0,  # 均值比值阈值\n",
    "    range_ratio_threshold=10.0,  # 数值范围比值阈值\n",
    "    std_ratio_threshold=10.0,  # 标准差比值阈值\n",
    "    output_dir=None,  # 输出图表目录\n",
    "    verbose=True,  # 打印详细信息\n",
    ")\n",
    "\n",
    "print(\"\\n筛选后保留的质量良好属性:\")\n",
    "for attr in good_attributes:\n",
    "    print(f\"- {attr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4bdff3",
   "metadata": {},
   "source": [
    "## 随机森林重要性和相关性分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd0117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用随机森林评估特征重要性并移除冗余特征\n",
    "selected_features = select_best_features(\n",
    "    well_data=data_H6_2_well_selected,\n",
    "    attribute_columns=good_attributes,\n",
    "    target_column=\"Thickness of facies(1: Fine sand)\",\n",
    "    n_features=3,\n",
    "    corr_threshold=0.85,\n",
    "    output_dir=output_dir,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 输出特征选择结果\n",
    "print(\"\\n基于随机森林重要性和相关性分析的最佳特征:\")\n",
    "for i, feature in enumerate(selected_features):\n",
    "    print(f\"{i + 1}. {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6418d68",
   "metadata": {},
   "source": [
    "## 提取样本，准备设置虚拟井\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd54b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用筛选后的地震数据区域提取等间距样本\n",
    "seismic_samples = extract_uniform_seismic_samples(\n",
    "    seismic_data=data_H6_2_attr_filtered,\n",
    "    n_rows=40,\n",
    "    n_cols=40,\n",
    "    area_bounds=area_bounds,  # 直接传入边界字典\n",
    ")\n",
    "\n",
    "# 可视化真实井点和采样点\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 绘制地震数据点（使用抽样）\n",
    "sample_ratio = min(1.0, 5000 / len(data_H6_2_attr_filtered))\n",
    "seismic_sample = data_H6_2_attr_filtered.sample(frac=sample_ratio)\n",
    "plt.scatter(seismic_sample[\"X\"], seismic_sample[\"Y\"], color=\"lightgray\", alpha=0.3, s=10, label=\"地震数据(抽样)\")\n",
    "\n",
    "# 绘制真实井点位置\n",
    "plt.scatter(\n",
    "    data_H6_2_well_selected[\"X\"], data_H6_2_well_selected[\"Y\"], color=\"red\", s=100, marker=\"^\", label=\"真实井点\"\n",
    ")\n",
    "\n",
    "# 绘制等间距采样点位置\n",
    "plt.scatter(seismic_samples[\"X\"], seismic_samples[\"Y\"], color=\"blue\", s=50, marker=\"o\", label=\"等间距采样点\")\n",
    "\n",
    "# 添加标题和图例\n",
    "plt.title(\"真实井点与等间距采样点分布\", fontsize=16)\n",
    "plt.xlabel(\"X坐标\", fontsize=14)\n",
    "plt.ylabel(\"Y坐标\", fontsize=14)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# 保存图片\n",
    "plt.savefig(os.path.join(output_dir, \"real_wells_and_seismic_samples.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 保存提取的样本数据\n",
    "seismic_samples.to_csv(os.path.join(output_dir, \"seismic_samples.csv\"), index=False)\n",
    "print(f\"等间距地震样本数据已保存至 {os.path.join(output_dir, 'seismic_samples.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f8fc8",
   "metadata": {},
   "source": [
    "## 多线性模型一致性预测设置虚拟井\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efd7ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建融合属性和多模型预测\n",
    "print(\"======== 创建融合属性和多模型预测 ========\")\n",
    "target_column = \"Thickness of facies(1: Fine sand)\"\n",
    "min_corr_threshold = 0.2  # 最小相关性阈值，低于此值的属性将被排除\n",
    "\n",
    "# 检查每个选定属性在井点数据中的有效性\n",
    "print(\"检查属性在井点数据中的有效性:\")\n",
    "for feature in selected_features:\n",
    "    nan_count = data_H6_2_well_selected[feature].isna().sum()\n",
    "    print(\n",
    "        f\"属性 '{feature}' 在井点数据中的NaN值数量: {nan_count}/{len(data_H6_2_well_selected)} ({nan_count / len(data_H6_2_well_selected) * 100:.1f}%)\"\n",
    "    )\n",
    "\n",
    "# 筛选出所有选定属性都有有效值的井点\n",
    "valid_wells = data_H6_2_well_selected.dropna(subset=selected_features + [target_column])\n",
    "print(f\"\\n所有属性都有有效值的井点数量: {len(valid_wells)} / {len(data_H6_2_well_selected)}\")\n",
    "\n",
    "# 准备训练数据\n",
    "X_labeled = valid_wells[selected_features].values\n",
    "y_labeled = valid_wells[target_column].values\n",
    "\n",
    "# 准备未标记数据（地震样本点）\n",
    "X_unlabeled = seismic_samples[selected_features].dropna().values\n",
    "unlabeled_indices = seismic_samples[selected_features].dropna().index\n",
    "\n",
    "# 1. 融合属性线性加权预测\n",
    "print(\"\\n=== 模型1: 融合属性线性加权 ===\")\n",
    "\n",
    "# 计算相关性权重\n",
    "correlation_weights = {}\n",
    "for i, feature in enumerate(selected_features):\n",
    "    corr, _ = spearmanr(valid_wells[feature], valid_wells[target_column])\n",
    "    if abs(corr) >= min_corr_threshold:\n",
    "        correlation_weights[feature] = corr\n",
    "        print(f\"属性 '{feature}' 与砂厚的Spearman相关性: {corr:.4f}\")\n",
    "    else:\n",
    "        print(f\"属性 '{feature}' 与砂厚的相关性过低 ({corr:.4f})，不纳入融合\")\n",
    "\n",
    "# 如果没有有效属性，使用所有属性且权重相等\n",
    "if len(correlation_weights) == 0:\n",
    "    print(\"警告: 没有属性满足相关性阈值，将使用所有属性且权重相等\")\n",
    "    for feature in selected_features:\n",
    "        correlation_weights[feature] = 1.0\n",
    "        print(f\"属性 '{feature}' 使用默认权重: 1.0\")\n",
    "\n",
    "# 标准化数据\n",
    "scaler = StandardScaler()\n",
    "X_labeled_scaled = scaler.fit_transform(X_labeled)\n",
    "X_unlabeled_scaled = scaler.transform(X_unlabeled)\n",
    "\n",
    "\n",
    "# 创建融合属性函数\n",
    "def create_fused_attribute(X_scaled, features, weights):\n",
    "    \"\"\"\n",
    "    基于选定特征和权重创建融合属性\n",
    "\n",
    "    参数:\n",
    "        X_scaled (ndarray): 标准化后的特征矩阵\n",
    "        features (list): 特征列表\n",
    "        weights (dict): 每个特征的权重\n",
    "\n",
    "    返回:\n",
    "        ndarray: 融合属性\n",
    "    \"\"\"\n",
    "    # 初始化融合属性\n",
    "    fused_attr = np.zeros(X_scaled.shape[0])\n",
    "    weight_sum = 0\n",
    "\n",
    "    # 对每个特征进行加权融合\n",
    "    for i, feature in enumerate(features):\n",
    "        if feature in weights:\n",
    "            weight = weights[feature]\n",
    "            fused_attr += X_scaled[:, i] * weight\n",
    "            weight_sum += abs(weight)\n",
    "\n",
    "    # 归一化融合结果\n",
    "    if weight_sum > 0:\n",
    "        fused_attr /= weight_sum\n",
    "\n",
    "    return fused_attr\n",
    "\n",
    "\n",
    "# 在训练数据上创建融合属性\n",
    "fused_attr_labeled = create_fused_attribute(X_labeled_scaled, selected_features, correlation_weights)\n",
    "\n",
    "# 在未标记数据上创建融合属性\n",
    "fused_attr_unlabeled = create_fused_attribute(X_unlabeled_scaled, selected_features, correlation_weights)\n",
    "\n",
    "# 使用融合属性拟合线性回归\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(fused_attr_labeled.reshape(-1, 1), y_labeled)\n",
    "\n",
    "# 预测\n",
    "fused_pred_labeled = linear_model.predict(fused_attr_labeled.reshape(-1, 1))\n",
    "fused_pred_unlabeled = linear_model.predict(fused_attr_unlabeled.reshape(-1, 1))\n",
    "\n",
    "# 评估融合属性预测效果\n",
    "fused_corr = np.corrcoef(fused_pred_labeled, y_labeled)[0, 1]\n",
    "print(f\"融合属性预测结果与真实砂厚的相关性: {fused_corr:.4f}\")\n",
    "\n",
    "# 2. Lasso回归 + Bootstrap\n",
    "print(\"\\n=== 模型2: LassoCV + Bootstrap ===\")\n",
    "\n",
    "n_bootstrap = 100  # Bootstrap重采样次数\n",
    "alpha_values = np.logspace(-4, 1, 30)  # alpha候选值\n",
    "\n",
    "# 主Lasso模型\n",
    "lasso_model = LassoCV(alphas=alpha_values, cv=5, max_iter=10000, tol=1e-3)\n",
    "lasso_model.fit(X_labeled_scaled, y_labeled)\n",
    "print(f\"Lasso最优alpha值: {lasso_model.alpha_:.6f}\")\n",
    "\n",
    "# 在训练数据上的预测\n",
    "lasso_pred_labeled = lasso_model.predict(X_labeled_scaled)\n",
    "lasso_corr = np.corrcoef(lasso_pred_labeled, y_labeled)[0, 1]\n",
    "print(f\"Lasso预测结果与真实砂厚的相关性: {lasso_corr:.4f}\")\n",
    "\n",
    "# Bootstrap重采样预测\n",
    "lasso_bootstrap_preds = np.zeros((n_bootstrap, X_unlabeled_scaled.shape[0]))\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    # Bootstrap重采样\n",
    "    indices = np.random.choice(len(X_labeled_scaled), len(X_labeled_scaled), replace=True)\n",
    "    X_boot, y_boot = X_labeled_scaled[indices], y_labeled[indices]\n",
    "\n",
    "    # 拟合模型\n",
    "    lasso_boot = LassoCV(alphas=alpha_values, cv=5, max_iter=10000, tol=1e-3)\n",
    "    lasso_boot.fit(X_boot, y_boot)\n",
    "\n",
    "    # 预测\n",
    "    lasso_bootstrap_preds[i, :] = lasso_boot.predict(X_unlabeled_scaled)\n",
    "\n",
    "# 计算预测均值和置信区间\n",
    "lasso_pred_unlabeled = np.mean(lasso_bootstrap_preds, axis=0)\n",
    "lasso_lower_ci = np.percentile(lasso_bootstrap_preds, 2.5, axis=0)\n",
    "lasso_upper_ci = np.percentile(lasso_bootstrap_preds, 97.5, axis=0)\n",
    "\n",
    "# 3. Ridge回归 + Bootstrap\n",
    "print(\"\\n=== 模型3: RidgeCV + Bootstrap ===\")\n",
    "\n",
    "alpha_values = np.logspace(-3, 3, 30)  # alpha候选值\n",
    "\n",
    "# 主Ridge模型\n",
    "ridge_model = RidgeCV(alphas=alpha_values, cv=5)\n",
    "ridge_model.fit(X_labeled_scaled, y_labeled)\n",
    "print(f\"Ridge最优alpha值: {ridge_model.alpha_:.6f}\")\n",
    "\n",
    "# 在训练数据上的预测\n",
    "ridge_pred_labeled = ridge_model.predict(X_labeled_scaled)\n",
    "ridge_corr = np.corrcoef(ridge_pred_labeled, y_labeled)[0, 1]\n",
    "print(f\"Ridge预测结果与真实砂厚的相关性: {ridge_corr:.4f}\")\n",
    "\n",
    "# Bootstrap重采样预测\n",
    "ridge_bootstrap_preds = np.zeros((n_bootstrap, X_unlabeled_scaled.shape[0]))\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    # Bootstrap重采样\n",
    "    indices = np.random.choice(len(X_labeled_scaled), len(X_labeled_scaled), replace=True)\n",
    "    X_boot, y_boot = X_labeled_scaled[indices], y_labeled[indices]\n",
    "\n",
    "    # 拟合模型\n",
    "    ridge_boot = RidgeCV(alphas=alpha_values, cv=5)\n",
    "    ridge_boot.fit(X_boot, y_boot)\n",
    "\n",
    "    # 预测\n",
    "    ridge_bootstrap_preds[i, :] = ridge_boot.predict(X_unlabeled_scaled)\n",
    "\n",
    "# 计算预测均值和置信区间\n",
    "ridge_pred_unlabeled = np.mean(ridge_bootstrap_preds, axis=0)\n",
    "ridge_lower_ci = np.percentile(ridge_bootstrap_preds, 2.5, axis=0)\n",
    "ridge_upper_ci = np.percentile(ridge_bootstrap_preds, 97.5, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0acb61",
   "metadata": {},
   "source": [
    "## 平衡区间分布与质量控制的伪样本筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29883820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 平衡分布与质量控制的优化采样\n",
    "print(\"\\n=== 平衡分布与质量控制的优化采样 ===\")\n",
    "\n",
    "# 将三个模型的预测结果整合\n",
    "predictions = np.column_stack([fused_pred_unlabeled, lasso_pred_unlabeled, ridge_pred_unlabeled])\n",
    "\n",
    "# 计算每个点三个预测值的最大差异\n",
    "max_diffs = np.max(predictions, axis=1) - np.min(predictions, axis=1)\n",
    "\n",
    "# 计算平均预测值用于相对差异计算\n",
    "mean_preds = np.mean(predictions, axis=1)\n",
    "\n",
    "# 预先处理预测值，将负值替换为0\n",
    "mean_preds = np.maximum(mean_preds, 0)\n",
    "fused_pred_unlabeled = np.maximum(fused_pred_unlabeled, 0)\n",
    "lasso_pred_unlabeled = np.maximum(lasso_pred_unlabeled, 0)\n",
    "ridge_pred_unlabeled = np.maximum(ridge_pred_unlabeled, 0)\n",
    "\n",
    "# 计算bootstrap结果的标准差\n",
    "lasso_std = np.std(lasso_bootstrap_preds, axis=0)\n",
    "ridge_std = np.std(ridge_bootstrap_preds, axis=0)\n",
    "\n",
    "# 定义砂厚区间\n",
    "bins = [0, 0.1, 5, 10, np.inf]\n",
    "bin_labels = [\"0-0.1\", \"0.1-5\", \"5-10\", \">10\"]\n",
    "\n",
    "# 将预测结果分配到各个区间\n",
    "bin_indices = []\n",
    "for i in range(len(bins) - 1):\n",
    "    mask = (mean_preds >= bins[i]) & (mean_preds < bins[i + 1])\n",
    "    bin_indices.append(np.where(mask)[0])\n",
    "\n",
    "# 计算各区间样本数量\n",
    "bin_counts = [len(indices) for indices in bin_indices]\n",
    "print(\"各砂厚区间原始分布:\")\n",
    "for i, count in enumerate(bin_counts):\n",
    "    print(f\"  {bin_labels[i]}: {count}个 ({count / len(mean_preds) * 100:.1f}%)\")\n",
    "\n",
    "# 设定目标分布 - 根据实际数据调整\n",
    "# 使用更接近现实数据分布的目标分布\n",
    "target_distribution = np.array([0.40, 0.30, 0.20, 0.10])\n",
    "\n",
    "# 目标样本总数（最小200个）\n",
    "target_total_samples = max(200, int(0.3 * len(mean_preds)))\n",
    "min_samples_per_bin = 15  # 每个区间的最小样本数\n",
    "\n",
    "# 计算每个区间的目标样本数\n",
    "target_bin_samples = np.round(target_distribution * target_total_samples).astype(int)\n",
    "# 确保每个区间至少有最小样本数\n",
    "target_bin_samples = np.maximum(target_bin_samples, min_samples_per_bin)\n",
    "# 调整以匹配总目标\n",
    "target_bin_samples = np.round(target_bin_samples / sum(target_bin_samples) * target_total_samples).astype(int)\n",
    "\n",
    "print(\"\\n目标样本分布:\")\n",
    "for i, count in enumerate(target_bin_samples):\n",
    "    print(f\"  {bin_labels[i]}: 目标{count}个样本 ({count / sum(target_bin_samples) * 100:.1f}%)\")\n",
    "\n",
    "\n",
    "# 计算连续的质量分数函数\n",
    "def calculate_quality_score(mean_pred, max_diff, rel_diff, lasso_std, ridge_std):\n",
    "    \"\"\"计算样本的质量分数（0-1，越高越好）\"\"\"\n",
    "    # 基础权重\n",
    "    w_consistency = 0.6  # 一致性权重\n",
    "    w_uncertainty = 0.4  # 不确定性权重\n",
    "\n",
    "    # 统一使用基本阈值，不再区分小值/大值区域\n",
    "    abs_threshold = 4.0  # 统一的绝对差异阈值\n",
    "    rel_threshold = 0.2  # 统一的相对差异阈值\n",
    "    std_threshold = 3.5  # 统一的标准差阈值\n",
    "\n",
    "    # 计算一致性得分 (0-1)\n",
    "    abs_score = max(0, 1 - max_diff / abs_threshold)\n",
    "    rel_score = max(0, 1 - rel_diff / rel_threshold)\n",
    "    consistency_score = 0.5 * abs_score + 0.5 * rel_score\n",
    "\n",
    "    # 计算不确定性得分 (0-1)\n",
    "    lasso_std_score = max(0, 1 - lasso_std / std_threshold)\n",
    "    ridge_std_score = max(0, 1 - ridge_std / std_threshold)\n",
    "    uncertainty_score = 0.5 * lasso_std_score + 0.5 * ridge_std_score\n",
    "\n",
    "    # 组合得分\n",
    "    final_score = w_consistency * consistency_score + w_uncertainty * uncertainty_score\n",
    "\n",
    "    return final_score\n",
    "\n",
    "\n",
    "# 计算所有样本的质量分数\n",
    "rel_diffs = max_diffs / (mean_preds + 1e-10)  # 避免除零\n",
    "quality_scores = np.zeros(len(mean_preds))\n",
    "\n",
    "for i in range(len(mean_preds)):\n",
    "    quality_scores[i] = calculate_quality_score(mean_preds[i], max_diffs[i], rel_diffs[i], lasso_std[i], ridge_std[i])\n",
    "\n",
    "# 定义最低质量阈值\n",
    "min_quality_threshold = 0.5  # 不接受低于此质量的样本\n",
    "\n",
    "\n",
    "# 动态阈值调整：为稀缺区间放宽标准\n",
    "def get_dynamic_quality_threshold(bin_idx, sample_ratio):\n",
    "    \"\"\"\n",
    "    根据区间样本充足程度动态调整质量阈值\n",
    "\n",
    "    参数:\n",
    "        bin_idx: 区间索引\n",
    "        sample_ratio: 当前样本数/目标样本数\n",
    "\n",
    "    返回:\n",
    "        调整后的质量阈值\n",
    "    \"\"\"\n",
    "    base_threshold = min_quality_threshold\n",
    "\n",
    "    # 高砂厚区间（bin_idx为2或3）对质量更加宽容\n",
    "    if bin_idx >= 2:\n",
    "        base_threshold = max(0.4, base_threshold - 0.1)  # 高砂厚区间基准阈值更低\n",
    "\n",
    "    # 如果样本不足，降低阈值，但不低于最低阈值\n",
    "    if sample_ratio < 0.5:  # 严重不足\n",
    "        return max(0.3, base_threshold - 0.2)\n",
    "    elif sample_ratio < 0.8:  # 轻微不足\n",
    "        return max(0.4, base_threshold - 0.1)\n",
    "    else:  # 接近或超过目标\n",
    "        return base_threshold\n",
    "\n",
    "\n",
    "# 计算J评分函数\n",
    "def calculate_j_score(selected_indices, quality_scores, mean_preds, target_dist, bins):\n",
    "    \"\"\"计算当前选择的J评分\"\"\"\n",
    "    if len(selected_indices) == 0:\n",
    "        return 0\n",
    "\n",
    "    # 1. 计算质量分数\n",
    "    selected_quality = quality_scores[selected_indices]\n",
    "    Q_total = np.mean(selected_quality)\n",
    "\n",
    "    # 2. 计算分布相似度\n",
    "    selected_values = mean_preds[selected_indices]\n",
    "    hist, _ = np.histogram(selected_values, bins=bins)\n",
    "    current_dist = hist / (np.sum(hist) + 1e-10)\n",
    "\n",
    "    # Jensen-Shannon散度\n",
    "    from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "    jsd = jensenshannon(current_dist, target_dist)\n",
    "    dist_similarity = 1 - jsd\n",
    "\n",
    "    # 3. 计算样本数量得分\n",
    "    n_samples = len(selected_indices)\n",
    "    min_samples = 200\n",
    "    max_samples = 1000\n",
    "    quantity_score = min(1.0, max(0.0, (n_samples - min_samples) / (max_samples - min_samples)))\n",
    "\n",
    "    # 计算J值 - 平衡质量和分布\n",
    "    # 降低分布的权重，增加质量权重，使算法更灵活地选择高质量样本\n",
    "    J = 0.5 * Q_total + 0.3 * dist_similarity + 0.2 * quantity_score\n",
    "\n",
    "    return J\n",
    "\n",
    "\n",
    "# 迭代优化样本选择\n",
    "print(\"\\n开始迭代优化样本选择...\")\n",
    "\n",
    "# 初始化选择的样本\n",
    "selected_indices = np.array([], dtype=int)  # 使用空NumPy数组而不是空列表\n",
    "max_iterations = 10\n",
    "current_j = 0\n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    print(f\"\\n迭代 {iteration + 1}/{max_iterations}\")\n",
    "\n",
    "    # 1. 计算当前各区间已选样本数\n",
    "    current_bin_samples = [0] * len(bin_labels)\n",
    "    if len(selected_indices) > 0:  # 修改条件检查方式\n",
    "        selected_values = mean_preds[selected_indices]\n",
    "        for i in range(len(bins) - 1):\n",
    "            current_bin_samples[i] = np.sum((selected_values >= bins[i]) & (selected_values < bins[i + 1]))\n",
    "\n",
    "    # 2. 识别样本不足的区间\n",
    "    bin_ratios = np.zeros(len(bins) - 1)\n",
    "    for i in range(len(bins) - 1):\n",
    "        bin_ratios[i] = current_bin_samples[i] / target_bin_samples[i] if target_bin_samples[i] > 0 else 1.0\n",
    "\n",
    "    # 按照样本充足率排序区间（优先处理最不足的区间）\n",
    "    bin_priority = np.argsort(bin_ratios)\n",
    "\n",
    "    improved = False\n",
    "\n",
    "    # 3. 逐个处理各区间\n",
    "    for bin_idx in bin_priority:\n",
    "        if bin_ratios[bin_idx] >= 1.0:  # 该区间样本已经足够\n",
    "            continue\n",
    "\n",
    "        # 4. 动态调整质量阈值\n",
    "        quality_threshold = get_dynamic_quality_threshold(bin_idx, bin_ratios[bin_idx])\n",
    "\n",
    "        # 5. 获取该区间的候选样本\n",
    "        candidates = bin_indices[bin_idx]\n",
    "\n",
    "        # 排除已选择的样本\n",
    "        candidates = np.setdiff1d(candidates, selected_indices)\n",
    "\n",
    "        if len(candidates) == 0:\n",
    "            print(f\"  区间 {bin_labels[bin_idx]} 没有更多可用样本\")\n",
    "            continue\n",
    "\n",
    "        # 6. 按质量排序\n",
    "        candidates = candidates[np.argsort(-quality_scores[candidates])]\n",
    "\n",
    "        # 7. 筛选符合质量要求的样本\n",
    "        qualified_candidates = candidates[quality_scores[candidates] >= quality_threshold]\n",
    "\n",
    "        if len(qualified_candidates) == 0:\n",
    "            print(f\"  区间 {bin_labels[bin_idx]} 没有符合质量要求(>{quality_threshold:.2f})的样本\")\n",
    "            continue\n",
    "\n",
    "        # 8. 计算需要添加的样本数\n",
    "        needed = target_bin_samples[bin_idx] - current_bin_samples[bin_idx]\n",
    "        to_add = min(needed, len(qualified_candidates))\n",
    "\n",
    "        if to_add <= 0:\n",
    "            continue\n",
    "\n",
    "        print(f\"  向区间 {bin_labels[bin_idx]} 添加 {to_add} 个样本 (质量阈值>{quality_threshold:.2f})\")\n",
    "\n",
    "        # 9. 添加样本并评估J值变化\n",
    "        new_selected = (\n",
    "            np.concatenate([selected_indices, qualified_candidates[:to_add]])\n",
    "            if len(selected_indices) > 0\n",
    "            else qualified_candidates[:to_add]\n",
    "        )\n",
    "        new_j = calculate_j_score(new_selected, quality_scores, mean_preds, target_distribution, bins)\n",
    "\n",
    "        # 10. 如果J值提高，接受新样本\n",
    "        if new_j > current_j or len(selected_indices) == 0:\n",
    "            selected_indices = new_selected\n",
    "            current_j = new_j\n",
    "            improved = True\n",
    "            print(f\"  J值提高到 {current_j:.4f}\")\n",
    "\n",
    "    # 如果本轮迭代没有改进，提前结束\n",
    "    if not improved:\n",
    "        print(\"  本轮迭代无改进，提前结束\")\n",
    "        break\n",
    "\n",
    "    # 打印当前状态\n",
    "    selected_values = mean_preds[selected_indices]\n",
    "    current_hist, _ = np.histogram(selected_values, bins=bins)\n",
    "    print(f\"  当前已选择 {len(selected_indices)} 个样本，J值 = {current_j:.4f}\")\n",
    "    for i in range(len(bins) - 1):\n",
    "        print(f\"    {bin_labels[i]}: {current_hist[i]}个 ({current_hist[i] / len(selected_indices) * 100:.1f}%)\")\n",
    "\n",
    "# 最终确定的样本索引\n",
    "final_selected_indices = selected_indices\n",
    "\n",
    "# 创建最终掩码\n",
    "final_mask = np.zeros(len(mean_preds), dtype=bool)\n",
    "final_mask[final_selected_indices] = True\n",
    "\n",
    "# 统计最终结果\n",
    "print(\"\\n=== 最终样本选择结果 ===\")\n",
    "final_hist, _ = np.histogram(mean_preds[final_selected_indices], bins=bins)\n",
    "print(f\"总计选择样本: {len(final_selected_indices)}个\")\n",
    "for i in range(len(bins) - 1):\n",
    "    original_count = bin_counts[i]\n",
    "    final_count = final_hist[i]\n",
    "    print(\n",
    "        f\"砂厚范围 {bin_labels[i]}: {final_count}个 ({final_count / len(final_selected_indices) * 100:.1f}%) | 原始: {original_count}个\"\n",
    "    )\n",
    "\n",
    "# 统计最终质量分数\n",
    "final_quality = quality_scores[final_selected_indices]\n",
    "print(f\"\\n样本质量统计:\")\n",
    "print(f\"  平均质量分数: {np.mean(final_quality):.4f}\")\n",
    "print(f\"  最低质量分数: {np.min(final_quality):.4f}\")\n",
    "print(f\"  最高质量分数: {np.max(final_quality):.4f}\")\n",
    "\n",
    "# 创建伪标记数据\n",
    "X_pseudo = X_unlabeled[final_selected_indices]\n",
    "y_pseudo = mean_preds[final_selected_indices]\n",
    "\n",
    "# 获取优化样本在原始seismic_samples中的索引\n",
    "optimized_orig_indices = unlabeled_indices[final_selected_indices]\n",
    "\n",
    "# 添加预测结果到seismic_samples\n",
    "seismic_samples[\"Fused_Pred\"] = np.nan\n",
    "seismic_samples[\"Lasso_Pred\"] = np.nan\n",
    "seismic_samples[\"Ridge_Pred\"] = np.nan\n",
    "seismic_samples[\"Mean_Pred\"] = np.nan\n",
    "seismic_samples[\"Max_Diff\"] = np.nan\n",
    "seismic_samples[\"Quality_Score\"] = np.nan\n",
    "seismic_samples[\"Is_Selected\"] = False\n",
    "\n",
    "# 填充预测结果\n",
    "seismic_samples.loc[unlabeled_indices, \"Fused_Pred\"] = fused_pred_unlabeled\n",
    "seismic_samples.loc[unlabeled_indices, \"Lasso_Pred\"] = lasso_pred_unlabeled\n",
    "seismic_samples.loc[unlabeled_indices, \"Ridge_Pred\"] = ridge_pred_unlabeled\n",
    "seismic_samples.loc[unlabeled_indices, \"Mean_Pred\"] = mean_preds\n",
    "seismic_samples.loc[unlabeled_indices, \"Max_Diff\"] = max_diffs\n",
    "seismic_samples.loc[unlabeled_indices, \"Quality_Score\"] = quality_scores\n",
    "seismic_samples.loc[optimized_orig_indices, \"Is_Selected\"] = True\n",
    "\n",
    "# 统计并报告负值数量\n",
    "neg_count = (seismic_samples[\"Mean_Pred\"] < 0).sum()\n",
    "if neg_count > 0:\n",
    "    print(f\"\\n注意: 有 {neg_count} 个负的砂厚预测值已被替换为0\")\n",
    "\n",
    "# 将负数的Mean_Pred值置为0\n",
    "seismic_samples[\"Mean_Pred\"] = seismic_samples[\"Mean_Pred\"].clip(lower=0)\n",
    "\n",
    "# 可视化优化前后的分布变化\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# 原始分布\n",
    "bin_percents = np.array(bin_counts) / sum(bin_counts) * 100\n",
    "plt.bar(np.arange(len(bin_labels)) - 0.2, bin_percents, width=0.4, label=\"原始分布\", color=\"lightblue\")\n",
    "\n",
    "# 优化后分布\n",
    "optimized_percents = final_hist / sum(final_hist) * 100\n",
    "plt.bar(np.arange(len(bin_labels)) + 0.2, optimized_percents, width=0.4, label=\"优化后分布\", color=\"orange\")\n",
    "\n",
    "# 添加目标分布线\n",
    "target_percents = target_distribution / sum(target_distribution) * 100\n",
    "plt.plot(np.arange(len(bin_labels)), target_percents, \"r--\", label=\"目标分布\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"砂厚区间\")\n",
    "plt.ylabel(\"百分比 (%)\")\n",
    "plt.title(\"样本分布优化前后对比\")\n",
    "plt.xticks(np.arange(len(bin_labels)), bin_labels)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "plt.savefig(os.path.join(output_dir, \"distribution_optimization.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# 保存预测结果\n",
    "seismic_samples.to_csv(os.path.join(output_dir, \"seismic_samples_with_predictions.csv\"), index=False)\n",
    "print(f\"\\n预测结果已保存至 {os.path.join(output_dir, 'seismic_samples_with_predictions.csv')}\")\n",
    "\n",
    "# 保存优化后的虚拟井数据\n",
    "optimized_samples = seismic_samples[seismic_samples[\"Is_Selected\"] == True].copy()\n",
    "optimized_samples.to_csv(os.path.join(output_dir, \"optimized_pseudo_wells.csv\"), index=False)\n",
    "print(f\"优化后的虚拟井数据已保存至 {os.path.join(output_dir, 'optimized_pseudo_wells.csv')}\")\n",
    "\n",
    "# 计算质量-分布的J指标\n",
    "final_j = calculate_j_score(final_selected_indices, quality_scores, mean_preds, target_distribution, bins)\n",
    "print(f\"\\n最终优化J评分: {final_j:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d87c3",
   "metadata": {},
   "source": [
    "## 展示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855d32a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备小工区地震数据（使用已筛选的data_H6_2_attr_filtered）\n",
    "# 1. 首先创建融合属性\n",
    "print(\"为整个小工区地震数据创建融合属性...\")\n",
    "\n",
    "# 标准化数据\n",
    "scaler = StandardScaler()\n",
    "attr_data = data_H6_2_attr_filtered[selected_features].copy()\n",
    "attr_data_scaled = scaler.fit_transform(attr_data)\n",
    "\n",
    "# 使用之前定义的create_fused_attribute函数创建融合属性\n",
    "fused_attr = create_fused_attribute(attr_data_scaled, selected_features, correlation_weights)\n",
    "\n",
    "# 将融合属性添加到地震数据中\n",
    "data_H6_2_attr_filtered[\"Fused_Attribute\"] = fused_attr\n",
    "\n",
    "# 2. 准备真实井点数据\n",
    "real_wells = data_H6_2_well_selected\n",
    "\n",
    "# 3. 准备虚拟井点数据（使用之前优化选择的虚拟井）\n",
    "pseudo_wells = seismic_samples[seismic_samples[\"Is_Selected\"] == True].copy()\n",
    "\n",
    "# 可视化融合属性分布与井点位置\n",
    "visualize_attribute_map(\n",
    "    data_points=data_H6_2_attr_filtered,\n",
    "    attribute_name=\"Fused_Attribute\",\n",
    "    attribute_label=\"地震融合属性值\",\n",
    "    real_wells=real_wells,\n",
    "    pseudo_wells=pseudo_wells,\n",
    "    target_column=\"Thickness of facies(1: Fine sand)\",\n",
    "    output_dir=output_dir,\n",
    "    filename_prefix=\"fused_attribute\",\n",
    "    class_thresholds=[0.1, 10],\n",
    "    figsize=(16, 14),\n",
    "    dpi=300,\n",
    "    cmap=\"viridis\",\n",
    "    point_size=140,\n",
    "    well_size=200,\n",
    ")\n",
    "\n",
    "print(\"融合属性分布与井点位置可视化完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e3208c",
   "metadata": {},
   "source": [
    "## SVR、随机森林（限制树深）、XGBoost（控制学习率和复杂度）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c370044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子以确保结果可复现\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"======== 真实井点与虚拟井点结合建模 ========\")\n",
    "\n",
    "# 步骤1: 准备数据 - 合并真实样本和伪样本\n",
    "pseudo_wells = pd.read_csv(os.path.join(output_dir, \"optimized_pseudo_wells.csv\"))\n",
    "print(f\"加载了 {len(pseudo_wells)} 个虚拟井点数据\")\n",
    "\n",
    "# 创建用于建模的真实井点数据\n",
    "real_wells = data_H6_2_well_selected.copy()\n",
    "print(f\"真实井点数据数量: {len(real_wells)}\")\n",
    "\n",
    "# 确定共同的特征列（选用之前筛选出的最佳特征）\n",
    "common_features = selected_features.copy()\n",
    "print(f\"使用的特征: {common_features}\")\n",
    "\n",
    "# 确保真实井点和虚拟井点都有这些特征列\n",
    "real_wells_valid = real_wells.dropna(subset=common_features + [\"Thickness of facies(1: Fine sand)\"])\n",
    "print(f\"有效真实井点数据数量: {len(real_wells_valid)} (丢弃了缺失值)\")\n",
    "\n",
    "# 创建合并数据集\n",
    "# 从真实井点中提取特征和目标\n",
    "X_real = real_wells_valid[common_features].values\n",
    "y_real = real_wells_valid[\"Thickness of facies(1: Fine sand)\"].values\n",
    "\n",
    "# 从虚拟井点中提取特征和目标(使用平均预测作为虚拟井的砂厚)\n",
    "X_pseudo = pseudo_wells[common_features].values\n",
    "y_pseudo = pseudo_wells[\"Mean_Pred\"].values  # 使用平均预测作为目标值\n",
    "\n",
    "# 合并数据\n",
    "X_combined = np.vstack((X_real, X_pseudo))\n",
    "y_combined = np.concatenate((y_real, y_pseudo))\n",
    "print(f\"合并后的样本数量: {len(X_combined)}\")\n",
    "\n",
    "# 创建样本权重 - 给真实样本更高的权重\n",
    "sample_weights = np.ones(len(X_combined))\n",
    "# 设置真实样本权重为虚拟样本的2倍\n",
    "sample_weights[: len(X_real)] = 2.0\n",
    "\n",
    "# 步骤2: 划分训练集和测试集(仅使用真实样本做测试)\n",
    "# 首先划分真实样本\n",
    "X_real_train, X_real_test, y_real_train, y_real_test = train_test_split(X_real, y_real, test_size=0.3, random_state=42)\n",
    "\n",
    "# 合并真实训练样本和所有虚拟样本作为完整训练集\n",
    "X_train = np.vstack((X_real_train, X_pseudo))\n",
    "y_train = np.concatenate((y_real_train, y_pseudo))\n",
    "\n",
    "# 创建训练样本权重\n",
    "train_weights = np.ones(len(X_train))\n",
    "# 设置真实样本权重为虚拟样本的2倍\n",
    "train_weights[: len(X_real_train)] = 2.0\n",
    "\n",
    "print(f\"训练集大小: {len(X_train)}, 测试集大小: {len(X_real_test)}\")\n",
    "\n",
    "# 标准化特征\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_real_test)\n",
    "\n",
    "\n",
    "# 步骤3: 模型训练与评估\n",
    "# 创建评估函数\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name, sample_weights=None):\n",
    "    \"\"\"评估模型性能并返回指标\"\"\"\n",
    "    # 训练模型\n",
    "    if sample_weights is not None:\n",
    "        model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    # 在测试集上预测\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 计算指标\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\n{model_name} 模型评估:\")\n",
    "    print(f\"  - RMSE: {rmse:.4f}\")\n",
    "    print(f\"  - R²: {r2:.4f}\")\n",
    "    print(f\"  - MAE: {mae:.4f}\")\n",
    "\n",
    "    # 绘制真实值vs预测值散点图\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], \"r--\")\n",
    "    plt.xlabel(\"真实值\")\n",
    "    plt.ylabel(\"预测值\")\n",
    "    plt.title(f\"{model_name}: 真实值 vs 预测值\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(\n",
    "        os.path.join(output_dir, f\"{model_name.lower().replace(' ', '_')}_pred_vs_true.png\"),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    return {\"model\": model, \"rmse\": rmse, \"r2\": r2, \"mae\": mae, \"y_pred\": y_pred}\n",
    "\n",
    "\n",
    "# 模型1: 支持向量回归(SVR)\n",
    "print(\"\\n训练SVR模型...\")\n",
    "# SVR对特征数量敏感，使用较少特征以避免维度灾难\n",
    "# 如果特征数量较多，可以考虑使用前2-3个最重要的特征\n",
    "svr_features = min(3, len(common_features))\n",
    "X_train_svr = X_train_scaled[:, :svr_features]\n",
    "X_test_svr = X_test_scaled[:, :svr_features]\n",
    "\n",
    "# 设置SVR参数网格\n",
    "param_grid_svr = {\"C\": [0.1, 1, 10], \"gamma\": [\"scale\", 0.01, 0.1], \"epsilon\": [0.1, 0.2], \"kernel\": [\"rbf\"]}\n",
    "\n",
    "# 创建并训练SVR模型\n",
    "svr = SVR()\n",
    "grid_search_svr = GridSearchCV(svr, param_grid_svr, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "grid_search_svr.fit(X_train_svr, y_train, sample_weight=train_weights)\n",
    "best_svr = grid_search_svr.best_estimator_\n",
    "\n",
    "print(f\"SVR最佳参数: {grid_search_svr.best_params_}\")\n",
    "svr_results = evaluate_model(best_svr, X_train_svr, y_train, X_test_svr, y_real_test, \"SVR\", train_weights)\n",
    "\n",
    "# 模型2: 随机森林(RandomForest) - 限制树深以避免过拟合\n",
    "print(\"\\n训练随机森林模型...\")\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"min_samples_leaf\": [3, 5],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "grid_search_rf.fit(X_train_scaled, y_train, sample_weight=train_weights)\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "print(f\"随机森林最佳参数: {grid_search_rf.best_params_}\")\n",
    "rf_results = evaluate_model(best_rf, X_train_scaled, y_train, X_test_scaled, y_real_test, \"随机森林\", train_weights)\n",
    "\n",
    "# 模型3: XGBoost - 控制学习率和复杂度\n",
    "print(\"\\n训练XGBoost模型...\")\n",
    "param_grid_xgb = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"subsample\": [0.8],\n",
    "    \"colsample_bytree\": [0.8],\n",
    "    \"min_child_weight\": [3, 5],\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "grid_search_xgb = GridSearchCV(xgb, param_grid_xgb, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "grid_search_xgb.fit(X_train_scaled, y_train, sample_weight=train_weights)\n",
    "best_xgb = grid_search_xgb.best_estimator_\n",
    "\n",
    "print(f\"XGBoost最佳参数: {grid_search_xgb.best_params_}\")\n",
    "xgb_results = evaluate_model(best_xgb, X_train_scaled, y_train, X_test_scaled, y_real_test, \"XGBoost\", train_weights)\n",
    "\n",
    "# 步骤4: 比较模型性能\n",
    "models_comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"模型\": [\"SVR\", \"随机森林\", \"XGBoost\"],\n",
    "        \"RMSE\": [svr_results[\"rmse\"], rf_results[\"rmse\"], xgb_results[\"rmse\"]],\n",
    "        \"R²\": [svr_results[\"r2\"], rf_results[\"r2\"], xgb_results[\"r2\"]],\n",
    "        \"MAE\": [svr_results[\"mae\"], rf_results[\"mae\"], xgb_results[\"mae\"]],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n模型性能比较:\")\n",
    "print(models_comparison)\n",
    "\n",
    "# 保存比较结果\n",
    "models_comparison.to_csv(os.path.join(output_dir, \"model_comparison.csv\"), index=False)\n",
    "\n",
    "# 选择最佳模型(基于R²)\n",
    "best_model_idx = models_comparison[\"R²\"].idxmax()\n",
    "best_model_name = models_comparison.loc[best_model_idx, \"模型\"]\n",
    "print(f\"\\n最佳模型: {best_model_name}\")\n",
    "\n",
    "# 为地震数据创建预测\n",
    "print(\"\\n使用最佳模型为整个工区生成预测...\")\n",
    "\n",
    "# 准备地震数据\n",
    "seismic_data = data_H6_2_attr.copy()\n",
    "X_seismic = seismic_data[common_features].fillna(seismic_data[common_features].mean())\n",
    "\n",
    "# 标准化特征\n",
    "X_seismic_scaled = scaler.transform(X_seismic)\n",
    "\n",
    "# 根据最佳模型选择预测方法\n",
    "if best_model_name == \"SVR\":\n",
    "    X_seismic_model = X_seismic_scaled[:, :svr_features]\n",
    "    predictions = best_svr.predict(X_seismic_model)\n",
    "    best_model = best_svr\n",
    "elif best_model_name == \"随机森林\":\n",
    "    predictions = best_rf.predict(X_seismic_scaled)\n",
    "    best_model = best_rf\n",
    "else:  # XGBoost\n",
    "    predictions = best_xgb.predict(X_seismic_scaled)\n",
    "    best_model = best_xgb\n",
    "\n",
    "# 将预测结果添加到地震数据\n",
    "seismic_data[\"Predicted_Sand_Thickness\"] = predictions\n",
    "\n",
    "# 将负值预测设为0\n",
    "if (predictions < 0).any():\n",
    "    neg_count = (predictions < 0).sum()\n",
    "    print(f\"注意: {neg_count} 个负的砂厚预测值已被替换为0\")\n",
    "    seismic_data[\"Predicted_Sand_Thickness\"] = seismic_data[\"Predicted_Sand_Thickness\"].clip(lower=0)\n",
    "\n",
    "# 保存预测结果\n",
    "seismic_data.to_csv(os.path.join(output_dir, \"seismic_with_predictions.csv\"), index=False)\n",
    "\n",
    "# 步骤5: 可视化预测结果\n",
    "print(\"\\n可视化预测结果...\")\n",
    "\n",
    "# 使用最佳模型进行可视化\n",
    "visualize_attribute_map(\n",
    "    data_points=seismic_data,\n",
    "    attribute_name=\"Predicted_Sand_Thickness\",\n",
    "    attribute_label=f\"砂厚预测值(米) - {best_model_name}模型\",\n",
    "    real_wells=real_wells_valid,\n",
    "    pseudo_wells=None,\n",
    "    target_column=\"Thickness of facies(1: Fine sand)\",\n",
    "    output_dir=output_dir,\n",
    "    filename_prefix=f\"predicted_sand_thickness_{best_model_name.lower()}\",\n",
    "    class_thresholds=[0.1, 10],  # 分类阈值：低值(<0.1)、中值(0.1-10)、高值(>10)\n",
    "    figsize=(16, 14),\n",
    "    dpi=300,\n",
    "    cmap=\"viridis\",\n",
    "    point_size=10,\n",
    "    well_size=50,\n",
    ")\n",
    "\n",
    "# 可选：特征重要性分析（对于树模型）\n",
    "if best_model_name in [\"随机森林\", \"XGBoost\"]:\n",
    "    print(\"\\n分析特征重要性...\")\n",
    "\n",
    "    # 获取特征重要性\n",
    "    if best_model_name == \"随机森林\":\n",
    "        importances = best_rf.feature_importances_\n",
    "    else:\n",
    "        importances = best_xgb.feature_importances_\n",
    "\n",
    "    # 创建特征重要性DataFrame\n",
    "    feature_importance_df = pd.DataFrame({\"特征\": common_features, \"重要性\": importances})\n",
    "    feature_importance_df = feature_importance_df.sort_values(\"重要性\", ascending=False)\n",
    "\n",
    "    # 绘制特征重要性\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance_df[\"特征\"], feature_importance_df[\"重要性\"])\n",
    "    plt.xlabel(\"特征重要性\")\n",
    "    plt.title(f\"{best_model_name}模型特征重要性\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(output_dir, f\"{best_model_name.lower()}_feature_importance.png\"), dpi=300, bbox_inches=\"tight\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    # 保存特征重要性\n",
    "    feature_importance_df.to_csv(\n",
    "        os.path.join(output_dir, f\"{best_model_name.lower()}_feature_importance.csv\"), index=False\n",
    "    )\n",
    "\n",
    "print(\"\\n建模与预测完成。所有结果已保存到输出目录。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-af",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
